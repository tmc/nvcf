<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Function Creation &mdash; NVIDIA Cloud Functions latest documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/pygments_dark.css" type="text/css" />
      <link rel="stylesheet" href="../_static/theme-switcher-general.css" type="text/css" />
      <link rel="stylesheet" href="../_static/omni-style-dark.css" type="text/css" />
      <link rel="stylesheet" href="../_static/api-styles-dark.css" type="text/css" />
      <link rel="stylesheet" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="../_static/api-styles.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/mermaid-init.js"></script>
        <script src="../_static/external-links.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/theme-setter.js"></script>
        <script src="../_static/design-tabs.js"></script>
        <script src="../_static/version.js"></script>
        <script src="../_static/social-media.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Function Deployment" href="function-deployment.html" />
    <link rel="prev" title="Function Lifecycle" href="function-lifecycle.html" />
 

    <script src="https://assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js" ></script>
    


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="../index.html">
  <img src="../_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">NVIDIA Cloud Functions</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="elastic-nim.html">Elastic NIM</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="function-lifecycle.html">Function Lifecycle</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Function Creation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#working-with-ngc-private-registry">Working with NGC Private Registry</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#generate-an-ngc-personal-api-key">Generate an NGC Personal API Key</a></li>
<li class="toctree-l3"><a class="reference internal" href="#download-configure-the-ngc-cli">Download &amp; Configure the NGC CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="#authenticate-with-ngc-docker-registry">Authenticate with NGC Docker Registry</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optional-push-a-container-to-the-ngc-private-registry">(Optional) Push a Container to the NGC Private Registry</a></li>
<li class="toctree-l3"><a class="reference internal" href="#best-practices-with-ngc-docker-registry-and-cloud-functions">Best Practices with NGC Docker Registry and Cloud Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#container-versioning">Container Versioning</a></li>
<li class="toctree-l4"><a class="reference internal" href="#usage-of-ngc-teams">Usage of NGC Teams</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#container-based-function-creation">Container-Based Function Creation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#resources">Resources</a></li>
<li class="toctree-l3"><a class="reference internal" href="#container-endpoints">Container Endpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="#composing-a-fastapi-container">Composing a FastAPI Container</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#create-the-requirements-txt-file">Create the “requirements.txt” File</a></li>
<li class="toctree-l4"><a class="reference internal" href="#implement-the-server">Implement the Server</a></li>
<li class="toctree-l4"><a class="reference internal" href="#create-the-dockerfile">Create the Dockerfile</a></li>
<li class="toctree-l4"><a class="reference internal" href="#build-the-container-create-the-function">Build the Container &amp; Create the Function</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#composing-a-pytriton-container">Composing a PyTriton Container</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id5">Create the “requirements.txt” File</a></li>
<li class="toctree-l4"><a class="reference internal" href="#create-the-run-py-file">Create the “run.py” File</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">Create the “Dockerfile”</a></li>
<li class="toctree-l4"><a class="reference internal" href="#build-the-docker-image">Build the Docker Image</a></li>
<li class="toctree-l4"><a class="reference internal" href="#push-the-docker-image">Push the Docker Image</a></li>
<li class="toctree-l4"><a class="reference internal" href="#create-the-function">Create the Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="#additional-examples">Additional Examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#triton-based-container-configuration">Triton-based Container Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#configuration">Configuration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#creating-functions-with-ngc-models-resources">Creating Functions with NGC Models &amp; Resources</a></li>
<li class="toctree-l3"><a class="reference internal" href="#creating-grpc-based-functions">Creating gRPC-based Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l4"><a class="reference internal" href="#grpc-function-creation-via-ui">gRPC Function Creation via UI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#grpc-function-creation-via-cli">gRPC Function Creation via CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#grpc-function-creation-via-api">gRPC Function Creation via API</a></li>
<li class="toctree-l4"><a class="reference internal" href="#grpc-function-invocation">gRPC Function Invocation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#available-container-variables">Available Container Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="#adding-partial-response-progress">Adding Partial Response (Progress)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#storing-partial-and-complete-outputs">Storing Partial and Complete Outputs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#creating-a-progress-file">Creating a Progress File</a></li>
<li class="toctree-l4"><a class="reference internal" href="#best-practices">Best Practices</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#helm-based-function-creation">Helm-Based Function Creation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#helm-prereq">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="#secret-management">Secret Management</a></li>
<li class="toctree-l3"><a class="reference internal" href="#create-a-helm-based-function">Create a Helm-based Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="#limitations">Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#helm-chart-overrides">Helm Chart Overrides</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="function-deployment.html">Function Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="function-management.html">Function Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="function-monitoring.html">Function Monitoring</a></li>
<li class="toctree-l1"><a class="reference internal" href="function-permissions.html">Function Permissions</a></li>
<li class="toctree-l1"><a class="reference internal" href="assets.html">Asset Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="cluster-management.html">Cluster Setup &amp; Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="account-access.html">NGC Account Access</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">NVIDIA Cloud Functions</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">


<li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
  
<li>Function Creation</li>

      <li class="wy-breadcrumbs-aside">
      </li>
<li class="wy-breadcrumbs-aside">

  <span>&nbsp;</span>
</li>

  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="function-creation">
<span id="id1"></span><h1>Function Creation<a class="headerlink" href="#function-creation" title="Permalink to this headline"></a></h1>
<p>This page describes the steps to create a function within Cloud Functions.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Please ensure before function creation, you’ve <a class="reference internal" href="#ngc-private-registry"><span class="std std-ref">installed and configured the NGC CLI for working with the NGC Private Registry</span></a>.</p>
</div>
<p>Functions can be created in one of three ways, listed below, and also visible in the Cloud Functions UI.</p>
<img alt="../_images/function-creation-start.png" class="align-center" src="../_images/function-creation-start.png" />
<ol class="arabic simple">
<li><p>Custom Container</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Enables any container-based workload as long as the container exposes an inference endpoint and a health check.</p></li>
<li><p>Option to leverage any server, ex. <a class="reference external" href="https://triton-inference-server.github.io/pytriton/">PyTriton</a>, <a class="reference external" href="https://fastapi.tiangolo.com/">FastAPI</a>, <a class="reference external" href="https://developer.nvidia.com/triton-inference-server">Triton</a>.</p></li>
<li><p>More easily take advantage of Cloud Functions features such as the <a class="reference internal" href="assets.html#assets"><span class="std std-ref">Asset Management</span></a> API for input sizes &gt;5MB, HTTP streaming or gRPC, and partial response reporting.</p></li>
<li><p>See <a class="reference internal" href="#container-functions"><span class="std std-ref">Container-Based Function Creation</span></a>.</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>Helm Chart</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Enables orchestration across multiple containers. For complex use cases where a single container isn’t flexible enough.</p></li>
<li><p>Requires one “mini-service” container defined as the inference entry point for the function.</p></li>
<li><p>Does not support partial response reporting, gRPC or HTTP streaming-based invocation.</p></li>
<li><p>See <a class="reference internal" href="#helm-functions"><span class="std std-ref">Helm-Based Function Creation</span></a>.</p></li>
</ul>
</div></blockquote>
<section id="working-with-ngc-private-registry">
<span id="ngc-private-registry"></span><h2>Working with NGC Private Registry<a class="headerlink" href="#working-with-ngc-private-registry" title="Permalink to this headline"></a></h2>
<p>Function creation requires your model, container, helm chart and/or static resources to be hosted within <a class="reference external" href="https://docs.nvidia.com/ngc/gpu-cloud/ngc-private-registry-user-guide/index.html#using-ngc-registry-from-docker-command-line">NGC Private Registry</a> as a prerequisite. Follow the steps below to optimally configure the <a class="reference external" href="https://docs.ngc.nvidia.com/cli/index.html">NGC CLI</a> to work with NGC Private Registry and Cloud Functions.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>NGC Private Registry has <a class="reference external" href="https://docs.nvidia.com/ngc/gpu-cloud/ngc-private-registry-user-guide/index.html#private-registry-size-limits">size constraints</a> on layers, images, models and resources.</p>
<p>Ensure that your uploaded resources conform to these constraints.</p>
</div>
<section id="generate-an-ngc-personal-api-key">
<h3>Generate an NGC Personal API Key<a class="headerlink" href="#generate-an-ngc-personal-api-key" title="Permalink to this headline"></a></h3>
<p>Do this by navigating to the <a class="reference external" href="https://org.ngc.nvidia.com/setup/personal-keys">Personal Keys Page</a>. For more details see <a class="reference internal" href="api.html#generate-personal-key"><span class="std std-ref">Generate an NGC Personal API Key</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It’s recommended that the API Key that you generate includes both <strong>Cloud Functions and Private Registry scopes</strong> to enable ideal Cloud Functions workflows.</p>
</div>
</section>
<section id="download-configure-the-ngc-cli">
<span id="ngc-cli-setup"></span><h3>Download &amp; Configure the NGC CLI<a class="headerlink" href="#download-configure-the-ngc-cli" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>Navigate to the <a class="reference external" href="https://ngc.nvidia.com/setup/installers/cli">NGC CLI Installer Page</a> to download the CLI and follow the installation instructions for your platform.</p></li>
<li><p>Find your NGC organization name within the <a class="reference external" href="https://org.ngc.nvidia.com/profile">NGC Organization Profile Page</a>. This is <em>not</em> the Display Name. For example: <code class="docutils literal notranslate"><span class="pre">qdrlnbkss123</span></code>.</p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">ngc</span> <span class="pre">config</span> <span class="pre">set</span></code> and input the Personal API Key generated in the previous step, along with your organization name. If prompted, default to <code class="docutils literal notranslate"><span class="pre">no-team</span></code> and <code class="docutils literal notranslate"><span class="pre">no-ace</span></code>.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>&gt; ngc config <span class="nb">set</span>
<span class="linenos">2</span>Enter API key <span class="o">[</span>****bi9Z<span class="o">]</span>. Choices: <span class="o">[</span>&lt;VALID_APIKEY&gt;, <span class="s1">&#39;no-apikey&#39;</span><span class="o">]</span>: <span class="nv">$API_KEY</span>
<span class="linenos">3</span>Enter CLI output format <span class="nb">type</span> <span class="o">[</span>json<span class="o">]</span>. Choices: <span class="o">[</span><span class="s1">&#39;ascii&#39;</span>, <span class="s1">&#39;csv&#39;</span>, <span class="s1">&#39;json&#39;</span><span class="o">]</span>: json
<span class="linenos">4</span>Enter org <span class="o">[</span>ax3ysqem02xw<span class="o">]</span>. Choices: <span class="o">[</span><span class="s1">&#39;$ORG_NAME&#39;</span><span class="o">]</span>: <span class="nv">$ORG_NAME</span>
<span class="linenos">5</span>Enter team <span class="o">[</span>no-team<span class="o">]</span>. Choices: <span class="o">[</span><span class="s1">&#39;no-team&#39;</span><span class="o">]</span>:
<span class="linenos">6</span>Enter ace <span class="o">[</span>no-ace<span class="o">]</span>. Choices: <span class="o">[</span><span class="s1">&#39;no-ace&#39;</span><span class="o">]</span>:
</pre></div>
</div>
</section>
<section id="authenticate-with-ngc-docker-registry">
<span id="ngc-private-registry-auth"></span><h3>Authenticate with NGC Docker Registry<a class="headerlink" href="#authenticate-with-ngc-docker-registry" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>Run <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">login</span> <span class="pre">nvcr.io</span></code> and input the following, note <code class="docutils literal notranslate"><span class="pre">$oauthtoken</span></code> is the actual string to input, and <code class="docutils literal notranslate"><span class="pre">$API_KEY</span></code> is the Personal API key generated in the first step.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>&gt; docker login nvcr.io
<span class="linenos">2</span>Username: <span class="nv">$oauthtoken</span>
<span class="linenos">3</span>Password: <span class="nv">$API_KEY</span>
</pre></div>
</div>
</section>
<section id="optional-push-a-container-to-the-ngc-private-registry">
<h3>(Optional) Push a Container to the NGC Private Registry<a class="headerlink" href="#optional-push-a-container-to-the-ngc-private-registry" title="Permalink to this headline"></a></h3>
<p>You should now be able to push a container to the NGC Private Registry. Optionally, validate this by pushing an example container from the <a class="reference external" href="https://github.com/NVIDIA/nv-cloud-function-helpers/tree/main/examples">samples repository</a>:</p>
<ol class="arabic simple">
<li><p>First clone and build the docker image.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>&gt; git clone https://github.com/NVIDIA/nv-cloud-function-helpers.git
<span class="linenos">2</span>&gt; <span class="nb">cd</span> nv-cloud-function-helpers/examples/fastapi_echo_sample
<span class="linenos">3</span>&gt; docker build . -t fastapi_echo_sample
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Now tag and push the docker image to the NGC Private Registry.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>&gt; docker tag fastapi_echo_sample:latest nvcr.io/<span class="nv">$ORG_NAME</span>/fastapi_echo_sample:latest
<span class="linenos">2</span>&gt; docker push nvcr.io/<span class="nv">$ORG_NAME</span>/fastapi_echo_sample:latest
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Note that any additional slashes in the path when tagging and pushing to <code class="docutils literal notranslate"><span class="pre">nvcr.io</span></code> will be detected by Private Registry as specifying a team. This is most likely not what you want.</p>
</div>
<ol class="arabic simple" start="3">
<li><p>Once this finishes, you’ll now be able to see the new container in the <a class="reference external" href="https://registry.ngc.nvidia.com/containers">NGC Private Registry Containers Page</a> and it will be available for use in function creation.</p></li>
</ol>
</section>
<section id="best-practices-with-ngc-docker-registry-and-cloud-functions">
<h3>Best Practices with NGC Docker Registry and Cloud Functions<a class="headerlink" href="#best-practices-with-ngc-docker-registry-and-cloud-functions" title="Permalink to this headline"></a></h3>
<section id="container-versioning">
<span id="id3"></span><h4>Container Versioning<a class="headerlink" href="#container-versioning" title="Permalink to this headline"></a></h4>
<ul>
<li><p>Ensure that any resources that you tag for deployment into production environments are not simply using “latest” and are following a standard version control convention.</p>
<blockquote>
<div><ul class="simple">
<li><p>During autoscaling, a function scaling any additional instances will pull the same specificed container image and version. If version is set to “latest”, and the “latest” container image is updated between instance scaling, this can lead to undefined behavior.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Function versions created are immutable, this means that the container image and version cannot be updated for a function without creating a new version of the function.</p></li>
</ul>
</section>
<section id="usage-of-ngc-teams">
<h4>Usage of NGC Teams<a class="headerlink" href="#usage-of-ngc-teams" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><p>For easier handling of authorization and accessibility, we recommend pushing your containers, helm charts, models and resources to the root of your NGC organization (i.e. “No Team”), not to a team within the organization.</p></li>
<li><p>Note that any additional slashes in the path when tagging and pushing to <code class="docutils literal notranslate"><span class="pre">nvcr.io</span></code> will be detected as an NGC team.</p></li>
</ul>
</section>
</section>
</section>
<section id="container-based-function-creation">
<span id="container-functions"></span><h2>Container-Based Function Creation<a class="headerlink" href="#container-based-function-creation" title="Permalink to this headline"></a></h2>
<p>Container-based functions require building and pushing a Cloud Functions compatible <a class="reference external" href="https://docker.com">Docker</a> container image to the NGC Private Registry.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Before proceeding, ensure that you have the NGC CLI installed and configured with an API Key that has the required scopes for Cloud Functions and Private Registry.</p>
<p>See <a class="reference internal" href="#ngc-private-registry"><span class="std std-ref">Working with NGC Private Registry</span></a> for instructions.</p>
</div>
<section id="resources">
<h3>Resources<a class="headerlink" href="#resources" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Example containers can be found <a class="reference external" href="https://github.com/NVIDIA/nv-cloud-function-helpers/tree/main/examples">here</a>.</p></li>
<li><p>The repository also contains <a class="reference external" href="https://github.com/NVIDIA/nv-cloud-function-helpers/blob/main/nv_cloud_function_helpers/nvcf_container/helpers.py">helper functions</a> that are useful when authoring your container, including:</p>
<blockquote>
<div><ul class="simple">
<li><p>Helpers that parse Cloud Functions-specific parameters on invocation</p></li>
<li><p>Helpers that can be used to instrument your container with Cloud Functions compatible logs</p></li>
<li><p>Helpers for working with assets</p></li>
</ul>
</div></blockquote>
</li>
<li><p>After container creation, but before proceeding to deployment, it is <strong>strongly recommended</strong> to validate your container’s configuration locally, see <a class="reference internal" href="function-deployment.html#validate-deployment"><span class="std std-ref">Deployment Validation</span></a>.</p></li>
<li><p>It’s always a <strong>best practice to emit logs</strong> from your inference container. See <a class="reference internal" href="function-monitoring.html#logging-metrics"><span class="std std-ref">Logging and Metrics</span></a> for how to add logs to your container. Cloud Functions also supports third-party logging and metrics emission from your container.</p></li>
</ul>
</section>
<section id="container-endpoints">
<h3>Container Endpoints<a class="headerlink" href="#container-endpoints" title="Permalink to this headline"></a></h3>
<p>Any server can be implemented within the container, as long as it implements the following:</p>
<ul class="simple">
<li><p>For HTTP-based functions, a health check endpoint that returns a 200 HTTP Status Code on success.</p></li>
<li><p>For gRPC-based functions, a standard gRPC health check. See <a class="reference external" href="https://github.com/grpc/grpc-proto/blob/master/grpc/health/v1/health.proto">these docs for more info</a>  also <a class="reference external" href="https://grpc.io/docs/guides/health-checking/">gRPC Health Checking</a>.</p></li>
<li><p>An inference endpoint (this endpoint will be called during function invocation)</p></li>
</ul>
<p>These endpoints are expected to be served on the same port, defined as the <code class="docutils literal notranslate"><span class="pre">inferencePort</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Cloud Functions reserves the following ports on your container for internal monitoring and metrics:</p>
<ul class="simple">
<li><p>Port <code class="docutils literal notranslate"><span class="pre">8080</span></code></p></li>
<li><p>Port <code class="docutils literal notranslate"><span class="pre">8010</span></code></p></li>
</ul>
<p>Cloud Functions also expects the following directories in the container to remain read-only for caching purposes:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">/config/</span></code> directory</p></li>
<li><p>Nested directories created inside <code class="docutils literal notranslate"><span class="pre">/config/</span></code></p></li>
</ul>
</div>
</section>
<section id="composing-a-fastapi-container">
<h3>Composing a FastAPI Container<a class="headerlink" href="#composing-a-fastapi-container" title="Permalink to this headline"></a></h3>
<p>It’s possible to use any container with Cloud Functions as long as it implements a server with the above endpoints. The below is an example of a FastAPI-based container compatible with Cloud Functions. Clone the <a class="reference external" href="https://github.com/NVIDIA/nv-cloud-function-helpers/tree/main/examples/fastapi_echo_sample">full example here</a>.</p>
<section id="create-the-requirements-txt-file">
<h4>Create the “requirements.txt” File<a class="headerlink" href="#create-the-requirements-txt-file" title="Permalink to this headline"></a></h4>
<div class="literal-block-wrapper docutils container" id="id11">
<div class="code-block-caption"><span class="caption-text">requirements.txt</span><a class="headerlink" href="#id11" title="Permalink to this code"></a></div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>fastapi==0.110.0
<span class="linenos">2</span>uvicorn==0.29.0
</pre></div>
</div>
</div>
</section>
<section id="implement-the-server">
<h4>Implement the Server<a class="headerlink" href="#implement-the-server" title="Permalink to this headline"></a></h4>
<div class="literal-block-wrapper docutils container" id="id12">
<div class="code-block-caption"><span class="caption-text">http_echo_server.py</span><a class="headerlink" href="#id12" title="Permalink to this code"></a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="kn">import</span> <span class="nn">os</span>
<span class="linenos"> 2</span><span class="kn">import</span> <span class="nn">time</span>
<span class="linenos"> 3</span><span class="kn">import</span> <span class="nn">uvicorn</span>
<span class="linenos"> 4</span><span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>
<span class="linenos"> 5</span><span class="kn">from</span> <span class="nn">fastapi</span> <span class="kn">import</span> <span class="n">FastAPI</span><span class="p">,</span> <span class="n">status</span>
<span class="linenos"> 6</span><span class="kn">from</span> <span class="nn">fastapi.responses</span> <span class="kn">import</span> <span class="n">StreamingResponse</span>
<span class="linenos"> 7</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="n">app</span> <span class="o">=</span> <span class="n">FastAPI</span><span class="p">()</span>
<span class="linenos">10</span>
<span class="linenos">11</span><span class="k">class</span> <span class="nc">HealthCheck</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="linenos">12</span>    <span class="n">status</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;OK&quot;</span>
<span class="linenos">13</span>
<span class="linenos">14</span><span class="c1"># Implement the health check endpoint</span>
<span class="linenos">15</span><span class="nd">@app</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;/health&quot;</span><span class="p">,</span> <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;healthcheck&quot;</span><span class="p">],</span> <span class="n">summary</span><span class="o">=</span><span class="s2">&quot;Perform a Health Check&quot;</span><span class="p">,</span> <span class="n">response_description</span><span class="o">=</span><span class="s2">&quot;Return HTTP Status Code 200 (OK)&quot;</span><span class="p">,</span> <span class="n">status_code</span><span class="o">=</span><span class="n">status</span><span class="o">.</span><span class="n">HTTP_200_OK</span><span class="p">,</span> <span class="n">response_model</span><span class="o">=</span><span class="n">HealthCheck</span><span class="p">)</span>
<span class="linenos">16</span><span class="k">def</span> <span class="nf">get_health</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">HealthCheck</span><span class="p">:</span>
<span class="linenos">17</span>    <span class="k">return</span> <span class="n">HealthCheck</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="s2">&quot;OK&quot;</span><span class="p">)</span>
<span class="linenos">18</span>
<span class="linenos">19</span><span class="k">class</span> <span class="nc">Echo</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="linenos">20</span>    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span>
<span class="linenos">21</span>    <span class="n">delay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.000001</span>
<span class="linenos">22</span>    <span class="n">repeats</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="linenos">23</span>    <span class="n">stream</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="linenos">24</span>
<span class="linenos">25</span>
<span class="linenos">26</span><span class="c1"># Implement the inference endpoint</span>
<span class="linenos">27</span><span class="nd">@app</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="s2">&quot;/echo&quot;</span><span class="p">)</span>
<span class="linenos">28</span><span class="k">async</span> <span class="k">def</span> <span class="nf">echo</span><span class="p">(</span><span class="n">echo</span><span class="p">:</span> <span class="n">Echo</span><span class="p">):</span>
<span class="linenos">29</span>    <span class="k">if</span> <span class="n">echo</span><span class="o">.</span><span class="n">stream</span><span class="p">:</span>
<span class="linenos">30</span>        <span class="k">def</span> <span class="nf">stream_text</span><span class="p">():</span>
<span class="linenos">31</span>            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">echo</span><span class="o">.</span><span class="n">repeats</span><span class="p">):</span>
<span class="linenos">32</span>                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">echo</span><span class="o">.</span><span class="n">delay</span><span class="p">)</span>
<span class="linenos">33</span>                <span class="k">yield</span> <span class="sa">f</span><span class="s2">&quot;data: </span><span class="si">{</span><span class="n">echo</span><span class="o">.</span><span class="n">message</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">&quot;</span>
<span class="linenos">34</span>        <span class="k">return</span> <span class="n">StreamingResponse</span><span class="p">(</span><span class="n">stream_text</span><span class="p">(),</span> <span class="n">media_type</span><span class="o">=</span><span class="s2">&quot;text/event-stream&quot;</span><span class="p">)</span>
<span class="linenos">35</span>    <span class="k">else</span><span class="p">:</span>
<span class="linenos">36</span>        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">echo</span><span class="o">.</span><span class="n">delay</span><span class="p">)</span>
<span class="linenos">37</span>        <span class="k">return</span> <span class="n">echo</span><span class="o">.</span><span class="n">message</span><span class="o">*</span><span class="n">echo</span><span class="o">.</span><span class="n">repeats</span>
<span class="linenos">38</span>
<span class="linenos">39</span><span class="c1"># Serve the endpoints on a port</span>
<span class="linenos">40</span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
<span class="linenos">41</span>    <span class="n">uvicorn</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">app</span><span class="p">,</span> <span class="n">host</span><span class="o">=</span><span class="s2">&quot;0.0.0.0&quot;</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="mi">8000</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;WORKER_COUNT&#39;</span><span class="p">,</span> <span class="mi">500</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<p>Note in the example above, the function’s configuration during creation will be:</p>
<ul class="simple">
<li><p>Inference Protocol: HTTP</p></li>
<li><p>Inference Endpoint: <code class="docutils literal notranslate"><span class="pre">/echo</span></code></p></li>
<li><p>Health Endpoint: <code class="docutils literal notranslate"><span class="pre">/health</span></code></p></li>
<li><p>Inference Port (also used for health check): <code class="docutils literal notranslate"><span class="pre">8000</span></code></p></li>
</ul>
</section>
<section id="create-the-dockerfile">
<h4>Create the Dockerfile<a class="headerlink" href="#create-the-dockerfile" title="Permalink to this headline"></a></h4>
<div class="literal-block-wrapper docutils container" id="id13">
<div class="code-block-caption"><span class="caption-text">Dockerfile</span><a class="headerlink" href="#id13" title="Permalink to this code"></a></div>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="k">FROM</span><span class="w"> </span><span class="s">python:3.10.13-bookworm</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="k">ENV</span><span class="w"> </span><span class="nv">WORKER_COUNT</span><span class="o">=</span><span class="m">10</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="k">WORKDIR</span><span class="w"> </span><span class="s">/app</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span><span class="k">COPY</span><span class="w"> </span>requirements.txt ./
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="k">RUN</span><span class="w"> </span>python -m pip install --no-cache-dir -U pip <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="linenos">10</span>    python -m pip install --no-cache-dir -r requirements.txt
<span class="linenos">11</span>
<span class="linenos">12</span><span class="k">COPY</span><span class="w"> </span>http_echo_server.py /app/
<span class="linenos">13</span>
<span class="linenos">14</span><span class="k">CMD</span><span class="w"> </span>uvicorn http_echo_server:app --host<span class="o">=</span><span class="m">0</span>.0.0.0 --workers<span class="o">=</span><span class="nv">$WORKER_COUNT</span>
</pre></div>
</div>
</div>
</section>
<section id="build-the-container-create-the-function">
<h4>Build the Container &amp; Create the Function<a class="headerlink" href="#build-the-container-create-the-function" title="Permalink to this headline"></a></h4>
<p>See the <a class="reference internal" href="quickstart.html#quick-start"><span class="std std-ref">Quickstart</span></a> for the remaining steps.</p>
</section>
</section>
<section id="composing-a-pytriton-container">
<h3>Composing a PyTriton Container<a class="headerlink" href="#composing-a-pytriton-container" title="Permalink to this headline"></a></h3>
<p>NVIDIA’s <a class="reference external" href="https://triton-inference-server.github.io/pytriton/">PyTriton</a> is a Python native solution of Triton inference server. A minimum version of 0.3.0 is required.</p>
<section id="id5">
<h4>Create the “requirements.txt” File<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><p>This file should list the Python dependencies required for your model.</p></li>
<li><p>Add nvidia-pytriton to your <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> file.</p></li>
</ul>
<p>Here is an example of a <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> file:</p>
<div class="literal-block-wrapper docutils container" id="id14">
<div class="code-block-caption"><span class="caption-text">requirements.txt</span><a class="headerlink" href="#id14" title="Permalink to this code"></a></div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>--extra-index-url https://pypi.ngc.nvidia.com
<span class="linenos">2</span>opencv-python-headless
<span class="linenos">3</span>pycocotools
<span class="linenos">4</span>matplotlib
<span class="linenos">5</span>torch==2.1.0
<span class="linenos">6</span>nvidia-pytriton==0.3.0
<span class="linenos">7</span>numpy
</pre></div>
</div>
</div>
</section>
<section id="create-the-run-py-file">
<h4>Create the “run.py” File<a class="headerlink" href="#create-the-run-py-file" title="Permalink to this headline"></a></h4>
<ol class="arabic simple">
<li><p>Your <code class="docutils literal notranslate"><span class="pre">run.py</span></code> file (or similar Python file) needs to define a PyTriton model.</p></li>
<li><p>This involves importing your model dependencies, creating a PyTritonServer class with an <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function, an <code class="docutils literal notranslate"><span class="pre">_infer_fn</span></code> function and a <code class="docutils literal notranslate"><span class="pre">run</span></code> function that serves the inference_function, defining the model name, the inputs and the outputs along with optional configuration.</p></li>
</ol>
<p>Here is an example of a <code class="docutils literal notranslate"><span class="pre">run.py</span></code> file:</p>
<div class="literal-block-wrapper docutils container" id="id15">
<div class="code-block-caption"><span class="caption-text">run.py</span><a class="headerlink" href="#id15" title="Permalink to this code"></a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="linenos"> 2</span><span class="kn">from</span> <span class="nn">pytriton.model_config</span> <span class="kn">import</span> <span class="n">ModelConfig</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="linenos"> 3</span><span class="kn">from</span> <span class="nn">pytriton.triton</span> <span class="kn">import</span> <span class="n">Triton</span><span class="p">,</span> <span class="n">TritonConfig</span>
<span class="linenos"> 4</span><span class="kn">import</span> <span class="nn">time</span>
<span class="linenos"> 5</span><span class="o">....</span>
<span class="linenos"> 6</span><span class="k">class</span> <span class="nc">PyTritonServer</span><span class="p">:</span>
<span class="linenos"> 7</span>    <span class="sd">&quot;&quot;&quot;triton server for timed_sleeper&quot;&quot;&quot;</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="linenos">10</span>        <span class="c1"># basically need to accept image, mask(PIL Images), prompt, negative_prompt(str), seed(int)</span>
<span class="linenos">11</span>        <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;timed_sleeper&quot;</span>
<span class="linenos">12</span>
<span class="linenos">13</span>    <span class="k">def</span> <span class="nf">_infer_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">requests</span><span class="p">):</span>
<span class="linenos">14</span>        <span class="n">responses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="linenos">15</span>        <span class="k">for</span> <span class="n">req</span> <span class="ow">in</span> <span class="n">requests</span><span class="p">:</span>
<span class="linenos">16</span>            <span class="n">req_data</span> <span class="o">=</span> <span class="n">req</span><span class="o">.</span><span class="n">data</span>
<span class="linenos">17</span>            <span class="n">sleep_duration</span> <span class="o">=</span> <span class="n">numpy_array_to_variable</span><span class="p">(</span><span class="n">req_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;sleep_duration&quot;</span><span class="p">))</span>
<span class="linenos">18</span>            <span class="c1"># deal with header dict keys being lowerscale</span>
<span class="linenos">19</span>            <span class="n">request_parameters_dict</span> <span class="o">=</span> <span class="n">uppercase_keys</span><span class="p">(</span><span class="n">req</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>
<span class="linenos">20</span>            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">sleep_duration</span><span class="p">)</span>
<span class="linenos">21</span>            <span class="n">responses</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;sleep_duration&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">sleep_duration</span><span class="p">])})</span>
<span class="linenos">22</span>
<span class="linenos">23</span>        <span class="k">return</span> <span class="n">responses</span>
<span class="linenos">24</span>
<span class="linenos">25</span>    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="linenos">26</span>        <span class="sd">&quot;&quot;&quot;run triton server&quot;&quot;&quot;</span>
<span class="linenos">27</span>        <span class="k">with</span> <span class="n">Triton</span><span class="p">(</span>
<span class="linenos">28</span>            <span class="n">config</span><span class="o">=</span><span class="n">TritonConfig</span><span class="p">(</span>
<span class="linenos">29</span>                <span class="n">http_header_forward_pattern</span><span class="o">=</span><span class="s2">&quot;NVCF-*&quot;</span><span class="p">,</span>  <span class="c1"># this is required</span>
<span class="linenos">30</span>                <span class="n">http_port</span><span class="o">=</span><span class="mi">8000</span><span class="p">,</span>
<span class="linenos">31</span>                <span class="n">grpc_port</span><span class="o">=</span><span class="mi">8001</span><span class="p">,</span>
<span class="linenos">32</span>                <span class="n">metrics_port</span><span class="o">=</span><span class="mi">8002</span><span class="p">,</span>
<span class="linenos">33</span>            <span class="p">)</span>
<span class="linenos">34</span>        <span class="p">)</span> <span class="k">as</span> <span class="n">triton</span><span class="p">:</span>
<span class="linenos">35</span>            <span class="n">triton</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span>
<span class="linenos">36</span>                <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;timed_sleeper&quot;</span><span class="p">,</span>
<span class="linenos">37</span>                <span class="n">infer_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_infer_fn</span><span class="p">,</span>
<span class="linenos">38</span>                <span class="n">inputs</span><span class="o">=</span><span class="p">[</span>
<span class="linenos">39</span>                    <span class="n">Tensor</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;sleep_duration&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,)),</span>
<span class="linenos">40</span>                <span class="p">],</span>
<span class="linenos">41</span>                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">Tensor</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;sleep_duration&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))],</span>
<span class="linenos">42</span>                <span class="n">config</span><span class="o">=</span><span class="n">ModelConfig</span><span class="p">(</span><span class="n">batching</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="linenos">43</span>            <span class="p">)</span>
<span class="linenos">44</span>            <span class="n">triton</span><span class="o">.</span><span class="n">serve</span><span class="p">()</span>
<span class="linenos">45</span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
<span class="linenos">46</span>    <span class="n">server</span> <span class="o">=</span> <span class="n">PyTritonServer</span><span class="p">()</span>
<span class="linenos">47</span>    <span class="n">server</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div>
</section>
<section id="id6">
<h4>Create the “Dockerfile”<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h4>
<ol class="arabic simple">
<li><p>Create a file named <code class="docutils literal notranslate"><span class="pre">Dockerfile</span></code> in your model directory.</p></li>
<li><p>It’s <strong>strongly recommended to use NVIDIA-optimized containers like CUDA, Pytorch or TensorRT as your base container</strong>. They can be downloaded from the <a class="reference external" href="https://catalog.ngc.nvidia.com/">NGC Catalog</a>.</p></li>
<li><p>Make sure to install your Python requirements in your <code class="docutils literal notranslate"><span class="pre">Dockerfile</span></code>.</p></li>
<li><p>Copy in your model source code, and model weights unless you plan to host them in NGC Private Registry.</p></li>
</ol>
<p>Here is an example of a <code class="docutils literal notranslate"><span class="pre">Dockerfile</span></code>:</p>
<div class="literal-block-wrapper docutils container" id="id16">
<div class="code-block-caption"><span class="caption-text">Dockerfile</span><a class="headerlink" href="#id16" title="Permalink to this code"></a></div>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="k">FROM</span><span class="w"> </span><span class="s">nvcr.io/nvidia/cuda:12.1.1-devel-ubuntu22.04</span>
<span class="linenos"> 2</span><span class="k">RUN</span><span class="w"> </span>apt-get update <span class="o">&amp;&amp;</span> apt-get install -y <span class="se">\</span>
<span class="linenos"> 3</span>    git <span class="se">\</span>
<span class="linenos"> 4</span>    python3 <span class="se">\</span>
<span class="linenos"> 5</span>    python3-pip <span class="se">\</span>
<span class="linenos"> 6</span>    python-is-python3 <span class="se">\</span>
<span class="linenos"> 7</span>    libsm6 <span class="se">\</span>
<span class="linenos"> 8</span>    libxext6 <span class="se">\</span>
<span class="linenos"> 9</span>    libxrender-dev <span class="se">\</span>
<span class="linenos">10</span>    curl <span class="se">\</span>
<span class="linenos">11</span>    <span class="o">&amp;&amp;</span> rm -rf /var/lib/apt/lists/*
<span class="linenos">12</span><span class="k">WORKDIR</span><span class="w"> </span><span class="s">/workspace/</span>
<span class="linenos">13</span>
<span class="linenos">14</span><span class="c"># Install requirements file</span>
<span class="linenos">15</span><span class="k">COPY</span><span class="w"> </span>requirements.txt requirements.txt
<span class="linenos">16</span><span class="k">RUN</span><span class="w"> </span>pip install --no-cache-dir --upgrade pip
<span class="linenos">17</span><span class="k">RUN</span><span class="w"> </span>pip install --no-cache-dir -r requirements.txt
<span class="linenos">18</span><span class="k">ENV</span><span class="w"> </span><span class="nv">DEBIAN_FRONTEND</span><span class="o">=</span>noninteractive
<span class="linenos">19</span>
<span class="linenos">20</span><span class="c"># Copy model source code and weights</span>
<span class="linenos">21</span><span class="k">COPY</span><span class="w"> </span>model_weights /models
<span class="linenos">22</span><span class="k">COPY</span><span class="w"> </span>model_source .
<span class="linenos">23</span><span class="k">COPY</span><span class="w"> </span>run.py .
<span class="linenos">24</span>
<span class="linenos">25</span><span class="c"># Set run command to start PyTriton to serve the model</span>
<span class="linenos">26</span><span class="k">CMD</span><span class="w"> </span>python3 run.py
</pre></div>
</div>
</div>
</section>
<section id="build-the-docker-image">
<h4>Build the Docker Image<a class="headerlink" href="#build-the-docker-image" title="Permalink to this headline"></a></h4>
<ol class="arabic simple">
<li><p>Open a terminal or command prompt.</p></li>
<li><p>Navigate to the <code class="docutils literal notranslate"><span class="pre">my_model</span></code> directory.</p></li>
<li><p>Run the following command to build the docker image:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker build -t my_model_image .
</pre></div>
</div>
<p>Replace <code class="docutils literal notranslate"><span class="pre">my_model_image</span></code> with the desired name for your docker image.</p>
</section>
<section id="push-the-docker-image">
<h4>Push the Docker Image<a class="headerlink" href="#push-the-docker-image" title="Permalink to this headline"></a></h4>
<p>Before beginning, ensure that you have <a class="reference internal" href="#ngc-private-registry-auth"><span class="std std-ref">authenticated with the NGC Docker Registry</span></a>.</p>
<ol class="arabic simple">
<li><p>Tag and push the docker image to the NGC Private Registry.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>&gt; docker tag my_model_image:latest nvcr.io/<span class="nv">$ORG_NAME</span>/my_model_image:latest
<span class="linenos">2</span>&gt; docker push nvcr.io/<span class="nv">$ORG_NAME</span>/my_model_image:latest
</pre></div>
</div>
</section>
<section id="create-the-function">
<h4>Create the Function<a class="headerlink" href="#create-the-function" title="Permalink to this headline"></a></h4>
<ol class="arabic simple">
<li><p>Create the function via API by running the following curl with an <code class="docutils literal notranslate"><span class="pre">$API_KEY</span></code> and your <code class="docutils literal notranslate"><span class="pre">$ORG_NAME</span></code>. In this example, we defined the inference endpoint as <code class="docutils literal notranslate"><span class="pre">8000</span></code> and are using the default inference and health endpoint paths.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span> curl --location <span class="s1">&#39;https://api.ngc.nvidia.com/v2/nvcf/functions&#39;</span> <span class="se">\</span>
<span class="linenos"> 2</span> --header <span class="s1">&#39;Content-Type: application/json&#39;</span> <span class="se">\</span>
<span class="linenos"> 3</span> --header <span class="s1">&#39;Accept: application/json&#39;</span> <span class="se">\</span>
<span class="linenos"> 4</span> --header <span class="s1">&#39;Authorization: Bearer $API_KEY&#39;</span> <span class="se">\</span>
<span class="linenos"> 5</span> --data <span class="s1">&#39;{</span>
<span class="linenos"> 6</span><span class="s1">     &quot;name&quot;: &quot;my-model-function&quot;,</span>
<span class="linenos"> 7</span><span class="s1">     &quot;inferenceUrl&quot;: &quot;/v2/models/my_model_image/infer&quot;,</span>
<span class="linenos"> 8</span><span class="s1">     &quot;healthUri&quot;: &quot;/v2/health/ready&quot;,</span>
<span class="linenos"> 9</span><span class="s1">     &quot;inferencePort&quot;: 8000,</span>
<span class="linenos">10</span><span class="s1">     &quot;containerImage&quot;: &quot;nvcr.io/$ORG_NAME/my_model_image:latest&quot;</span>
<span class="linenos">11</span><span class="s1"> }&#39;</span>
</pre></div>
</div>
</section>
<section id="additional-examples">
<h4>Additional Examples<a class="headerlink" href="#additional-examples" title="Permalink to this headline"></a></h4>
<p>See more examples of PyTriton containers that are Cloud Functions compatible <a class="reference external" href="https://github.com/NVIDIA/nv-cloud-function-helpers/tree/main/examples">here</a>.</p>
</section>
</section>
<section id="triton-based-container-configuration">
<h3>Triton-based Container Configuration<a class="headerlink" href="#triton-based-container-configuration" title="Permalink to this headline"></a></h3>
<p>NVIDIA Cloud Functions is designed to work natively with <a class="reference external" href="https://developer.nvidia.com/triton-inference-server">Triton Inference Server</a> based containers, including leveraging metrics and health checks from the server.</p>
<p>Pre-built Triton docker images can be found within <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver">NGC’s Container catalog</a>. A minimum version of 23.04 (2.33.0) is required.</p>
<section id="configuration">
<h4>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline"></a></h4>
<p>The default health <code class="docutils literal notranslate"><span class="pre">/v2/health/ready</span></code>, port <code class="docutils literal notranslate"><span class="pre">8000</span></code>, and inference endpoint (<code class="docutils literal notranslate"><span class="pre">v2/models/$MODEL_NAME/infer</span></code>) work automatically with Triton-based containers.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The docker image’s run command <em>must</em> be configured with the following:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>CMD tritonserver --model-repository<span class="o">=</span><span class="si">${</span><span class="nv">MODEL_PATH</span><span class="si">}</span> --http-header-forward-pattern NVCF-.*
</pre></div>
</div>
</div>
<p>Here is an example of a <code class="docutils literal notranslate"><span class="pre">Dockerfile</span></code>:</p>
<div class="literal-block-wrapper docutils container" id="id17">
<div class="code-block-caption"><span class="caption-text">Dockerfile</span><a class="headerlink" href="#id17" title="Permalink to this code"></a></div>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="k">FROM</span><span class="w"> </span><span class="s">nvcr.io/nvidia/tritonserver:24.01-py3</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="c"># install requirements file</span>
<span class="linenos"> 4</span><span class="k">COPY</span><span class="w"> </span>requirements.txt requirements.txt
<span class="linenos"> 5</span><span class="k">RUN</span><span class="w"> </span>pip install --no-cache-dir --upgrade pip
<span class="linenos"> 6</span><span class="k">RUN</span><span class="w"> </span>pip install --no-cache-dir -r requirements.txt
<span class="linenos"> 7</span>
<span class="linenos"> 8</span><span class="k">COPY</span><span class="w"> </span>model_repository /model_repository
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="k">ENV</span><span class="w"> </span>CUDA_MODULE_LOADING LAZY
<span class="linenos">11</span><span class="k">ENV</span><span class="w"> </span>LOG_VERBOSE <span class="m">0</span>
<span class="linenos">12</span>
<span class="linenos">13</span><span class="k">CMD</span><span class="w"> </span>tritonserver --log-verbose <span class="si">${</span><span class="nv">LOG_VERBOSE</span><span class="si">}</span> --http-header-forward-pattern <span class="o">(</span>nvcf-.*<span class="p">|</span>NVCF-.*<span class="o">)</span> <span class="se">\</span>
<span class="linenos">14</span>    --model-repository /model_repository/ --model-control-mode<span class="o">=</span>none --strict-readiness <span class="m">1</span>
</pre></div>
</div>
</div>
<p>See a <a class="reference external" href="https://github.com/NVIDIA/nv-cloud-function-helpers/tree/main/examples/triton_echo_sample">full example of a Triton container</a>.</p>
</section>
</section>
<section id="creating-functions-with-ngc-models-resources">
<span id="model-path"></span><h3>Creating Functions with NGC Models &amp; Resources<a class="headerlink" href="#creating-functions-with-ngc-models-resources" title="Permalink to this headline"></a></h3>
<p>When creating a function, <a class="reference external" href="https://docs.nvidia.com/ngc/gpu-cloud/ngc-private-registry-user-guide/index.html#ngc-models">models</a> and <a class="reference external" href="https://docs.nvidia.com/ngc/gpu-cloud/ngc-private-registry-user-guide/index.html#ngc-resources">resources</a> can be mounted to the function instance. The models will be available under <code class="code docutils literal notranslate"><span class="pre">/config/models/{modelName}</span></code> and <code class="code docutils literal notranslate"><span class="pre">/config/resources/{resourceName}</span></code> where <code class="code docutils literal notranslate"><span class="pre">modelName</span></code> and <code class="code docutils literal notranslate"><span class="pre">resourceName</span></code> are specified as part of the API request.</p>
<p>Here is an example where a model and resource are added to a function creation API call, for an echo sample function:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span>curl -X <span class="s1">&#39;POST&#39;</span> <span class="se">\</span>
<span class="linenos"> 2</span>  <span class="s1">&#39;https://api.ngc.nvidia.com/v2/nvcf/functions&#39;</span> <span class="se">\</span>
<span class="linenos"> 3</span>  -H <span class="s1">&#39;Authorization: Bearer $API_KEY&#39;</span> <span class="se">\</span>
<span class="linenos"> 4</span>  -H <span class="s1">&#39;accept: application/json&#39;</span> <span class="se">\</span>
<span class="linenos"> 5</span>  -H <span class="s1">&#39;Content-Type: application/json&#39;</span> <span class="se">\</span>
<span class="linenos"> 6</span>  -d <span class="s1">&#39;{</span>
<span class="linenos"> 7</span><span class="s1">  &quot;name&quot;: &quot;echo_function&quot;,</span>
<span class="linenos"> 8</span><span class="s1">  &quot;inferenceUrl&quot;: &quot;/echo&quot;,</span>
<span class="linenos"> 9</span><span class="s1">  &quot;containerImage&quot;: &quot;nvcr.io/$ORG_NAME/echo:latest&quot;,</span>
<span class="linenos">10</span><span class="s1">  &quot;apiBodyFormat&quot;: &quot;CUSTOM&quot;,</span>
<span class="linenos">11</span><span class="s1">  &quot;models&quot;: [</span>
<span class="linenos">12</span><span class="s1">    {</span>
<span class="linenos">13</span><span class="s1">      &quot;name&quot;: &quot;simple_int8&quot;,</span>
<span class="linenos">14</span><span class="s1">      &quot;version&quot;: &quot;1&quot;,</span>
<span class="linenos">15</span><span class="s1">      &quot;uri&quot;: &quot;v2/org/cf/$ORG_NAME/models/simple_int8/versions/1/zip&quot;</span>
<span class="linenos">16</span><span class="s1">    }</span>
<span class="linenos">17</span><span class="s1">  ],</span>
<span class="linenos">18</span><span class="s1">  &quot;resources&quot;: [</span>
<span class="linenos">19</span><span class="s1">    {</span>
<span class="linenos">20</span><span class="s1">      &quot;name&quot;: &quot;simple_resource&quot;,</span>
<span class="linenos">21</span><span class="s1">      &quot;version&quot;: &quot;1&quot;,</span>
<span class="linenos">22</span><span class="s1">      &quot;uri&quot;: &quot;v2/org/cf/$ORG_NAME/resources/simple_resource/versions/1/zip&quot;</span>
<span class="linenos">23</span><span class="s1">    }</span>
<span class="linenos">24</span><span class="s1">  ]</span>
<span class="linenos">25</span><span class="s1">}&#39;</span>
</pre></div>
</div>
<p>Within the container, once the function instance is deployed, the model would be mounted at <code class="code docutils literal notranslate"><span class="pre">/config/models/simple_int8</span></code> and resource mounted at <code class="code docutils literal notranslate"><span class="pre">/config/resources/simple_int8</span></code></p>
</section>
<section id="creating-grpc-based-functions">
<h3>Creating gRPC-based Functions<a class="headerlink" href="#creating-grpc-based-functions" title="Permalink to this headline"></a></h3>
<p>Cloud Functions supports function invocation via gRPC. During function creation, specify that the function is a gRPC function by setting the “Inference Protocol”, or <code class="docutils literal notranslate"><span class="pre">inferenceUrl</span></code> field to <code class="docutils literal notranslate"><span class="pre">/grpc</span></code>.</p>
<section id="prerequisites">
<h4>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline"></a></h4>
<ul>
<li><p>The function container must implement a gRPC port, endpoint and health check. The health check is expected to be served by the gRPC inference port, there is no need to define a separate health endpoint path.</p>
<blockquote>
<div><ul class="simple">
<li><p>See <a class="reference external" href="https://grpc.io/docs/guides/health-checking/">gRPC health checking</a>.</p></li>
<li><p>See an <a class="reference external" href="https://github.com/NVIDIA/nv-cloud-function-helpers/blob/main/examples/grpc_echo_sample/grpc_echo_server.py">example container</a> with a gRPC server that is Cloud Functions compatible.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</section>
<section id="grpc-function-creation-via-ui">
<h4>gRPC Function Creation via UI<a class="headerlink" href="#grpc-function-creation-via-ui" title="Permalink to this headline"></a></h4>
<p>In the Function Creation Page, set the “Inference Protocol” to <code class="docutils literal notranslate"><span class="pre">gRPC</span></code> and port to whatever your gRPC server has implemented.</p>
<img alt="gRPC Function Creation" class="align-center" src="../_images/grpc-function-creation.png" />
</section>
<section id="grpc-function-creation-via-cli">
<h4>gRPC Function Creation via CLI<a class="headerlink" href="#grpc-function-creation-via-cli" title="Permalink to this headline"></a></h4>
<p>When creating the gRPC function, set the <code class="docutils literal notranslate"><span class="pre">--inference-url</span></code> argument to <code class="docutils literal notranslate"><span class="pre">/grpc</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span> ngc cf <span class="k">function</span> create --inference-port <span class="m">8001</span> --container-image nvcr.io/<span class="nv">$ORG_NAME</span>/grpc_echo_sample:latest --name my-grpc-function --inference-url /grpc
</pre></div>
</div>
</section>
<section id="grpc-function-creation-via-api">
<h4>gRPC Function Creation via API<a class="headerlink" href="#grpc-function-creation-via-api" title="Permalink to this headline"></a></h4>
<p>When creating the gRPC function, set the <code class="docutils literal notranslate"><span class="pre">inferenceURl</span></code> field to <code class="docutils literal notranslate"><span class="pre">/grpc</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span> curl --location <span class="s1">&#39;https://api.ngc.nvidia.com/v2/nvcf/functions&#39;</span> <span class="se">\</span>
<span class="linenos"> 2</span> --header <span class="s1">&#39;Content-Type: application/json&#39;</span> <span class="se">\</span>
<span class="linenos"> 3</span> --header <span class="s1">&#39;Accept: application/json&#39;</span> <span class="se">\</span>
<span class="linenos"> 4</span> --header <span class="s1">&#39;Authorization: Bearer $API_KEY&#39;</span> <span class="se">\</span>
<span class="linenos"> 5</span> --data <span class="s1">&#39;{</span>
<span class="linenos"> 6</span><span class="s1">     &quot;name&quot;: &quot;my-grpc-function&quot;,</span>
<span class="linenos"> 7</span><span class="s1">     &quot;inferenceUrl&quot;: &quot;/grpc&quot;,</span>
<span class="linenos"> 8</span><span class="s1">     &quot;inferencePort&quot;: 8001,</span>
<span class="linenos"> 9</span><span class="s1">     &quot;containerImage&quot;: &quot;nvcr.io/$ORG_NAME/grpc_echo_sample:latest&quot;</span>
<span class="linenos">10</span><span class="s1"> }&#39;</span>
</pre></div>
</div>
</section>
<section id="grpc-function-invocation">
<h4>gRPC Function Invocation<a class="headerlink" href="#grpc-function-invocation" title="Permalink to this headline"></a></h4>
<p>See <a class="reference internal" href="api.html#grpc-invocation"><span class="std std-ref">gRPC Invocation</span></a> for details on how to authenticate and invoke your gRPC function.</p>
</section>
</section>
<section id="available-container-variables">
<h3>Available Container Variables<a class="headerlink" href="#available-container-variables" title="Permalink to this headline"></a></h3>
<p>The following is a reference of available variables via the headers of the invocation message (auto-populated by Cloud Functions), accessible within the container.</p>
<p>For examples of how to extract and use some of these variables, see <a class="reference external" href="https://github.com/NVIDIA/nv-cloud-function-helpers/tree/main">NVCF Container Helper Functions</a>.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 34%" />
<col style="width: 66%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>NVCF-REQID</p></td>
<td><p>Request ID for this request.</p></td>
</tr>
<tr class="row-odd"><td><p>NVCF-SUB</p></td>
<td><p>Message subject.</p></td>
</tr>
<tr class="row-even"><td><p>NVCF-NCAID</p></td>
<td><p>Function’s organization’s NCA ID.</p></td>
</tr>
<tr class="row-odd"><td><p>NVCF-FUNCTION-NAME</p></td>
<td><p>Function name.</p></td>
</tr>
<tr class="row-even"><td><p>NVCF-FUNCTION-ID</p></td>
<td><p>Function ID.</p></td>
</tr>
<tr class="row-odd"><td><p>NVCF-FUNCTION-VERSION-ID</p></td>
<td><p>Function version ID.</p></td>
</tr>
<tr class="row-even"><td><p>NVCF-ASSET-DIR</p></td>
<td><p>Asset directory path. Not available for helm deployments.</p></td>
</tr>
<tr class="row-odd"><td><p>NVCF-LARGE-OUTPUT-DIR</p></td>
<td><p>Large output directory path.</p></td>
</tr>
<tr class="row-even"><td><p>NVCF-MAX-RESPONSE-SIZE-BYTES</p></td>
<td><p>Max response size in bytes for the function.</p></td>
</tr>
<tr class="row-odd"><td><p>NVCF-NSPECTID</p></td>
<td><p>NVIDIA reserved variable.</p></td>
</tr>
<tr class="row-even"><td><p>NVCF-BACKEND</p></td>
<td><p>Backend or “Cluster Group” the function is deployed on.</p></td>
</tr>
<tr class="row-odd"><td><p>NVCF-INSTANCETYPE</p></td>
<td><p>Instance type the function is deployed on.</p></td>
</tr>
<tr class="row-even"><td><p>NVCF-REGION</p></td>
<td><p>Region or zone the function is deployed in.</p></td>
</tr>
<tr class="row-odd"><td><p>NVCF-ENV</p></td>
<td><p>Spot environment if deployed on spot instances.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="adding-partial-response-progress">
<h3>Adding Partial Response (Progress)<a class="headerlink" href="#adding-partial-response-progress" title="Permalink to this headline"></a></h3>
<p>Below are instructions on setting up output directories and efficiently tracking and communicating inferencing progress using Cloud Functions. <strong>This functionality is only supported for container-based functions.</strong></p>
<ul class="simple">
<li><p>Cloud Functions automatically configures the output directory for you. To access the path, simply read the <code class="docutils literal notranslate"><span class="pre">NVCF-LARGE-OUTPUT-DIR</span></code> header. <code class="docutils literal notranslate"><span class="pre">NVCF-LARGE-OUTPUT-DIR</span></code> points to the directory for that particular <code class="docutils literal notranslate"><span class="pre">requestId</span></code>.</p></li>
<li><p>To enable partial progress reporting, you will need to store partial and completed outputs, and create a <code class="docutils literal notranslate"><span class="pre">progress</span></code> file in the output directory.</p></li>
<li><p>Once the output file and progress file are correctly set up in the output directory under the correct request id, Cloud Functions will automatically detect them.</p></li>
<li><p>When using the <a class="reference internal" href="api.html#polling"><span class="std std-ref">invocation API to poll for a response</span></a>, <code class="docutils literal notranslate"><span class="pre">progress</span></code> will be returned as the header <code class="docutils literal notranslate"><span class="pre">NVCF-PERCENT-COMPLETE</span></code>, along with any partial response data.</p></li>
</ul>
<section id="storing-partial-and-complete-outputs">
<h4>Storing Partial and Complete Outputs<a class="headerlink" href="#storing-partial-and-complete-outputs" title="Permalink to this headline"></a></h4>
<ol class="arabic simple">
<li><p>When your Custom <a class="reference external" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/python_backend/README.html#business-logic-scripting">BLS</a> generates large outputs, save them temporarily with the “*.partial” extension inside the <code class="docutils literal notranslate"><span class="pre">NVCF-LARGE-OUTPUT-DIR</span></code> directory. For instance, if you’re writing an image, name it <code class="docutils literal notranslate"><span class="pre">image1.partial</span></code>.</p></li>
<li><p>Once the writing of the output file is complete, rename it from “*.partial” to its appropriate extension. Continuing with our example, rename <code class="docutils literal notranslate"><span class="pre">image1.partial</span></code> to <code class="docutils literal notranslate"><span class="pre">image1.jpg</span></code>.</p></li>
</ol>
</section>
<section id="creating-a-progress-file">
<h4>Creating a Progress File<a class="headerlink" href="#creating-a-progress-file" title="Permalink to this headline"></a></h4>
<p>Cloud Functions actively observes the output directory for a file named <code class="docutils literal notranslate"><span class="pre">progress</span></code>. This file is used to communicate progress and partial responses back to the caller.</p>
<p>This file should contain well-formed JSON data. Structure the JSON content as follows:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="p">{</span><span class="w"></span>
<span class="linenos">2</span><span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;{requestId}&quot;</span><span class="p">,</span><span class="w"></span>
<span class="linenos">3</span><span class="w">   </span><span class="nt">&quot;progress&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span><span class="w"></span>
<span class="linenos">4</span><span class="w">   </span><span class="nt">&quot;partialResponse&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="linenos">5</span><span class="w">      </span><span class="nt">&quot;exampleKey&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Insert any well-formed JSON here, but ensure its size is less than 250K&quot;</span><span class="w"></span>
<span class="linenos">6</span><span class="w">   </span><span class="p">}</span><span class="w"></span>
<span class="linenos">7</span><span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>Replace <code class="docutils literal notranslate"><span class="pre">requestId</span></code> with the actual request id if it’s present. Modify the progress integer as needed, ranging from 0 (just started) to 100 (fully complete). Within <code class="docutils literal notranslate"><span class="pre">partialResponse</span></code>, insert any JSON content you want to send as a partial response, making sure it’s smaller than 250KB.</p>
</section>
<section id="best-practices">
<h4>Best Practices<a class="headerlink" href="#best-practices" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><p>Always use the “.partial” extension to avoid sending partial or incomplete data.</p></li>
<li><p>Rename to the final extension only when the writing process is fully complete.</p></li>
<li><p>Ensure your progress file remains under 250KB to maintain efficiency and avoid errors.</p></li>
</ul>
</section>
</section>
</section>
<section id="helm-based-function-creation">
<span id="helm-functions"></span><h2>Helm-Based Function Creation<a class="headerlink" href="#helm-based-function-creation" title="Permalink to this headline"></a></h2>
<p>Cloud functions support helm-based functions for orchestration across multiple containers.</p>
<section id="helm-prereq">
<span id="id10"></span><h3>Prerequisites<a class="headerlink" href="#helm-prereq" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>The helm chart <strong>must have a “mini-service” container defined, which will be used as the inference entry point.</strong></p></li>
<li><p>The name of this service in your helm chart should be supplied by setting <code class="docutils literal notranslate"><span class="pre">helmChartServiceName</span></code> during the function definition. This allows Cloud Functions to communicate and make inference requests to the “mini-service” endpoint.</p></li>
</ol>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>The <code class="docutils literal notranslate"><span class="pre">servicePort</span></code> defined within the helm chart should be used as the <code class="docutils literal notranslate"><span class="pre">inferencePort</span></code> supplied during function creation. Otherwise, Cloud Functions will not be able to reach the “mini-service”.</p>
</div>
<ol class="arabic simple" start="3">
<li><p>Ensure you have the <a class="reference internal" href="#ngc-cli-setup"><span class="std std-ref">NGC CLI configured</span></a> and have pushed your helm chart to NGC Private Registry. Refer to <a class="reference external" href="https://docs.nvidia.com/ngc/gpu-cloud/ngc-private-registry-user-guide/index.html#managing-helm-charts-using-ngc-cli">Managing Helm Charts Using the NGC CLI</a>.</p></li>
</ol>
</section>
<section id="secret-management">
<h3>Secret Management<a class="headerlink" href="#secret-management" title="Permalink to this headline"></a></h3>
<p>For pulling containers defined as part of the helm chart from NGC Private Registry, a new value named <code class="docutils literal notranslate"><span class="pre">ngcImagePullSecretName</span></code> needs to be defined in the chart.The value is referred to in deployment spec as <code class="docutils literal notranslate"><span class="pre">spec.imagePullSecrets.name</span></code> of pods in the chart.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Containers defined in the helm chart should be in the same NGC Organization and Team that the helm chart itself is being pulled from.</p>
</div>
</section>
<section id="create-a-helm-based-function">
<h3>Create a Helm-based Function<a class="headerlink" href="#create-a-helm-based-function" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>Ensure your helm chart is uploaded to NGC Private Registry and adheres to the <a class="reference internal" href="#helm-prereq"><span class="std std-ref">Prerequisites</span></a> listed above.</p></li>
<li><p>Create the function:</p>
<ul>
<li><p>Include the following additional parameters in the function definition</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">helmChart</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">helmChartServiceName</span></code></p></li>
</ul>
</div></blockquote>
</li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">helmChart</span></code> property should be set to the URL hosted by the NGC Model Registry pointing to the helm chart that will deploy the “mini-service”. Please note, that this helm chart URL should be accessible to the NGC org in which the function will eventually be deployed. The helm chart URL should follow the format: <code class="docutils literal notranslate"><span class="pre">https://helm.ngc.nvidia.com/$ORG_ID/$TEAM_NAME/charts/$NAME-X.Y.Z.tgz</span></code> for example, <code class="docutils literal notranslate"><span class="pre">https://helm.ngc.nvidia.com/abc123/teamA/charts/nginx-0.1.5.tgz</span></code> would be a valid chart URL but <code class="docutils literal notranslate"><span class="pre">https://helm.ngc.nvidia.com/abc123/teamA/charts/nginx-0.1.5-hello.tgz</span></code> would not.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">helmChartServiceName</span></code> is used for checking if the “mini-service” is ready for inference and is also scraped for function metrics. At this time, templatized service names are not supported. <strong>This must match the service name of your “mini-service” with the exposed entry point port.</strong></p></li>
<li><p>Important: The Helm chart name should not contain underscores or other special symbols, as that may cause issues during deployment.</p></li>
</ul>
</li>
</ol>
<p><strong>Example Creation via API</strong></p>
<p>Please see our <a class="reference external" href="https://github.com/NVIDIA/nv-cloud-function-helpers/tree/main/examples/helmchart_echo_sample">sample helm chart used</a> in this example for reference.</p>
<p>Below is an example function creation API call creating a helm-based function:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span>curl -X <span class="s1">&#39;POST&#39;</span> <span class="se">\</span>
<span class="linenos"> 2</span>    <span class="s1">&#39;https://api.ngc.nvidia.com/v2/nvcf/functions&#39;</span> <span class="se">\</span>
<span class="linenos"> 3</span>    -H <span class="s1">&#39;Authorization: Bearer $API_KEY&#39;</span> <span class="se">\</span>
<span class="linenos"> 4</span>    -H <span class="s1">&#39;accept: application/json&#39;</span> <span class="se">\</span>
<span class="linenos"> 5</span>    -H <span class="s1">&#39;Content-Type: application/json&#39;</span> <span class="se">\</span>
<span class="linenos"> 6</span>    -d <span class="s1">&#39;{</span>
<span class="linenos"> 7</span><span class="s1">    &quot;name&quot;: &quot;function_name&quot;,</span>
<span class="linenos"> 8</span><span class="s1">    &quot;inferenceUrl&quot;: &quot;v2/models/model_name/versions/model_version/infer&quot;,</span>
<span class="linenos"> 9</span><span class="s1">    &quot;inferencePort&quot;: 8001,</span>
<span class="linenos">10</span><span class="s1">    &quot;helmChart&quot;: &quot;https://helm.ngc.nvidia.com/$ORG_ID/$TEAM_NAME/charts/inference-test-1.0.tgz&quot;,</span>
<span class="linenos">11</span><span class="s1">    &quot;helmChartServiceName&quot;: &quot;service_name&quot;,</span>
<span class="linenos">12</span><span class="s1">    &quot;apiBodyFormat&quot;: &quot;CUSTOM&quot;</span>
<span class="linenos">13</span><span class="s1">}&#39;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For gRPC-based functions, set <code class="docutils literal notranslate"><span class="pre">&quot;inferenceURL&quot;</span> <span class="pre">:</span> <span class="pre">&quot;/gRPC&quot;</span></code>. This signals to Cloud Functions that the function is using gRPC protocol and is not expected to have a <code class="docutils literal notranslate"><span class="pre">/gRPC</span></code> endpoint exposed for inferencing requests.</p>
</div>
<ol class="arabic simple" start="3">
<li><p>Proceed with function deployment and invocation normally.</p></li>
</ol>
</section>
<section id="limitations">
<h3>Limitations<a class="headerlink" href="#limitations" title="Permalink to this headline"></a></h3>
<dl>
<dt>When using helm charts, the following limitations need to be taken into consideration</dt><dd><ul>
<li><p>Automatic mounting of NGC Models and Resources for your container is not supported.</p></li>
<li><p>For any downloads (such as of assets or models) occurring within your function’s containers, download size is limited by the disk space on the VM - for GFN this is 100GB approximately, and for other clusters this limit will vary.</p></li>
<li><p>Progress/partial response reporting is not supported, including any additional artifacts generated during inferencing. Consider opting for HTTP streaming or gRPC bidirectional support.</p></li>
<li><p>Supported k8s artifacts under Helm Chart Namespace are listed below. Others will be rejected:</p>
<blockquote>
<div><ul class="simple">
<li><p>Deployment</p></li>
<li><p>Service</p></li>
<li><p>ServiceAccount</p></li>
<li><p>Role &amp; RoleBindings</p></li>
<li><p>ConfigMaps</p></li>
<li><p>Secrets</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</dd>
</dl>
</section>
<section id="helm-chart-overrides">
<h3>Helm Chart Overrides<a class="headerlink" href="#helm-chart-overrides" title="Permalink to this headline"></a></h3>
<p>To override keys in your helm chart <code class="docutils literal notranslate"><span class="pre">values.yml</span></code>, you can provide the <code class="docutils literal notranslate"><span class="pre">configuration</span></code> parameter and supply corresponding key-value pairs in JSON format which you would like to be overridden when the function is deployed.</p>
<div class="literal-block-wrapper docutils container" id="id18">
<div class="code-block-caption"><span class="caption-text">Example helm chart override</span><a class="headerlink" href="#id18" title="Permalink to this code"></a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span>curl -X <span class="s1">&#39;POST&#39;</span> <span class="se">\</span>
<span class="linenos"> 2</span> <span class="s1">&#39;https://api.ngc.nvidia.com/v2/nvcf/deployments/functions/fe6e6589-12bb-423a-9bf6-8b9d028b8bf4/versions/fe6e6589-12bb-423a-9bf6-8b9d028b8bf4&#39;</span> <span class="se">\</span>
<span class="linenos"> 3</span> -H <span class="s1">&#39;Authorization: Bearer $API_KEY&#39;</span> <span class="se">\</span>
<span class="linenos"> 4</span> -H <span class="s1">&#39;accept: application/json&#39;</span> <span class="se">\</span>
<span class="linenos"> 5</span> -H <span class="s1">&#39;Content-Type: application/json&#39;</span> <span class="se">\</span>
<span class="linenos"> 6</span> -d <span class="s1">&#39;{</span>
<span class="linenos"> 7</span><span class="s1">     &quot;deploymentSpecifications&quot;: [{</span>
<span class="linenos"> 8</span><span class="s1">         &quot;gpu&quot;: &quot;L40&quot;,</span>
<span class="linenos"> 9</span><span class="s1">         &quot;backend&quot;: &quot;OCI&quot;,</span>
<span class="linenos">10</span><span class="s1">         &quot;maxInstances&quot;: 2,</span>
<span class="linenos">11</span><span class="s1">         &quot;minInstances&quot;: 1,</span>
<span class="linenos">12</span><span class="s1">         &quot;configuration&quot;: {</span>
<span class="linenos">13</span><span class="s1">         &quot;key_one&quot;: &quot;&lt;value&gt;&quot;,</span>
<span class="linenos">14</span><span class="s1">         &quot;key_two&quot;: { &quot;key_two_subkey_one&quot;: &quot;&lt;value&gt;&quot;, &quot;key_two_subkey_two&quot;: &quot;&lt;value&gt;&quot; }</span>
<span class="linenos">15</span><span class="s1">     ...</span>
<span class="linenos">16</span><span class="s1">     },</span>
<span class="linenos">17</span><span class="s1">     {</span>
<span class="linenos">18</span><span class="s1">         &quot;gpu&quot;: &quot;T10&quot;,</span>
<span class="linenos">19</span><span class="s1">         &quot;backend&quot;: &quot;GFN&quot;,</span>
<span class="linenos">20</span><span class="s1">         &quot;maxInstances&quot;: 2,</span>
<span class="linenos">21</span><span class="s1">         &quot;minInstances&quot;: 1</span>
<span class="linenos">22</span><span class="s1">     }]</span>
<span class="linenos">23</span><span class="s1"> }&#39;</span>
</pre></div>
</div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
<img src="../_static/NVIDIA-LogoBlack.svg" class="only-light"/>
<img src="../_static/NVIDIA-LogoWhite.svg" class="only-dark"/>

<p class="notices">
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">Privacy Policy</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank">Manage My Privacy</a>
|
<a href="https://www.nvidia.com/en-us/preferences/start/" target="_blank">Do Not Sell or Share My Data</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">Terms of Service</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">Accessibility</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">Corporate Policies</a>
|
<a href="https://www.nvidia.com/en-us/product-security/" target="_blank">Product Security</a>
|
<a href="https://www.nvidia.com/en-us/contact/" target="_blank">Contact</a>
</p>

<p>
  Copyright &#169; 2024, NVIDIA Corporation.
</p>

    <p>
      <span class="lastupdated">Last updated on Aug 30, 2024.
      </span></p>

  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 

    <script type="text/javascript">if (typeof _satellite !== "undefined") {_satellite.pageBottom();}</script>
    



</body>
</html>