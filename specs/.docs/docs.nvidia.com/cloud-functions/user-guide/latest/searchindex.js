const searchData = {"envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 5, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "repo_docs.ext.toctree": 2, "repo_docs.ext.mermaid": 1, "repo_docs.ext.enhanced_search": 2, "sphinx": 56}, "data": [{"id": 0, "doc_id": 1, "filename": "cloud-function/account-access.html", "domain_name": "page", "name": "cloud-function/account-access#login-as-the-organization-owner", "display_name": "Login as the Organization Owner", "type": "section", "display_type": "Page section", "docname": "cloud-function/account-access", "anchor": "login-as-the-organization-owner", "priority": -1, "content": "The organization owner will receive an automated invite that looks like the following: Clicking the button to log in will take you to a page that looks like the following. From here please click \u201cCreate NVIDIA Cloud Account\u201d. The next step will then ask you to enter an account name for your NCA. This account name will be linked to your NGC organization. After completing Step 3 you will be redirected to your new NGC organization. Access Cloud Functions on the upper left navigation. Access your Organization Profile page, which includes your NGC organization details including your NVIDIA Cloud Account (NCA) ID by clicking \u201cOrganization\u201d - \u201cOrganization Profile\u201d from the navigation on the top right. You will receive another email asking to log in to finish setup. This step activates your NVIDIA Cloud Account for future use. Click the button to login.", "keywords": []}, {"id": 1, "doc_id": 1, "filename": "cloud-function/account-access.html", "domain_name": "std", "name": "cloud-function/account-access", "display_name": "NGC Account Access", "type": "doc", "display_type": "Page", "docname": "cloud-function/account-access", "anchor": "", "priority": -1, "content": "NVIDIA Cloud Functions is hosted on NVIDIA\u2019s NGC platform. This overview gives a quick run down on how to log in to your account the first time. For detailed instructions on adding and managing users within your organization as the organization owner, please see the NGC User Guide . When you are invited to an NGC organization with the Cloud Functions product enabled on it, your Cloud Functions account will be tied to both an NGC organization, and an NCA (NVIDIA Cloud Account) ID.", "keywords": []}, {"id": 2, "doc_id": 18, "filename": "cloud-function/api.html", "domain_name": "page", "name": "cloud-function/api#advantages-of-http-streaming", "display_name": "Advantages of HTTP Streaming", "type": "section", "display_type": "Page section", "docname": "cloud-function/api", "anchor": "advantages-of-http-streaming", "priority": -1, "content": "Reduces latency: Clients receive data as it becomes available. Lowers overhead: Eliminates the need for repeated polling requests. Flexibility: The inference container controls if the response will be streamed or not, allowing the client-side implementation to remain consistent regardless of server-side changes. This feature introduces the possibility of long-lived \u201cblocking\u201d client requests, which the system must manage efficiently, especially during a shutdown sequence.", "keywords": []}, {"id": 3, "doc_id": 18, "filename": "cloud-function/api.html", "domain_name": "page", "name": "cloud-function/api#api-key-scopes-and-domains", "display_name": "API Key Scopes and Domains", "type": "section", "display_type": "Page section", "docname": "cloud-function/api", "anchor": "api-key-scopes-and-domains", "priority": -1, "content": "There are multiple API key types within NGC. We strongly recommend using the NGC Personal API Key for complete NVCF API compatibility. Required domain names are documented below and are also pre-filled within our Postman Collection . Our OpenAPI Spec also describes the scopes required for each endpoint. Scope Name Domain Name API Category update_function https://api.ngc.nvidia.com Function Management register_function https://api.ngc.nvidia.com Function Management queue_details https://api.nvcf.nvidia.com Queue Details list_functions https://api.nvcf.nvidia.com Function Management list_cluster_groups https://api.ngc.nvidia.com Cluster Groups and GPUs invoke_function https://api.nvcf.nvidia.com Function Invocation and Asset Management deploy_function https://api.ngc.nvidia.com Function Deployment delete_function https://api.ngc.nvidia.com Function Management", "keywords": []}, {"id": 4, "doc_id": 18, "filename": "cloud-function/api.html", "domain_name": "page", "name": "cloud-function/api#authorization", "display_name": "Authorization", "type": "section", "display_type": "Page section", "docname": "cloud-function/api", "anchor": "authorization", "priority": -1, "content": "The NVCF API supports NGC API key-based authorization for calling the API directly, or indirectly via the NGC CLI and NGC SDK . The generated NGC Personal API Key will also be used for pushing and pulling containers, models, resources and helm charts to the NGC Private Registry to use during function creation.", "keywords": []}, {"id": 5, "doc_id": 18, "filename": "cloud-function/api.html", "domain_name": "page", "name": "cloud-function/api#common-function-invocation-errors", "display_name": "Common Function Invocation Errors", "type": "section", "display_type": "Page section", "docname": "cloud-function/api", "anchor": "common-function-invocation-errors", "priority": -1, "content": "Failure Type Description Invocation response returning 4xx or 5xx status code Check the \u201ctype\u201d of the error message response, if the type includes inference-service this indicates the error is coming from your inference container. Please check the OpenAPI Specification and for other possible status code failure reasons in the case where they are not generated from your inference container. Invocation request taking a long to get a result Check the capacity of your function using the Function Metrics UI or API, to see if your function is queuing. Consider instrumenting your container with additional metrics to your chosen monitoring solution for further debugging - NVCF containers allow public egress. Set NVCF-POLL-SECONDS header to 300 (maximum) to wait for a sync response for up to 20 min to rule out errors in your client\u2019s polling logic. Invocation response returning 401 or 403 This indicates that the caller is unauthorized, ensure the Authorization header is set correctly with a valid API Key. Container OOM This is difficult to detect without instrumenting your container with additional metrics unless your container is emitting logs that indicate out of memory. We recommend profiling the memory usage locally. For testing locally and in the function, you can look at a profile of the memory allocation using this guide.", "keywords": []}, {"id": 6, "doc_id": 18, "filename": "cloud-function/api.html", "domain_name": "page", "name": "cloud-function/api#generate-an-ngc-personal-api-key", "display_name": "Generate an NGC Personal API Key", "type": "section", "display_type": "Page section", "docname": "cloud-function/api", "anchor": "generate-an-ngc-personal-api-key", "priority": -1, "content": "The API Key can be generated via your account in the Personal Keys Page . It\u2019s recommended that the API Key that you generate includes both Cloud Functions and Private Registry scopes for seamless usage with the NGC CLI . API Key scopes are static. This means if the key is lost, it must be destroyed and recreated. For more information about NGC API Key management see the NGC API Key documentation . API Key Usage The API Key is passed in the Authorization header. Authorization: Bearer $API_KEY", "keywords": []}, {"id": 7, "doc_id": 18, "filename": "cloud-function/api.html", "domain_name": "page", "name": "cloud-function/api#grpc", "display_name": "gRPC", "type": "section", "display_type": "Page section", "docname": "cloud-function/api", "anchor": "grpc", "priority": -1, "content": "Users can invoke functions by including their authentication information and a specific function ID in the gRPC metadata. The data being transmitted is generic and based on Protobuf messages. Each model or container will have its own unique API, defined by the Protobuf messages it implements. gRPC connections will be kept alive for 30 seconds if idle, this is not configurable. gRPC functions have no input request size limit. Proxy Host &amp; Endpoint The gRPC proxy host is grpc.nvcf.nvidia.com:443 . Use this host when calling your gRPC endpoint. The Cloud Functions gRPC proxy will attempt to open a connection to your function instance for 30 seconds before timeout. API Key &amp; Metadata Keys Set your API Key as Call Credentials. Either use gRPC\u2019s support for Call Credentials , sometimes called Per RPC Credentials, to pass the API Key or manually set the authorization metadata as Bearer $API_Key . Set the function-id metadata key. Optionally, set the function-version-id metadata key. When the client is finished making gRPC calls close the gRPC client connection so that you do not tie up your function\u2019s workers longer than needed. Example See a complete grpc server and client example in our example containers repository. def call_grpc( create_grpc_function: CreateFunctionResponse, # function def info ) -&gt; None: channel = grpc.secure_channel(&quot;grpc.nvcf.nvidia.com:443&quot;, grpc.ssl_channel_credentials()) # proto generated grpc client grpc_client = grpc_service_pb2_grpc.GRPCInferenceServiceStub(channel) function_id = create_grpc_function.function.id function_version_id = create_grpc_function.function.version_id apiKey = &quot;$API_KEY&quot; metadata = [(&quot;function-id&quot;, function_id), # required (&quot;function-version-id&quot;, function_version_id), # optional (&quot;authorization&quot;, &quot;Bearer &quot; + apiKey)] # required # make 100 unary inferences in a row for i in range(ITERATIONS): # this would be your client, request, and body. # it does not have any proto def restriction. infer = grpc_client.ModelInfer(MODEL_INFER_REQUEST, metadata=metadata) _ = infer logging.info(f&quot;finished invoking {ITERATIONS} times&quot;) The official term for authorization handling using gRPC is \u201cCall Credentials\u201d. More details can be found at grpc.io documentation on credential types . The Python example provided does not showcase this. Instead, it demonstrates manually setting the \u201cauthorization\u201d with an API Key. Using call credentials would implicitly handle this.", "keywords": []}, {"id": 8, "doc_id": 18, "filename": "cloud-function/api.html", "domain_name": "page", "name": "cloud-function/api#http-polling", "display_name": "HTTP (Polling)", "type": "section", "display_type": "Page section", "docname": "cloud-function/api", "anchor": "http-polling", "priority": -1, "content": "NVCF employs long polling for function invocation and result retrieval. However, the invocation API can be used as a synchronous, blocking API up to the max timeout of 20 minutes. The polling response timeout is set to 1 minute by default, and configurable up to 20 minutes via setting the HTTP header NVCF-POLL-SECONDS on the request, refer to the API documentation. When you make a function invocation request, NVCF will hold your request open for the polling response period before returning with either: HTTP Status 200 completed result HTTP Status 202 polling response On receipt of a polling response your client should immediately poll NVCF to retrieve your result. Example Below is an invocation of the \u201cecho\u201d function built from any of the \u201cecho\u201d containers from the examples repository . curl --location &#x27;https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/{functionId}&#x27; \\ --header &#x27;Content-Type: application/json&#x27; \\ --header &#x27;Authorization: Bearer $API_KEY&#x27; \\ --data &#x27;{ &quot;inputs&quot;: [ { &quot;name&quot;: &quot;message&quot;, &quot;shape&quot;: [ 1 ], &quot;datatype&quot;: &quot;BYTES&quot;, &quot;data&quot;: [ &quot;Hello&quot; ] }, { &quot;name&quot;: &quot;response_delay_in_seconds&quot;, &quot;shape&quot;: [ 1 ], &quot;datatype&quot;: &quot;FP32&quot;, &quot;data&quot;: [ 0.1 ] } ], &quot;outputs&quot;: [ { &quot;name&quot;: &quot;echo&quot;, &quot;datatype&quot;: &quot;BYTES&quot;, &quot;shape&quot;: [ 1 ] } ] }&#x27; In the event, that NVCF responds erroneously your client should but is not required to protect itself by ensuring it does not make more than one polling request per second. This can be achieved by keeping a start_time when you make a polling request and sleeping for up to 1 second from that start_time before making another request. This does not mean your client should always be adding a sleep. For example, if 0.1 seconds have passed since making your last polling request your client should sleep for 0.9 seconds. If 5 seconds have passed since making your last polling request your client should not sleep.", "keywords": []}, {"id": 9, "doc_id": 18, "filename": "cloud-function/api.html", "domain_name": "page", "name": "cloud-function/api#http-streaming", "display_name": "HTTP Streaming", "type": "section", "display_type": "Page section", "docname": "cloud-function/api", "anchor": "http-streaming", "priority": -1, "content": "This feature allows clients to receive data as an event stream, eliminating the need for polling, or making repeated requests to check for new data. The server sends events to the client over a long-lived connection, allowing the client to receive updates in real-time. Note that HTTP streaming requests use the same invocation API endpoint. Prerequisites Cloud function deployed on NVCF Familiarity with the basic HTTP pexec invocation API HTTP (Polling) usage documented above Understanding of Server-Sent Events (SSE) Client Configuration The client initiates a connection by making a POST request to the NVCF pexec invocation API endpoint, including the header Accept: text/event-stream . curl --request POST \\ --url https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/eb1100de-60bf-4e9a-8617-b7d4652e0c37 \\ --header &#x27;Authorization: Bearer $API_KEY&#x27; \\ --header &#x27;Accept: text/event-stream&#x27; \\ --header &#x27;Content-Type: application/json&#x27; \\ --data &#x27;{ &quot;messages&quot;: [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello&quot; } ], &quot;temperature&quot;: 0.2, &quot;top_p&quot;: 0.7, &quot;max_tokens&quot;: 512 }&#x27; Upon receiving this request with the appropriate header, NVCF knows the client is prepared to receive streamed data. Handling Server Responses If the response from the inference container includes the header Content-Type: text/event-stream , the client keeps the connection open to the API and listens for data. The NVCF worker will read events from the inference container for up to a default of 20 minutes until the inference container closes the connection, whichever is earlier. Do not create an infinite event stream. Even if the client disconnects, the worker will still read events, which can tie up your function until the worker stops reading as described above, or the request times out. Data read from the inference container\u2019s response is buffered by the event and sent as an in-progress response to the NVCF API. Smaller events are more interactive, but they shouldn\u2019t be too small. If they are, the size of the event stream wrapper might exceed the actual data, causing increased data transmission for your clients. The maximum event size allowed is 4MB. This process continues until the stream is complete. Example See the streaming container and client in our example containers repository. Shutdown Behavior During a graceful shutdown, the NVCF API waits for all ongoing event stream requests to complete. There\u2019s a 5-minute global request timeout for these event stream requests.", "keywords": []}, {"id": 10, "doc_id": 18, "filename": "cloud-function/api.html", "domain_name": "page", "name": "cloud-function/api#inference-container-status-codes-and-responses", "display_name": "Inference Container Status Codes and Responses", "type": "section", "display_type": "Page section", "docname": "cloud-function/api", "anchor": "inference-container-status-codes-and-responses", "priority": -1, "content": "Error messages generated in your inference endpoint response are propagated from your inference container. Here is an example: { &quot;type&quot;: &quot;urn:inference-service:problem-details:bad-request&quot;, &quot;title&quot;: &quot;Bad Request&quot;, &quot;status&quot;: 400, &quot;detail&quot;: &quot;invalid datatype for input message&quot;, &quot;instance&quot;: &quot;/v2/nvcf/pexec/functions/{functionId}&quot;, &quot;requestId&quot;: &quot;{requestId}&quot; } Error responses are formed as follows: The type field in an error response will always include inference-service if the error is originating from your inference container. The response status code will be set to the status code your inference container returns. This is what is returned in the status and title fields as well. The instance and requestId fields are autofilled by the worker. The detail field includes the error message body that your inference container returns. Setting the Error Detail Field Your inference container error response format must return JSON and must set the  error  field : { &quot;error&quot;: &quot;put your error here&quot; } If this field is not set, the detail field in any error responses will be set to a generic Inference error string. It\u2019s highly encouraged to emit logs from your inference container. See Logging and Metrics for setting and viewing logs within the Cloud Functions UI.", "keywords": []}, {"id": 11, "doc_id": 18, "filename": "cloud-function/api.html", "domain_name": "page", "name": "cloud-function/api#jwt-based-authorization", "display_name": "JWT Based Authorization", "type": "section", "display_type": "Page section", "docname": "cloud-function/api", "anchor": "jwt-based-authorization", "priority": -1, "content": "The NVCF API also supports JWT-based authorization for all endpoints. This is managed via the creation of a Service Account Client associated with your organization. A signed JWT issued by the NVCF Service Account authorizes against the Cloud Functions API. This token is obtained by posting the Client\u2019s clientId and secret along with the required scopes to the Service Account. When the token expires, the client application will need to generate a new token. Secrets also expire on a regular cadence and require rotation. This type of authorization requires additional maintenance due to token generation and key rotation. Speak with your Account Manager if you need access to a Service Account Client for JWT-based authorization. JWT Token Generation When generating a token, the client application can specify only the scopes that are needed to perform the desired operations, limiting the \u201cblast radius\u201d if the token is leaked. The token will expire by default within 15 minutes. The token generation endpoint jwtTokenProvider , clientId , and secret will be shared with you during the Service Account Client setup. It\u2019s a best practice to only request the scopes your client needs each time when generating a token. See API Key Scopes and Domains for all available scopes. The Authorization header is a base64 encoded string of the following format: clientId:secret Here is an example of generating a token: curl --location &#x27;https://{jwtTokenProvider}.ssa.nvidia.com/token&#x27; \\ --header &#x27;Content-Type: application/x-www-form-urlencoded&#x27; \\ --header &#x27;Authorization: Basic &lt;Base64 encoded &quot;{clientId}:{secret}&quot;&gt;&#x27; \\ --data-urlencode &#x27;scope=register_function&#x27; \\ --data-urlencode &#x27;grant_type=client_credentials&#x27; Response: { &quot;access_token&quot;: &quot;&lt;generated $JWT_Token&gt;&quot;, &quot;token_type&quot;: &quot;bearer&quot;, &quot;expires_in&quot;: 3600, &quot;scope&quot;: &quot;register_function&quot; } JWT Token Usage The JWT token is passed in the Authorization header. Authorization: Bearer $JWT_Token", "keywords": []}, {"id": 12, "doc_id": 18, "filename": "cloud-function/api.html", "domain_name": "page", "name": "cloud-function/api#large-responses-302-status-code", "display_name": "Large Responses (302 Status Code)", "type": "section", "display_type": "Page section", "docname": "cloud-function/api", "anchor": "large-responses-302-status-code", "priority": -1, "content": "The result payload size may not exceed 5GB. If your payload exceeds 5MB, i.e. 5MB &lt; result size &lt; 5GB, you will receive a reference in the response to download the payload. When using the pexec invocation API, either during the initial invocation API call or when polling ( GET /v2/nvcf/pexec/status/{requestId} ), this will be indicated by a response with the HTTP 302 status code. The Location response header will contain the fully-qualified endpoint, there will be no response body. Your client should be configured to make a new HTTP request to the URL given in the Location response header. The new HTTP request must include an Authorization request header. The result retrieval URL\u2019s Time-To-Live (TTL) is 24 hours. To read more about assets, refer to the Assets API and the asset flow example .", "keywords": []}, {"id": 13, "doc_id": 18, "filename": "cloud-function/api.html", "domain_name": "page", "name": "cloud-function/api#nvcf-api-status-codes", "display_name": "NVCF API Status Codes", "type": "section", "display_type": "Page section", "docname": "cloud-function/api", "anchor": "nvcf-api-status-codes", "priority": -1, "content": "Please refer to the OpenAPI Docs for other possible status code failure reasons in cases where they are not generated from your inference container. For Function States, see Function Lifecycle . To easily differentiate between errors originating from within NVCF\u2019s API or control plane, and your own inference container, determine if the type field includes inference-service (indicating the error is from your inference container)", "keywords": []}, {"id": 14, "doc_id": 18, "filename": "cloud-function/api.html", "domain_name": "page", "name": "cloud-function/api#openapi-specification", "display_name": "OpenAPI Specification", "type": "section", "display_type": "Page section", "docname": "cloud-function/api", "anchor": "openapi-specification", "priority": -1, "content": "This page is a brief overview of using the NVCF API and does not cover all endpoints. Please refer to the OpenAPI Spec for the latest API information. The quickest way to begin using NVCF APIs is via the Postman Collection . The NVCF API is divided into the following sets of APIs: APIs Usage Function Invocation Execution of a function that runs on a worker node. Usually an inference call. Asset Management Used to manage large files for uploading for a requested and downloading results of a function. Cluster Groups &amp; GPUs Defines endpoints to list Cluster Groups and GPUs as targets for function deployment. Queue Details Used to view information about your environment such as queues &amp; GPUs. Function Management The creation, modification and deletion of functions Function Deployment Endpoints for creating and managing function deployments. API Versioning All API endpoints include versioning in the path prefix. /v2/nvcf", "keywords": []}, {"id": 15, "doc_id": 18, "filename": "cloud-function/api.html", "domain_name": "page", "name": "cloud-function/api#polling-after-initial-invocation", "display_name": "Polling After Initial Invocation", "type": "section", "display_type": "Page section", "docname": "cloud-function/api", "anchor": "polling-after-initial-invocation", "priority": -1, "content": "When making a function invocation request using the pexec endpoint, and an HTTP Status 202 is returned, the following headers will be included in the response: NVCF-REQID : Invocation request ID, referred to as requestId NVCF-STATUS : Invocation status NVCF-PERCENT-COMPLETE : Percentage complete The client is then expected to poll for a response using the requestId . Example curl --location &#x27;https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/{requestId}&#x27; \\ --header &#x27;Authorization: Bearer $API_KEY&#x27; Endpoint : GET /v2/nvcf/pexec/status/{requestId} Headers : NVCF-POLL-SECONDS (optional): HTTP polling response timeout, if other than default, in seconds Parameters : requestId (path, required): Function invocation request id, string($uuid) Responses : 200 : Invocation is fulfilled. The response body will be a passthrough of the response returned from your container. 202 : Result is pending. The client should continue to poll using the returned request ID. 302 : In this case, the result is in a different region or is a large response. The client should use the fully-qualified endpoint specified in the Location response header to fetch the result. The client can use the same API Key in the Authorization header when retrieving the result from the redirected region.", "keywords": []}, {"id": 16, "doc_id": 18, "filename": "cloud-function/api.html", "domain_name": "page", "name": "cloud-function/api#statuses-and-errors", "display_name": "Statuses and Errors", "type": "section", "display_type": "Page section", "docname": "cloud-function/api", "anchor": "statuses-and-errors", "priority": -1, "content": "Below is the list of statuses and error codes the API can produce. Function Invocation Response Status If the client receives an HTTP Status code 202 from a pexec invocation API call, the client is expected to poll or issue a GET request using this NVCF-REQID defined in the header. The NVCF-STATUS header can have the following values: pending-evaluation - The worker has not yet accepted the request. fulfilled - The process has been completed with results. rejected - The request was rejected by the service. errored - An error occurred during worker processing. in-progress - A worker is processing the request. Statuses fulfilled , rejected and errored are completed states, and you should not continue to poll.", "keywords": []}, {"id": 17, "doc_id": 18, "filename": "cloud-function/api.html", "domain_name": "page", "name": "cloud-function/api#using-the-nvcf-invocation-api", "display_name": "Using the NVCF Invocation API", "type": "section", "display_type": "Page section", "docname": "cloud-function/api", "anchor": "using-the-nvcf-invocation-api", "priority": -1, "content": "Invocation refers to the execution of an inference call to a function deployed into a cluster. Considerations The body of your request must be valid JSON and max 5MB. If calling the invocation API with no function version ID specified, and multiple function version IDs are deployed, then the inference call may go to instances hosting either function version. Cloud Functions use HTTP/2 persistent connections. For best performance, it is expected that clients will not close connections until it is determined that no further communication with a server is necessary. Cloud Functions invocation supports the following use cases: HTTP Streaming : Uses HTTP/2 persistent connections for continuous data transmission, maintaining open connections for optimal performance until no further communication with the server is necessary. HTTP (Polling) : NVCF responds with either an HTTP Status 200 for a completed result, or an HTTP Status 202 for a response that will require polling for the result on the client. gRPC : Allows users to invoke functions with authentication and function ID in the gRPC metadata, utilizing generic data based on Protobuf messages. HTTP Invocation Example using Function ID curl --request POST \\ --url https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/eb1100de-60bf-4e9a-8617-b7d4652e0c37 \\ --header &#x27;Authorization: Bearer $API_KEY&#x27; \\ --header &#x27;Accept: application/json&#x27; \\ --header &#x27;Content-Type: application/json&#x27; \\ --data &#x27;{ &quot;messages&quot;: [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello&quot; } ], &quot;temperature&quot;: 0.2, &quot;top_p&quot;: 0.7, &quot;max_tokens&quot;: 512 }&#x27;", "keywords": []}, {"id": 18, "doc_id": 18, "filename": "cloud-function/api.html", "domain_name": "std", "name": "cloud-function/api", "display_name": "API", "type": "doc", "display_type": "Page", "docname": "cloud-function/api", "anchor": "", "priority": -1, "content": "Domain: https://api.nvcf.nvidia.com NGC Domain (required for some APIs): https://api.ngc.nvidia.com", "keywords": []}, {"id": 19, "doc_id": 26, "filename": "cloud-function/assets.html", "domain_name": "page", "name": "cloud-function/assets#creating-an-asset-id-and-pre-signed-upload-url", "display_name": "Creating an Asset ID and Pre-signed Upload URL", "type": "section", "display_type": "Page section", "docname": "cloud-function/assets", "anchor": "creating-an-asset-id-and-pre-signed-upload-url", "priority": -1, "content": "For each input file/image, a corresponding asset-id and pre-signed-upload-url can be created as shown below: curl -X POST https://api.nvcf.nvidia.com/v2/nvcf/assets \\ -H &#x27;Authorization: Bearer &lt;JWT Token&gt;&#x27; \\ -H &#x27;accept: application/json&#x27; \\ -H &#x27;Content-Type: application/json&#x27; \\ -d &#x27;{ &quot;contentType&quot;: &quot;image/png&quot;, &quot;description&quot;: &quot;red-cat-photo&quot; }&#x27; This should result in a response like this: { &quot;assetId&quot;:&quot;b5b841c3-11c2-4c34-b057-9475e82c5369&quot;, &quot;uploadUrl&quot;:&quot;&lt;pre-signed-upload-url&gt;&quot;, &quot;contentType&quot;:&quot;image/png&quot;, &quot;description&quot;:&quot;red-cat-photo&quot; }", "keywords": []}, {"id": 20, "doc_id": 26, "filename": "cloud-function/assets.html", "domain_name": "page", "name": "cloud-function/assets#deleting-assets", "display_name": "Deleting Assets", "type": "section", "display_type": "Page section", "docname": "cloud-function/assets", "anchor": "deleting-assets", "priority": -1, "content": "Assets have a default TTL of 24 hours. You can delete an asset sooner by using the delete asset API. curl -X DELETE https://api.nvcf.nvidia.com/v2/nvcf/assets/{assetId} -H &quot;Authorization: Bearer &lt;JWT Token&gt;&quot;", "keywords": []}, {"id": 21, "doc_id": 26, "filename": "cloud-function/assets.html", "domain_name": "page", "name": "cloud-function/assets#listing-assets", "display_name": "Listing Assets", "type": "section", "display_type": "Page section", "docname": "cloud-function/assets", "anchor": "listing-assets", "priority": -1, "content": "You can get a list of asset-ids for your NVIDIA Cloud Account. curl -X GET https://api.nvcf.nvidia.com/v2/nvcf/assets -H &quot;Authorization: Bearer &lt;JWT Token&gt;&quot; Currently, this returns only 1000 asset-ids. If you have more than 1000 assets for your NVIDIA Cloud Account, you will not see all of them.", "keywords": []}, {"id": 22, "doc_id": 26, "filename": "cloud-function/assets.html", "domain_name": "page", "name": "cloud-function/assets#retrieving-asset-directory-and-asset-id-in-triton-python-backend", "display_name": "Retrieving Asset Directory and Asset ID in Triton Python Backend", "type": "section", "display_type": "Page section", "docname": "cloud-function/assets", "anchor": "retrieving-asset-directory-and-asset-id-in-triton-python-backend", "priority": -1, "content": "This section provides guidance on how to retrieve the asset directory and asset ID when using the Triton Python backend. This is particularly useful when dealing with asset management in custom containers and Functions. Steps Importing Helpers Module : Utilize the helpers module available in the Triton Python backend. This module provides essential methods to retrieve asset paths. from triton_python_backend_utils import helpers Getting Asset Paths : Use helpers.get_input_path() and helpers.get_output_path() methods to obtain the paths for the input and output assets, respectively. input_path = helpers.get_input_path() output_path = helpers.get_output_path() Accessing Asset ID : The asset ID can be retrieved from the environment variable NVCF_ASSET_ID . This ID is important for referencing specific assets. import os asset_id = os.environ.get(&#x27;NVCF_ASSET_ID&#x27;) Server Configuration : Ensure that the Triton server is started with the \u2013http-header-forward-pattern NVCF-.* option. This configuration is necessary for forwarding asset-related HTTP headers to the backend. tritonserver --http-header-forward-pattern NVCF-.*", "keywords": []}, {"id": 23, "doc_id": 26, "filename": "cloud-function/assets.html", "domain_name": "page", "name": "cloud-function/assets#specifying-assets-when-invoking-a-function", "display_name": "Specifying Assets when invoking a Function", "type": "section", "display_type": "Page section", "docname": "cloud-function/assets", "anchor": "specifying-assets-when-invoking-a-function", "priority": -1, "content": "You can specify one or more asset-ids when invoking a function using NVCF-INPUT-ASSET-REFERENCES HTTP header like this: curl -X https://api.nvcf.nvidia.com0/v2/nvcf/pexec/functions/&lt;function-id&gt; \\ -H &quot;Content-Type: application/json&quot; \\ -H &quot;NVCF-INPUT-ASSET-REFERENCES: uuid-asset-id1, ..&quot; \\ # Optional -H &quot;Authorization: Bearer $JWT&quot; \\ -d @- &lt;&lt;EOF { &quot;opaque&quot;: &quot;object&quot; // Your inference payload } EOF", "keywords": []}, {"id": 24, "doc_id": 26, "filename": "cloud-function/assets.html", "domain_name": "page", "name": "cloud-function/assets#uploading-assets-to-cloud-storage", "display_name": "Uploading Assets to Cloud Storage", "type": "section", "display_type": "Page section", "docname": "cloud-function/assets", "anchor": "uploading-assets-to-cloud-storage", "priority": -1, "content": "Once a pre-signed-upload-url is obtained, it can be used to upload the input file/image as shown below: curl -X PUT -T my-image-file.png &lt;pre-signed-URL&gt; -H &quot;Content-Type: image/png&quot; -H &quot;x-amz-meta-nvcf-asset-description: red-cat-photo&quot; Ensure that contentType in the POST /v2/nvcf/assets request payload matches the Content-Type header in the curl command to upload the asset. Similarly, make sure that the description in the POST /v2/nvcf/assets request payload matches the x-amz-meta-nvcf-asset-description header in the curl command used to upload the asset. Otherwise, you will see errors with the status code 400 .", "keywords": []}, {"id": 25, "doc_id": 26, "filename": "cloud-function/assets.html", "domain_name": "page", "name": "cloud-function/assets#using-assets-with-custom-containers", "display_name": "Using Assets with Custom Containers", "type": "section", "display_type": "Page section", "docname": "cloud-function/assets", "anchor": "using-assets-with-custom-containers", "priority": -1, "content": "When the customer container is invoked during function invocation, it can use the following HTTP headers to pick up the assets for the specific invocation: NVCF-REQID: \u201c&lt;reqId from the message&gt;\u201d NVCF-FUNCTION-ASSET-IDS: \u201c&lt;comma separated list of asset IDs&gt;\u201d NVCF-ASSET-DIR: \u201c&lt;absolute path to the directory where all assets will be for the specific invocation request&gt;\u201d The path to the assets will be in the request header: \u201cNVCF-ASSET-DIR\u201d. The header \u201cNVCF-FUNCTION-ASSET-IDS\u201d contains a comma-separated list of the asset IDs, the same as the asset filenames. Use these IDs to construct the full path to the assets. Example: NVCF-REQID: b9223090-b7da-11ed-afa1-0242ac120002 NVCF-ASSET-DIR=/var/inf/inputAssets/b9223090-b7da-11ed-afa1-0242ac120002 NVCF-FUNCTION-ASSET-IDS=38652fac-b7ee-11ed-afa1-0242ac120002,386531f0-b7ee-11ed-afa1-0242ac120003 The two asset files would be: /var/inf/inputAssets/b9223090-b7da-11ed-afa1-0242ac120002/38652fac-b7ee-11ed-afa1-0242ac120002 /var/inf/inputAssets/b9223090-b7da-11ed-afa1-0242ac120002/386531f0-b7ee-11ed-afa1-0242ac120003", "keywords": []}, {"id": 26, "doc_id": 26, "filename": "cloud-function/assets.html", "domain_name": "std", "name": "cloud-function/assets", "display_name": "Asset Management", "type": "doc", "display_type": "Page", "docname": "cloud-function/assets", "anchor": "", "priority": -1, "content": "For image-to-image or inpainting based generation or inference, one has to start with one or more existing images as input to a model such as Stable Diffusion. Using NVCF\u2019s Asset endpoints, customers can create an asset-id and corresponding pre-signed upload URL for each input image. The customer can then upload the existing images using their corresponding pre-signed URLs and specify the asset-ids when invoking the function. When the function is invoked with one or more asset-ids, NVCF creates a pre-signed download URLs and provides them to the Worker. The Worker uses these pre-signed download URLs to download the previously uploaded input images and copies them over to a fixed location on the filesystem. The customer\u2019s container can then load these input images from that fixed location and use them during inferencing. Pre-signed URLs provided by the NVCF Asset endpoint have a TTL of 1 hour. So, customers should use it to upload their input image before the TTL expires. These images or assets get deleted after 24 hours. There is a 5GB max limit on the size of each asset upload. Instead of images, one can imagine that assets can be anything that one needs to use during a specific function invocation request. For example, an archive/zip can be an asset.", "keywords": []}, {"id": 27, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#advanced-manual-instance-configuration", "display_name": "Advanced: Manual Instance Configuration", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "advanced-manual-instance-configuration", "priority": -1, "content": "It is highly recommended to rely on the Dynamic GPU Discovery, and therefore the NVIDIA GPU Operator, as manual instance configuration is error-prone. This type of configuration is only necessary when the cluster Cloud Provider does not support the NVIDIA GPU Operator. To enable manual instance configuration, remove the \u201cDynamic GPU Discovery\u201d capability. All fields in the generated example configuration in the UI are required. Start by choosing \u201cApply Example\u201d to copy over the example configuration, and then modify it to your cluster\u2019s instance specifications.", "keywords": []}, {"id": 28, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#advanced-nvca-operator-configuration-options", "display_name": "Advanced: NVCA Operator Configuration Options", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "advanced-nvca-operator-configuration-options", "priority": -1, "content": "Below are additional configuration options for reference purposes.", "keywords": []}, {"id": 29, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#advanced-settings", "display_name": "Advanced Settings", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "advanced-settings", "priority": -1, "content": "See below for descriptions of all capability options in the \u201cAdvanced Settings\u201d section of the cluster configuration. Note that for customer-managed clusters (registered via the Cluster Agent) Dynamic GPU Discovery is enabled by default. For NVIDIA internal clusters, Collect Function Logs is also enabled by default. Capability Description Dynamic GP Discovery Enables automatic detection and management of allocatable GPU capacity within the cluster via the NVIDIA GPU Operator. This capability is strongly recommended and would only be disabled in cases where Manual Instance Configuration is required. Collect Function Logs This capability enables the emission of comprehensive Cluster Agent logs, which are then forwarded to the NVIDIA internal team, aiding in diagnosing and resolving issues effectively. When enabled these will not be visible in the UI, but are always available by running commands to retrieve logs directly on the cluster. Caching Support Enhances application performance by storing frequently accessed data (models, resources and containers) in a cache. See Caching Support . Removing the Dynamic GPU Discovery will require manual instance configuration. See Manual Instance Configuration .", "keywords": []}, {"id": 30, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#caching-support", "display_name": "Caching Support", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "caching-support", "priority": -1, "content": "Enabling caching for models, resources and containers is recommended for optimal performance. You must create StorageClass configurations for caching within your cluster to fully enable \u201cCaching Support\u201d with the Cluster Agent. See examples below: StorageClass Configurations in GCP nvcf-sc.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: nvcf-sc provisioner: pd.csi.storage.gke.io allowVolumeExpansion: true volumeBindingMode: Immediate reclaimPolicy: Retain parameters: type: pd-ssd csi.storage.k8s.io/fstype: xfs nvcf-cc-sc.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: nvcf-cc-sc provisioner: pd.csi.storage.gke.io allowVolumeExpansion: true volumeBindingMode: Immediate reclaimPolicy: Retain parameters: type: pd-ssd csi.storage.k8s.io/fstype: xfs GCP currently allows only 10 VM\u2019s to mount a Persistent Volume in Read-Only mode. StorageClass Configurations in Azure nvcf-sc.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: nvcf-sc provisioner: file.csi.azure.com allowVolumeExpansion: true volumeBindingMode: Immediate reclaimPolicy: Retain parameters: skuName: Standard_LRS csi.storage.k8s.io/fstype: xfs nvcf-cc-sc.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: nvcf-cc-sc provisioner: file.csi.azure.com allowVolumeExpansion: true volumeBindingMode: Immediate reclaimPolicy: Retain parameters: skuName: Standard_LRS csi.storage.k8s.io/fstype: xfs Apply the StorageClass Configurations Save the StorageClass template to files nvcf-sc.yaml and nvcf-cc-sc.yaml and apply them as: kubectl create -f nvcf-sc.yaml kubectl create -f nvcf-cc-sc.yaml", "keywords": []}, {"id": 31, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#cluster-key-rotation", "display_name": "Cluster Key Rotation", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "cluster-key-rotation", "priority": -1, "content": "To regenerate or rotate a cluster\u2019s key, choose the \u201cRegenerate Key\u201d option from the Clusters table on the Settings page. Please refer to this command snippet for the most up-to-date upgrade instructions. Updating your Service Key may interrupt any in-progress updates or deployments to existing functions, therefore it\u2019s important to pause deployments before upgrading.", "keywords": []}, {"id": 32, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#configuration", "display_name": "Configuration", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "configuration", "priority": -1, "content": "See below for descriptions of all cluster configuration options. Field Description Cluster Name The name for the cluster. This field is not changeable once configured. Cluster Group The name of the cluster group. This is usually identical to the cluster name, except in cases when there are multiple clusters you\u2019d like to group. This would be done to enable a function to deploy on any of the clusters when the group is selected (for example, due to identical hardware support). Compute Platform The cloud platform the cluster is deployed on. This field is a standard part of the node name label format that the cluster agent uses: &lt;Platform&gt;.GPU.&lt;GPUName&gt; Region The region the cluster is deployed in. This field is required for enabling future optimization and configuration when deploying functions. Cluster Description Optional description for the cluster, this provides additional context about the cluster and will be returned in the cluster list under the Settings page, and the /listClusters API response. Other Attributes Tag your cluster with additional properties. CacheOptimized : Enables rapid instance spin-up, requires extra storage configuration and caching support attributed in the Advanced Cluster Setup - See Advanced Settings . KataRunTimeIsolation : Cluster is equipped with enhanced setup to ensure superior workload isolation using Kata Containers . Elevating efficiency for rapid instance spin-up, mandating extra storage configuration and caching support attribute in Advanced cluster setup. By default, the cluster will be authorized to the NCA ID of the current NGC organization being used during cluster configuration. If you choose to share the cluster with other NGC organizations, you will need to retrieve their corresponding NCA IDs. Sharing the cluster will allow other NVCF accounts to deploy cloud functions on it, with no limitations on how many GPUs within the cluster they deploy on. NVCF \u201caccounts\u201d are directly tied to, and defined by, NCA IDs (\u201cNVIDIA Cloud Account\u201d). Each NGC organization, with access to the Cloud Functions UI, has a corresponding NGC Organization Name and NCA ID. Please see the NGC Organization Profile Page to find these details. Once functions from other NGC organizations have been deployed on the cluster, removing them from the authorized NCA IDs list, or removing sharing completely from the cluster, can cause disruption of service. Ideally, any functions tied to other NCA IDs should be undeployed before the NCA ID is removed from the authorized NCA IDs list.", "keywords": []}, {"id": 33, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#considerations", "display_name": "Considerations", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "considerations", "priority": -1, "content": "The NVIDIA Cluster Agent currently only supports caching if the cluster is enabled with StorageClass configurations. If the \u201cCaching Support\u201d capability is enabled, the agent will make the best effort by attempting to detect storage during deployments and fall back on non-cached workflows. All NVIDIA-managed clusters support autoscaling functionality fully for all heuristics. However, clusters registered to NVCF via the agent only support autoscaling via the function queue depth heuristic.", "keywords": []}, {"id": 34, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#install-the-cluster-agent", "display_name": "Install the Cluster Agent", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "install-the-cluster-agent", "priority": -1, "content": "After configuring the cluster, an NGC Cluster Key will be generated for authenticating to NGC, and you will be presented with a command snippet for installing the NVIDIA Cluster Agent Operator. Please refer to this command snippet for the most up-to-date installation instructions. The NGC Cluster Key has a default expiration of 90 days. Either on a regular cadence or when nearing expiration, you must rotate your NGC Cluster Key . Once the Cluster Agent Operator installation is complete, the operator will automatically install the desired NVIDIA Cluster Agent version and the Status of the cluster in the Cluster Page will become \u201cReady\u201d. Afterward, you will be able to modify the configuration at any time. The cluster name and SSA client ID (only available for NVIDIA internal clusters) are not reconfigurable. Please refer to any additional installation instructions for reconfiguration in the UI. Once the configuration is updated, the Cluster Agent Operator, which polls for changes every 15 minutes, will apply the new configuration.", "keywords": []}, {"id": 35, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#logs", "display_name": "Logs", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "logs", "priority": -1, "content": "Both the Cluster Agent and Cluster Agent Operator emit logs locally by default. Local logs for the NVIDIA Cluster Agent Operator can be obtained via kubectl : kubectl logs -l app.kubernetes.io/instance=nvca-operator -n nvca-operator --tail 20 Similarly, NVIDIA Cluster Agent logs can be obtained with the following command via kubectl: kubectl logs -l app.kubernetes.io/instance=nvca -n nvca-system --tail 20 Current function-level inference container logs are not supported for functions deployed on non-NVIDIA-managed clusters. Customers are encouraged to emit logs directly from their inference containers running on their own clusters to any third-party tool, there are no public egress limitations for containers.", "keywords": []}, {"id": 36, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#metrics", "display_name": "Metrics", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "metrics", "priority": -1, "content": "The cluster agent and operator emit Prometheus-style metrics. The following metrics and labels are available by default. Metric Name Metric Description nvca_event_queue_length The length of a named event queue nvca_event_process_latency The amount of time for processing an event in NVCA Metric Label Metric Label Description nvca_event_name The name of the event nvca_nca_id The NCA ID of this NVCA instance nvca_cluster_name The NVCA cluster name nvca_cluster_group The NVCA cluster group nvca_version The NVCA version Cluster maintainers can scrape the available metrics using the following examples of a PodMonitor for NVCA Operator and ServiceMonitor for NVCA for reference: Sample NVCA Operator PodMonitor apiVersion: monitoring.coreos.com/v1 kind: PodMonitor metadata: labels: app.kubernetes.io/component: metrics app.kubernetes.io/instance: prometheus-agent app.kubernetes.io/name: metrics-nvca-operator jobLabel: metrics-nvca-operator release: prometheus-agent prometheus.agent/podmonitor-discover: &quot;true&quot; name: metrics-nvca-operator namespace: monitoring spec: podMetricsEndpoints: - port: http scheme: http path: /metrics jobLabel: jobLabel selector: matchLabels: app.kubernetes.io/name: nvca-operator namespaceSelector: matchNames: - nvca-operator Sample NVCA ServiceMonitor apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: app.kubernetes.io/component: metrics app.kubernetes.io/instance: prometheus-agent app.kubernetes.io/name: metrics-nvca jobLabel: metrics-nvca release: prometheus-agent prometheus.agent/servicemonitor-discover: &quot;true&quot; name: prometheus-agent-nvca namespace: monitoring spec: endpoints: - port: nvca jobLabel: jobLabel selector: matchLabels: app.kubernetes.io/name: nvca namespaceSelector: matchNames: - nvca-system", "keywords": []}, {"id": 37, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#ngc-configuration", "display_name": "NGC Configuration", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "ngc-configuration", "priority": -1, "content": "Name Description Value ngcConfig.username Username for the registry authentication $oauthtoken ngcConfig.serviceKey ServiceKey (password) for authentication \u201c\u201d ngcConfig.apiURL NGC API URL for requesting auth tokens https://api.ngc.nvidia.com", "keywords": []}, {"id": 38, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#node-affinity", "display_name": "Node Affinity", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "node-affinity", "priority": -1, "content": "The cluster agent determines which GPU nodes are schedulable for Cloud Function workloads using node affinity . For example, to mark all nodes as schedulable in a cluster: kubectl label nodes -l &#x27;nvidia.com/gpu.present=true&#x27; nvca.nvcf.nvidia.io/schedule=true To mark a single node as unschedulable / cordoned: kubectl label node &lt;node-name&gt; nvca.nvcf.nvidia.io/schedule-", "keywords": []}, {"id": 39, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#node-selector-configuration", "display_name": "Node Selector Configuration", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "node-selector-configuration", "priority": -1, "content": "Name Description Value nodeSelector.key Node-selector Label key node.kubernetes.io/instance-type nodeSelector.value Node-selector Label value \u201c\u201d", "keywords": []}, {"id": 40, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#nvca-operator-parameters", "display_name": "NVCA Operator Parameters", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "nvca-operator-parameters", "priority": -1, "content": "Name Description Value image.repository NVCA Operator container registry path, without tag nvcr.io/nvidia/nvcf-byoc/nvca-operator image.tag NVCA Operator container image tag. This defaults to the chart version \u201c\u201d image.pullPolicy K8s ImagePullPolicy IfNotPresent nvcaImage.repositoryOverride (Optional) Full NVCA container registry path, without tag. Only set this if the default needs to be overridden, for example, \u201cnvcr.io/nvidia/nvcf-byoc/nvca\u201d. The tag is set in the cluster config \u201c\u201d nvcaImage.pullPolicy K8s ImagePullPolicy IfNotPresent replicaCount Replica count for the operator deployment 1 systemNamespace Namespace in which NVCFBackend objects are created. nvca-operator logLevel Logging level for the module info ncaID NVIDIA Cloud Account ID of the Primary Account \u201c\u201d clusterID ID of the Cluster for this NVCA instance to manage \u201c\u201d clusterName For metrics &amp; telemetry \u201c\u201d k8sVersionOverride Override the K8s version that NVCA registers with \u201c\u201d priorityClassName K8s PriorityClassName for pod preference during evictions \u201c\u201d skipFluxInit Skip Flux install if admin already has one installed false", "keywords": []}, {"id": 41, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#opentelemetry-configuration", "display_name": "OpenTelemetry Configuration", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "opentelemetry-configuration", "priority": -1, "content": "Name Description Value otel.enabled Enable OpenTelemetry. false otel.lightstep.serviceName the name of the Lightstep service to push telemetry data to \u201c\u201d otel.lightstep.accessToken the access token for accessing the Lightstep API \u201c\u201d", "keywords": []}, {"id": 42, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#prerequisites", "display_name": "Prerequisites", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "prerequisites", "priority": -1, "content": "Access to a Kubernetes cluster including GPU-enabled nodes (\u201cGPU cluster\u201d) The cluster must have a compatible version of Kubernetes . The cluster must have the NVIDIA GPU Operator installed. If your cloud provider does not support the NVIDIA GPU Operator, Manual Instance Configuration is possible, but not recommended due to lack of maintainability. Registering the cluster requires kubectl and helm installed. The user registering the cluster must have the cluster-admin role privileges to install the NVIDIA Cluster Agent Operator ( nvca-operator ). The user registering the cluster must have the Cloud Functions Admin role within their NGC organization.", "keywords": []}, {"id": 43, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#register-the-cluster", "display_name": "Register the Cluster", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "register-the-cluster", "priority": -1, "content": "Reach the cluster registration page by navigating to Cloud Functions in the NGC product dropdown, and choosing \u201cSettings\u201d on the left-hand menu. You must be a Cloud Functions Admin to see this page. Choose \u201cRegister Cluster\u201d to begin the registration process.", "keywords": []}, {"id": 44, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#supported-kubernetes-versions", "display_name": "Supported Kubernetes Versions", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "supported-kubernetes-versions", "priority": -1, "content": "Minimum Kubernetes Supported Version: v1.25.0 Maximum Kubernetes Supported Version v1.29.x", "keywords": []}, {"id": 45, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#tracing", "display_name": "Tracing", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "tracing", "priority": -1, "content": "The NVIDIA Cluster Agent provides OpenTelemetry integration for exporting traces and events to compatible collectors. As of agent version 2.0, the only supported collector is Lightstep. See Advanced: NVCA Operator Configuration Options .", "keywords": []}, {"id": 46, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#verify-cluster-agent-installation-via-terminal", "display_name": "Verify Cluster Agent Installation via Terminal", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "verify-cluster-agent-installation-via-terminal", "priority": -1, "content": "Verify the installation was successful via the following command, you should see a \u201chealthy\u201d response, as in this example: &gt; kubectl get nvcfbackend -n nvca-operator NAME AGE VERSION HEALTH nvcf-trt-mgpu-cluster 3d16h 2.30.4 healthy", "keywords": []}, {"id": 47, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "page", "name": "cloud-function/cluster-management#verify-cluster-agent-installation-via-ui", "display_name": "Verify Cluster Agent Installation via UI", "type": "section", "display_type": "Page section", "docname": "cloud-function/cluster-management", "anchor": "verify-cluster-agent-installation-via-ui", "priority": -1, "content": "At any time, you can view the clusters you have begun registering, or registered, along with their status, on the Settings page. A status of Ready indicates the Cluster Agent has registered the cluster with NVCF successfully. A status of Not Ready indicates the registration command has either just been applied and is in progress, or that registration is failing. In cases when registration is failing, please use the following command to retrieve additional details: kubectl get nvcfbackend -n nvca-operator When a cluster is Not Ready , you can resume registration at any time to finish the installation. The \u201cGPU Utilization\u201d column is based on the number of GPUs occupied over the number of GPUs available within the cluster. The \u201cLast Connected\u201d column indicates when the last status update was received from the Cluster Agent to the NVCF control plane.", "keywords": []}, {"id": 48, "doc_id": 48, "filename": "cloud-function/cluster-management.html", "domain_name": "std", "name": "cloud-function/cluster-management", "display_name": "Cluster Setup & Management", "type": "doc", "display_type": "Page", "docname": "cloud-function/cluster-management", "anchor": "", "priority": -1, "content": "Cloud Functions admins can install the NVIDIA Cluster Agent to enable existing GPU Clusters to act as deployment targets for NVCF functions. The NVIDIA Cluster Agent is a function deployment orchestrator that communicates with the NVCF control plane. This page describes how to do the following: Register a cluster with NVCF using the NVIDIA Cluster Agent. Configure the cluster by defining GPU instance types, configurations, regions, and authorized NCA (NVIDIA Cloud Account) IDs. Verify the cluster setup was successful. After installing the NVIDIA Cluster Agent on a cluster: The registered cluster will show as a deployment option in the GET /v2/nvcf/clusterGroups API response, and Cloud Functions deployment menu. Any functions under the cluster\u2019s authorized NCA IDs can now deploy on the cluster.", "keywords": []}, {"id": 49, "doc_id": 61, "filename": "cloud-function/elastic-nim.html", "domain_name": "page", "name": "cloud-function/elastic-nim#about-nim", "display_name": "About NIM", "type": "section", "display_type": "Page section", "docname": "cloud-function/elastic-nim", "anchor": "about-nim", "priority": -1, "content": "NVIDIA NIM is a set of easy-to-use microservices designed to accelerate the deployment of foundation models for generative AI applications across various computing environments. Enterprises have two options when deploying NVIDIA NIM in production - they can subscribe to the Elastic NIM service deployed and managed by NVIDIA, in their virtual private cloud or DGX Cloud or export, deploy and self-manage a NIM themselves. Leveraging NVIDIA NIMs, and the NVIDIA Elastic NIM service ensures optimized performance at scale and the reliability of a managed service, along with data privacy and proximity. Key enterprise features and benefits include: Deployed in the customer VPC, NVIDIA Elastic NIM enables enterprises to comply with corporate governance and maintain the security of proprietary data. Ensures NVIDIA optimized performance on any accelerated infrastructure and is based on NVIDIA\u2019s best practices for data center scale control plane and cluster management. Unified orchestration across hybrid clouds ensures high availability and utilization across distributed accelerated compute clusters. Ongoing network performance optimization for full-stack acceleration Burst to DGX Cloud on-demand NVIDIA Enterprise Support and service level agreements for production AI workflows.", "keywords": []}, {"id": 50, "doc_id": 61, "filename": "cloud-function/elastic-nim.html", "domain_name": "page", "name": "cloud-function/elastic-nim#create-the-function", "display_name": "Create the Function", "type": "section", "display_type": "Page section", "docname": "cloud-function/elastic-nim", "anchor": "create-the-function", "priority": -1, "content": "Now that the llama3-8b-instruct NIM has been downloaded and uploaded to the private registry, next, we will add the NVCF function logic and dependencies. In the NGC NVCF console, create a function from a custom container Function name: nvcf-nim_meta-llama3-8b-instruct Define the following settings for the function: Container: Choose the image from the drop-down Tag: Choose from the available tags in the drop-down Models: leave blank Inference Protocol: HTTP Inference Endpoint: /v1/chat/completions Health Path: /v1/health/ready Environment Key: NGC_API_KEY Environment Value: Enter the value of your NGC Personal API Key. $API_KEY NIM will download the llama3-8b-instruct model from NGC during startup to the container\u2019s ephemeral disk. Click Create Function Deploy the Function to a backend cluster by clicking Deploy Version Within the Deploy Function dialog, fill in the following required fields: Function Name: prepopulated function name Function Version: prepopulated latest function version Backend: A collection of one or more (though usually one) clusters to deploy on, for example - a CSP such as Azure, OCI, GCP or an NVIDIA-specific cluster like GFN. GPU: prepopulated GPU model Instance Type: Each GPU type can support one or more instance types, which are different configurations, such as the number of CPU cores, and the number of GPUs per node. Max Concurrency: The number of simultaneous invocations your container can handle at any given time Min Instances: The minimum number of instances your function should be deployed on Max Instances: The maximum number of instances your function is allowed to autoscale to Click Deploy Function. The following dialog will be displayed. The Function will change states from Deploying to Active. Set the Function ID as an environment variable for convenience. This will be used for validating/testing the function. $ export FUNCTION_ID=&lt;your_function_id&gt; $ curl -X POST &quot;https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/${FUNCTION_ID}&quot; \\ -H &quot;Authorization: Bearer ${API_KEY}&quot; \\ -H &quot;Accept: application/json&quot; \\ -H &quot;Content-Type: application/json&quot; \\ -d &#x27;{ &quot;model&quot;: &quot;meta/llama3-8b-instruct&quot;, &quot;messages&quot;: [ { &quot;role&quot;:&quot;user&quot;, &quot;content&quot;:&quot;Can you write me a happysong?&quot; } ], &quot;max_tokens&quot;: 32 }&#x27; #output {&quot;id&quot;:&quot;cmpl-3ae8dd639f74451e98c2a2e2873441ec&quot;,&quot;object&quot;:&quot;chat.completion&quot;,&quot;created&quot;:1721774173,&quot;model&quot;:&quot;meta/llama3-8b-instruct&quot;,&quot;choices&quot;:[{&quot;index&quot;:0,&quot;message&quot;:{&quot;role&quot;:&quot;assistant&quot;,&quot;content&quot;:&quot;I&#x27;d be delighted to write a happy song for you!\\n\\nHere&#x27;s a brand new, original song, just for you:\\n\\n**Title:** \\&quot;Sparkle in&quot;},&quot;logprobs&quot;:null,&quot;finish_reason&quot;:&quot;length&quot;,&quot;stop_reason&quot;:null}],&quot;usage&quot;:{&quot;prompt_tokens&quot;:19,&quot;total_tokens&quot;:51,&quot;completion_tokens&quot;:32}}%", "keywords": []}, {"id": 51, "doc_id": 61, "filename": "cloud-function/elastic-nim.html", "domain_name": "page", "name": "cloud-function/elastic-nim#docker-login-to-ngc", "display_name": "Docker Login to NGC", "type": "section", "display_type": "Page section", "docname": "cloud-function/elastic-nim", "anchor": "docker-login-to-ngc", "priority": -1, "content": "To pull the NIM container image from NGC, first authenticate with the NVIDIA Container Registry with the following command: # with docker $ docker login nvcr.io Username: $oauthtoken Password: $API_KEY #with podman $ podman login nvcr.io Username: $oauthtoken Password: $API_KEY Use $oauthtoken as the username and API_KEY as the password. The $oauthtoken username is a special name that indicates that you will authenticate with an API key and not a username and password.", "keywords": []}, {"id": 52, "doc_id": 61, "filename": "cloud-function/elastic-nim.html", "domain_name": "page", "name": "cloud-function/elastic-nim#download-the-nim", "display_name": "Download the NIM", "type": "section", "display_type": "Page section", "docname": "cloud-function/elastic-nim", "anchor": "download-the-nim", "priority": -1, "content": "Execute the following on the client workstation/server to download the llama3-8b-instruct NIM from the public registry in NGC.", "keywords": []}, {"id": 53, "doc_id": 61, "filename": "cloud-function/elastic-nim.html", "domain_name": "page", "name": "cloud-function/elastic-nim#export-the-ngc-api-key", "display_name": "Export the NGC API Key", "type": "section", "display_type": "Page section", "docname": "cloud-function/elastic-nim", "anchor": "export-the-ngc-api-key", "priority": -1, "content": "Pass the value of the API Key to the docker run command as the API_KEY environment variable: #NGC Organization ID. The name of the org, not the display name export ORG_NAME=&lt;org_name&gt; #NGC Personal API Key. Starts with nvapi- export API_KEY=&lt;your_key_here&gt;", "keywords": []}, {"id": 54, "doc_id": 61, "filename": "cloud-function/elastic-nim.html", "domain_name": "page", "name": "cloud-function/elastic-nim#list-available-nims", "display_name": "List Available NIMs", "type": "section", "display_type": "Page section", "docname": "cloud-function/elastic-nim", "anchor": "list-available-nims", "priority": -1, "content": "NVIDIA regularly publishes new models that are available as downloadable NIMs to NVIDIA AI Enterprise customers. Use the NGC cli to see a list of available NIMs. Use the following command to list the available NIMs, in CSV format. ngc registry image list --format_type csv nvcr.io/nim/meta/\\* This should produce something like the following. Name,Repository,Latest Tag,Image Size,Updated Date,Permission,Signed Tag?,Access Type,Associated Products Llama3-70b-instruct,nim/meta/llama3-70b-instruct,1.0.0,5.96 GB,&quot;Jun 01, 2024&quot;,unlocked,True,LISTED,&quot;nv-ai-enterprise, nvidia-nim-da&quot; Llama3-8b-instruct,nim/meta/llama3-8b-instruct,1.0.0,5.96 GB,&quot;Jun 01, 2024&quot;,unlocked,True,LISTED,&quot;nv-ai-enterprise, nvidia-nim-da&quot; You will use the Repository and Latest Tag fields when you call the docker run command, in an upcoming step. Once the NIM has been downloaded, you will upload it to the NGC private registry. Download the NIM container image using either docker or podman. # with docker docker pull nvcr.io/nim/meta/llama3-8b-instruct:1.0.0 # with podman podman pull nvcr.io/nim/meta/llama3-8b-instruct:1.0.0 If you would like to customize the NIM image prior to uploading to the private registry, create a Dockerfile using NIM as the base image. See the example in the nim-deploy repository as a reference.", "keywords": []}, {"id": 55, "doc_id": 61, "filename": "cloud-function/elastic-nim.html", "domain_name": "page", "name": "cloud-function/elastic-nim#ngc-authentication", "display_name": "NGC Authentication", "type": "section", "display_type": "Page section", "docname": "cloud-function/elastic-nim", "anchor": "ngc-authentication", "priority": -1, "content": "An NGC Pesronal API Key is required to access NGC resources and a key can be generated here . Create a NGC Personal API Key and ensure that the following is selected from the \u201cServices Included\u201d dropdown: Cloud Functions AI Foundation Models and Endpoints NGC Catalog Private Registry Personal keys allow you to configure an expiration date, revoke or delete the key using an action button, and rotate the key as needed. For more information about key types, please refer to the NGC User Guide . Keep your key secret and in a safe place. Do not share it or store it in a place where others can see or copy it.", "keywords": []}, {"id": 56, "doc_id": 61, "filename": "cloud-function/elastic-nim.html", "domain_name": "page", "name": "cloud-function/elastic-nim#ngc-cli-tool", "display_name": "NGC CLI Tool", "type": "section", "display_type": "Page section", "docname": "cloud-function/elastic-nim", "anchor": "ngc-cli-tool", "priority": -1, "content": "This documentation uses the NGC CLI tool in a few of the steps. See the NGC CLI documentation for information on downloading and configuring the tool.", "keywords": []}, {"id": 57, "doc_id": 61, "filename": "cloud-function/elastic-nim.html", "domain_name": "page", "name": "cloud-function/elastic-nim#setup", "display_name": "Setup", "type": "section", "display_type": "Page section", "docname": "cloud-function/elastic-nim", "anchor": "setup", "priority": -1, "content": "Access to a Kubernetes cluster with available GPU nodes. Register Kubernetes cluster with NVCF using the NVIDIA Cluster Agent and configure cluster. Please refer to the Cluster Setup &amp; Management for additional prerequisites and step-by-step instructions. Access to a client Workstation or Server. Install Docker or Podman client Install Kubernetes client (kubectl) with access to the backend Kubernetes API Ensure the client workstation/server can download files from NVIDIA NGC. NVIDIA NGC Account with access to the Enterprise Catalog and a private registry. Please refer to the NGC Private Registry User Guide for more details. The private registry will be used in a later step for storing llama3-8b-instruct NIM.", "keywords": []}, {"id": 58, "doc_id": 61, "filename": "cloud-function/elastic-nim.html", "domain_name": "page", "name": "cloud-function/elastic-nim#troubleshooting", "display_name": "Troubleshooting", "type": "section", "display_type": "Page section", "docname": "cloud-function/elastic-nim", "anchor": "troubleshooting", "priority": -1, "content": "Issue: Pod is stuck in \u201cInitializing\u201d This process can take several minutes. If too much time has passed, ensure the API Personal Key is valid and there are no issues in the event logs. Issue: Pods are stuck in \u201cPending\u201d Ensure GPU-enabled pods can be scheduled in the cluster. Check that a GPU is available and no taints exist that would block the scheduler. See troubleshooting the GPU operator for more information Issue: Inference requests to the API endpoint do not return any output Ensure the NGC Personal API Key in the Authorization header is correct and has appropriate access. See Statuses and Errors for more tips.", "keywords": []}, {"id": 59, "doc_id": 61, "filename": "cloud-function/elastic-nim.html", "domain_name": "page", "name": "cloud-function/elastic-nim#upload-nim-to-private-registry", "display_name": "Upload NIM to Private Registry", "type": "section", "display_type": "Page section", "docname": "cloud-function/elastic-nim", "anchor": "upload-nim-to-private-registry", "priority": -1, "content": "NVCF requires the NIM container image to exist in the private registry prior to creating the function. Tag the image with the NGC Org name that has NVCF enabled and name the image to nvcf-nim with the tag meta-llama3-8b-instruct. #with docker docker tag nvcr.io/nim/meta/llama3-8b-instruct:1.0.0 nvcr.io/$ORG_NAME/nvcf-nim:meta-llama3-8b-instruct #with podman podman tag nvcr.io/nim/meta/llama3-8b-instruct:1.0.0 nvcr.io/$ORG_NAME/nvcf-nim:meta-llama3-8b-instruct Push the image to Private Registry # with docker docker push nvcr.io/$ORG_NAME/nvcf-nim:meta-llama3-8b-instruct #with podman podman push nvcr.io/$ORG_NAME/nvcf-nim:meta-llama3-8b-instruct", "keywords": []}, {"id": 60, "doc_id": 61, "filename": "cloud-function/elastic-nim.html", "domain_name": "page", "name": "cloud-function/elastic-nim#validating-the-function-deployment", "display_name": "Validating the Function Deployment", "type": "section", "display_type": "Page section", "docname": "cloud-function/elastic-nim", "anchor": "validating-the-function-deployment", "priority": -1, "content": "NVCF creates a pod for the function in the `nvcf-backend` namespace. The pod might take a few minutes to initialize depending on the size of the image and environment factors. $ kubectl get all -n nvcf-backend NAME READY STATUS RESTARTS AGE pod/0-sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e 2/2 Running 0 10m During initialization, pod logs are unavailable. Monitor the event log for status $ kubectl get events -n nvcf-backend LAST SEEN TYPE REASON OBJECT MESSAGE 12m Normal Scheduled pod/0-sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e Successfully assigned nvcf-backend/0-sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e to aks-ncvfgpu-13288136-vmss000000 12m Normal Pulled pod/0-sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e Container image &quot;nvcr.io/qtfpt1h0bieu/nvcf-core/nvcf_worker_init:0.24.10&quot; already present on machine 12m Normal Created pod/0-sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e Created container init 12m Normal Started pod/0-sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e Started container init 12m Normal Pulled pod/0-sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e Container image &quot;nvcr.io/0494738860185553/nvcf-nim:meta-llama3-8b-instruct&quot; already present on machine 12m Normal Created pod/0-sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e Created container inference 12m Normal Started pod/0-sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e Started container inference 12m Normal Pulled pod/0-sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e Container image &quot;nvcr.io/qtfpt1h0bieu/nvcf-core/nvcf_worker_utils:2.24.2&quot; already present on machine 12m Normal Created pod/0-sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e Created container utils 12m Normal Started pod/0-sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e Started container utils 25m Normal Killing pod/0-sr-dca52008-f0ae-43ca-9f5a-ff9c4ca8c00d Stopping container inference 25m Normal Killing pod/0-sr-dca52008-f0ae-43ca-9f5a-ff9c4ca8c00d Stopping container utils 12m Normal InstanceStatusUpdate spotrequest/sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e Request accepted for processing 12m Normal InstanceCreation spotrequest/sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e Creating 1 requested instances 12m Normal InstanceCreation spotrequest/sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e Created Pod Instance 0-sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e 12m Normal InstanceStatusUpdate spotrequest/sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e 0-sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e is running 25m Normal InstanceTermination spotrequest/sr-d4c0d756-1a30-41bd-afdb-b8f440ac5ce4 Stopped instance nvcf-backend/0-sr-dca52008-f0ae-43ca-9f5a-ff9c4ca8c00d 25m Normal InstanceStatusUpdate spotrequest/sr-d4c0d756-1a30-41bd-afdb-b8f440ac5ce4 0-sr-dca52008-f0ae-43ca-9f5a-ff9c4ca8c00d is terminated 23m Normal InstanceTermination spotrequest/sr-d4c0d756-1a30-41bd-afdb-b8f440ac5ce4 All instances terminated, request will be cleaned-up 23m Normal InstanceStatusUpdate spotrequest/sr-dca52008-f0ae-43ca-9f5a-ff9c4ca8c00d 0-sr-dca52008-f0ae-43ca-9f5a-ff9c4ca8c00d is terminated 23m Normal InstanceTermination spotrequest/sr-dca52008-f0ae-43ca-9f5a-ff9c4ca8c00d All instances terminated, request will be cleaned-up Once running, NIM will download the model during startup if it\u2019s not present on the disk. For large models, this can take several minutes. Example startup logs for llama3-8b-instruct $ kubectl logs 0-sr-60f3f280-10a3-42bd-945a-97dd9fc1e67e -n nvcf-backend Defaulted container &quot;inference&quot; out of: inference, utils, init (init) =========================================== == NVIDIA Inference Microservice LLM NIM == =========================================== NVIDIA Inference Microservice LLM NIM Version 1.0.0 Model: nim/meta/llama3-8b-instruct Container image Copyright (c) 2016-2024, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved. This NIM container is governed by the NVIDIA AI Product Agreement here: https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/. A copy of this license can be found under /opt/nim/LICENSE. The use of this model is governed by the AI Foundation Models Community License here: https://docs.nvidia.com/ai-foundation-models-community-license.pdf. ADDITIONAL INFORMATION: Meta Llama 3 Community License, Built with Meta Llama 3. A copy of the Llama 3 license can be found under /opt/nim/MODEL_LICENSE. 2024-07-23 22:27:23,428 [INFO] PyTorch version 2.2.2 available. 2024-07-23 22:27:24,016 [WARNING] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: error 2024-07-23 22:27:24,016 [INFO] [TRT-LLM] [I] Starting TensorRT-LLM init. 2024-07-23 22:27:24,202 [INFO] [TRT-LLM] [I] TensorRT-LLM inited. [TensorRT-LLM] TensorRT-LLM version: 0.10.1.dev2024053000 INFO 07-23 22:27:24.927 api_server.py:489] NIM LLM API version 1.0.0 INFO 07-23 22:27:24.929 ngc_profile.py:217] Running NIM without LoRA. Only looking for compatible profiles that do not support LoRA. INFO 07-23 22:27:24.929 ngc_profile.py:219] Detected 1 compatible profile(s). INFO 07-23 22:27:24.929 ngc_injector.py:106] Valid profile: 8835c31752fbc67ef658b20a9f78e056914fdef0660206d82f252d62fd96064d (vllm-fp16-tp1) on GPUs [0] INFO 07-23 22:27:24.929 ngc_injector.py:141] Selected profile: 8835c31752fbc67ef658b20a9f78e056914fdef0660206d82f252d62fd96064d (vllm-fp16-tp1) INFO 07-23 22:27:25.388 ngc_injector.py:146] Profile metadata: llm_engine: vllm INFO 07-23 22:27:25.388 ngc_injector.py:146] Profile metadata: precision: fp16 INFO 07-23 22:27:25.388 ngc_injector.py:146] Profile metadata: feat_lora: false INFO 07-23 22:27:25.388 ngc_injector.py:146] Profile metadata: tp: 1 INFO 07-23 22:27:25.389 ngc_injector.py:166] Preparing model workspace. This step might download additional files to run the model. INFO 07-23 22:28:14.764 ngc_injector.py:172] Model workspace is now ready. It took 49.375 seconds INFO 07-23 22:28:14.767 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model=&#x27;/tmp/meta--llama3-8b-instruct-k3_wpocb&#x27;, speculative_config=None, tokenizer=&#x27;/tmp/meta--llama3-8b-instruct-k3_wpocb&#x27;, skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend=&#x27;outlines&#x27;), seed=0) WARNING 07-23 22:28:15.0 logging.py:314] Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. INFO 07-23 22:28:15.16 utils.py:609] Found nccl from library /usr/local/lib/python3.10/dist-packages/nvidia/nccl/lib/libnccl.so.2 INFO 07-23 22:28:16 selector.py:28] Using FlashAttention backend. INFO 07-23 22:28:19 model_runner.py:173] Loading model weights took 14.9595 GB INFO 07-23 22:28:20.644 gpu_executor.py:119] # GPU blocks: 27793, # CPU blocks: 2048 INFO 07-23 22:28:22 model_runner.py:973] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set &#x27;enforce_eager=True&#x27; or use &#x27;--enforce-eager&#x27; in the CLI. INFO 07-23 22:28:22 model_runner.py:977] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage. INFO 07-23 22:28:27 model_runner.py:1054] Graph capturing finished in 5 secs. WARNING 07-23 22:28:28.194 logging.py:314] Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. INFO 07-23 22:28:28.205 serving_chat.py:347] Using default chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = &#x27;&lt;|start_header_id|&gt;&#x27; + message[&#x27;role&#x27;] + &#x27;&lt;|end_header_id|&gt; &#x27;+ message[&#x27;content&#x27;] | trim + &#x27;&lt;|eot_id|&gt;&#x27; %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ &#x27;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; &#x27; }}{% endif %} WARNING 07-23 22:28:28.420 logging.py:314] Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. INFO 07-23 22:28:28.431 api_server.py:456] Serving endpoints: 0.0.0.0:8000/openapi.json 0.0.0.0:8000/docs 0.0.0.0:8000/docs/oauth2-redirect 0.0.0.0:8000/metrics 0.0.0.0:8000/v1/health/ready 0.0.0.0:8000/v1/health/live 0.0.0.0:8000/v1/models 0.0.0.0:8000/v1/version 0.0.0.0:8000/v1/chat/completions 0.0.0.0:8000/v1/completions INFO 07-23 22:28:28.431 api_server.py:460] An example cURL request: curl -X &#x27;POST&#x27; \\ &#x27;http://0.0.0.0:8000/v1/chat/completions&#x27; \\ -H &#x27;accept: application/json&#x27; \\ -H &#x27;Content-Type: application/json&#x27; \\ -d &#x27;{ &quot;model&quot;: &quot;meta/llama3-8b-instruct&quot;, &quot;messages&quot;: [ { &quot;role&quot;:&quot;user&quot;, &quot;content&quot;:&quot;Hello! How are you?&quot; }, { &quot;role&quot;:&quot;assistant&quot;, &quot;content&quot;:&quot;Hi! I am quite well, how can I help you today?&quot; }, { &quot;role&quot;:&quot;user&quot;, &quot;content&quot;:&quot;Can you write me a song?&quot; } ], &quot;top_p&quot;: 1, &quot;n&quot;: 1, &quot;max_tokens&quot;: 15, &quot;stream&quot;: true, &quot;frequency_penalty&quot;: 1.0, &quot;stop&quot;: [&quot;hello&quot;] }&#x27; INFO 07-23 22:28:28.476 server.py:82] Started server process [32] INFO 07-23 22:28:28.477 on.py:48] Waiting for application startup. INFO 07-23 22:28:28.478 on.py:62] Application startup complete. INFO 07-23 22:28:28.480 server.py:214] Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit) INFO 07-23 22:28:28.574 httptools_impl.py:481] 127.0.0.1:40948 - &quot;GET /v1/health/ready HTTP/1.1&quot; 503 INFO 07-23 22:28:29.75 httptools_impl.py:481] 127.0.0.1:40962 - &quot;GET /v1/health/ready HTTP/1.1&quot; 200 INFO 07-23 22:28:38.478 metrics.py:334] Avg prompt throughput: 0.3 tokens/s, Avg generation throughput: 1.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0% INFO 07-23 22:28:48.478 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%", "keywords": []}, {"id": 61, "doc_id": 61, "filename": "cloud-function/elastic-nim.html", "domain_name": "std", "name": "cloud-function/elastic-nim", "display_name": "Elastic NIM", "type": "doc", "display_type": "Page", "docname": "cloud-function/elastic-nim", "anchor": "", "priority": -1, "content": "NVIDIA Elastic NIM is a managed AI inference service fabric enabling enterprises to seamlessly deploy and scale private Generative AI model endpoints securely on distributed accelerated cloud or data center infrastructure. Orchestrated and managed by NVIDIA, proprietary data never leaves the secure tenancy of enterprise hybrid virtual private clouds (VPCs). NVIDIA Elastic NIM inference services for Llama 3.x are now available for enterprises to securely scale across any accelerated computing infrastructure. This guide gives a step-by-step guide for creating and deploying the llama3-8b-instruct NIM as a function on NVIDIA Cloud Functions (NVCF).", "keywords": []}, {"id": 62, "doc_id": 62, "filename": "cloud-function/faq.html", "domain_name": "std", "name": "cloud-function/faq", "display_name": "FAQ", "type": "doc", "display_type": "Page", "docname": "cloud-function/faq", "anchor": "", "priority": -1, "content": "How can I monitor my functions? Refer to Logging and Metrics . How can I troubleshoot errors in my functions? Refer to Statuses and Errors for invocation errors. Refer to Deployment Failures for deployment errors. How can I join an existing Cloud Functions organization? The NGC organization owner of the organization with the Cloud Functions product entitlement needs to add you as a user within the organization and to ensure your user has the Cloud Functions role. Refer to the NGC Documentation . What happens to invocations/jobs in the queue when a function is undeployed or deleted? The corresponding queue for the function is cleared and deleted. How can I monitor Cloud Functions for incidents? Please feel free to subscribe to our Status Page . Is it expected to see functions from unknown NCA IDs/organizations while listing functions? Yes, this is expected. Please see Function Permissions . Where should I expect my mounted model files to be located in my container? See Creating Functions with NGC Models &amp; Resources . Does Cloud Functions load balance across functions? Every function version deployed also has a corresponding queue. Distribution of work is based on whether the function instance is \u201cbusy\u201d or not, as soon as it\u2019s available to receive requests, then requests are routed to it. Therefore, work is roughly balanced across all instances of a function version based on their availability. To distribute work across all versions of a function, invoke by function ID, not by function version ID. By doing so, work is distributed evenly across both function versions. To ensure one function version is not overloaded compared to another, all function versions should have relatively equal instance counts.", "keywords": []}, {"id": 63, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#adding-partial-response-progress", "display_name": "Adding Partial Response (Progress)", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "adding-partial-response-progress", "priority": -1, "content": "Below are instructions on setting up output directories and efficiently tracking and communicating inferencing progress using Cloud Functions. This functionality is only supported for container-based functions. Cloud Functions automatically configures the output directory for you. To access the path, simply read the NVCF-LARGE-OUTPUT-DIR header. NVCF-LARGE-OUTPUT-DIR points to the directory for that particular requestId . To enable partial progress reporting, you will need to store partial and completed outputs, and create a progress file in the output directory. Once the output file and progress file are correctly set up in the output directory under the correct request id, Cloud Functions will automatically detect them. When using the invocation API to poll for a response , progress will be returned as the header NVCF-PERCENT-COMPLETE , along with any partial response data.", "keywords": []}, {"id": 64, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#additional-examples", "display_name": "Additional Examples", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "additional-examples", "priority": -1, "content": "See more examples of PyTriton containers that are Cloud Functions compatible here .", "keywords": []}, {"id": 65, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#authenticate-with-ngc-docker-registry", "display_name": "Authenticate with NGC Docker Registry", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "authenticate-with-ngc-docker-registry", "priority": -1, "content": "Run docker login nvcr.io and input the following, note $oauthtoken is the actual string to input, and $API_KEY is the Personal API key generated in the first step. &gt; docker login nvcr.io Username: $oauthtoken Password: $API_KEY", "keywords": []}, {"id": 66, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#available-container-variables", "display_name": "Available Container Variables", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "available-container-variables", "priority": -1, "content": "The following is a reference of available variables via the headers of the invocation message (auto-populated by Cloud Functions), accessible within the container. For examples of how to extract and use some of these variables, see NVCF Container Helper Functions . Name Description NVCF-REQID Request ID for this request. NVCF-SUB Message subject. NVCF-NCAID Function\u2019s organization\u2019s NCA ID. NVCF-FUNCTION-NAME Function name. NVCF-FUNCTION-ID Function ID. NVCF-FUNCTION-VERSION-ID Function version ID. NVCF-ASSET-DIR Asset directory path. Not available for helm deployments. NVCF-LARGE-OUTPUT-DIR Large output directory path. NVCF-MAX-RESPONSE-SIZE-BYTES Max response size in bytes for the function. NVCF-NSPECTID NVIDIA reserved variable. NVCF-BACKEND Backend or \u201cCluster Group\u201d the function is deployed on. NVCF-INSTANCETYPE Instance type the function is deployed on. NVCF-REGION Region or zone the function is deployed in. NVCF-ENV Spot environment if deployed on spot instances.", "keywords": []}, {"id": 67, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#best-practices", "display_name": "Best Practices", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "best-practices", "priority": -1, "content": "Always use the \u201c.partial\u201d extension to avoid sending partial or incomplete data. Rename to the final extension only when the writing process is fully complete. Ensure your progress file remains under 250KB to maintain efficiency and avoid errors.", "keywords": []}, {"id": 68, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#build-the-container-create-the-function", "display_name": "Build the Container & Create the Function", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "build-the-container-create-the-function", "priority": -1, "content": "See the Quickstart for the remaining steps.", "keywords": []}, {"id": 69, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#build-the-docker-image", "display_name": "Build the Docker Image", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "build-the-docker-image", "priority": -1, "content": "Open a terminal or command prompt. Navigate to the my_model directory. Run the following command to build the docker image: docker build -t my_model_image . Replace my_model_image with the desired name for your docker image.", "keywords": []}, {"id": 70, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#composing-a-fastapi-container", "display_name": "Composing a FastAPI Container", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "composing-a-fastapi-container", "priority": -1, "content": "It\u2019s possible to use any container with Cloud Functions as long as it implements a server with the above endpoints. The below is an example of a FastAPI-based container compatible with Cloud Functions. Clone the full example here .", "keywords": []}, {"id": 71, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#composing-a-pytriton-container", "display_name": "Composing a PyTriton Container", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "composing-a-pytriton-container", "priority": -1, "content": "NVIDIA\u2019s PyTriton is a Python native solution of Triton inference server. A minimum version of 0.3.0 is required.", "keywords": []}, {"id": 72, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#configuration", "display_name": "Configuration", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "configuration", "priority": -1, "content": "The default health /v2/health/ready , port 8000 , and inference endpoint ( v2/models/$MODEL_NAME/infer ) work automatically with Triton-based containers. The docker image\u2019s run command must be configured with the following: CMD tritonserver --model-repository=${MODEL_PATH} --http-header-forward-pattern NVCF-.* Here is an example of a Dockerfile : Dockerfile FROM nvcr.io/nvidia/tritonserver:24.01-py3 # install requirements file COPY requirements.txt requirements.txt RUN pip install --no-cache-dir --upgrade pip RUN pip install --no-cache-dir -r requirements.txt COPY model_repository /model_repository ENV CUDA_MODULE_LOADING LAZY ENV LOG_VERBOSE 0 CMD tritonserver --log-verbose ${LOG_VERBOSE} --http-header-forward-pattern (nvcf-.*|NVCF-.*) \\ --model-repository /model_repository/ --model-control-mode=none --strict-readiness 1 See a full example of a Triton container .", "keywords": []}, {"id": 73, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#container-based-function-creation", "display_name": "Container-Based Function Creation", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "container-based-function-creation", "priority": -1, "content": "Container-based functions require building and pushing a Cloud Functions compatible Docker container image to the NGC Private Registry. Before proceeding, ensure that you have the NGC CLI installed and configured with an API Key that has the required scopes for Cloud Functions and Private Registry. See Working with NGC Private Registry for instructions.", "keywords": []}, {"id": 74, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#container-endpoints", "display_name": "Container Endpoints", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "container-endpoints", "priority": -1, "content": "Any server can be implemented within the container, as long as it implements the following: For HTTP-based functions, a health check endpoint that returns a 200 HTTP Status Code on success. For gRPC-based functions, a standard gRPC health check. See these docs for more info also gRPC Health Checking . An inference endpoint (this endpoint will be called during function invocation) These endpoints are expected to be served on the same port, defined as the inferencePort . Cloud Functions reserves the following ports on your container for internal monitoring and metrics: Port 8080 Port 8010 Cloud Functions also expects the following directories in the container to remain read-only for caching purposes: /config/ directory Nested directories created inside /config/", "keywords": []}, {"id": 75, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#container-versioning", "display_name": "Container Versioning", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "container-versioning", "priority": -1, "content": "Ensure that any resources that you tag for deployment into production environments are not simply using \u201clatest\u201d and are following a standard version control convention. During autoscaling, a function scaling any additional instances will pull the same specificed container image and version. If version is set to \u201clatest\u201d, and the \u201clatest\u201d container image is updated between instance scaling, this can lead to undefined behavior. Function versions created are immutable, this means that the container image and version cannot be updated for a function without creating a new version of the function.", "keywords": []}, {"id": 76, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#create-a-helm-based-function", "display_name": "Create a Helm-based Function", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "create-a-helm-based-function", "priority": -1, "content": "Ensure your helm chart is uploaded to NGC Private Registry and adheres to the Prerequisites listed above. Create the function: Include the following additional parameters in the function definition helmChart helmChartServiceName The helmChart property should be set to the URL hosted by the NGC Model Registry pointing to the helm chart that will deploy the \u201cmini-service\u201d. Please note, that this helm chart URL should be accessible to the NGC org in which the function will eventually be deployed. The helm chart URL should follow the format: https://helm.ngc.nvidia.com/$ORG_ID/$TEAM_NAME/charts/$NAME-X.Y.Z.tgz for example, https://helm.ngc.nvidia.com/abc123/teamA/charts/nginx-0.1.5.tgz would be a valid chart URL but https://helm.ngc.nvidia.com/abc123/teamA/charts/nginx-0.1.5-hello.tgz would not. The helmChartServiceName is used for checking if the \u201cmini-service\u201d is ready for inference and is also scraped for function metrics. At this time, templatized service names are not supported. This must match the service name of your \u201cmini-service\u201d with the exposed entry point port. Important: The Helm chart name should not contain underscores or other special symbols, as that may cause issues during deployment. Example Creation via API Please see our sample helm chart used in this example for reference. Below is an example function creation API call creating a helm-based function: curl -X &#x27;POST&#x27; \\ &#x27;https://api.ngc.nvidia.com/v2/nvcf/functions&#x27; \\ -H &#x27;Authorization: Bearer $API_KEY&#x27; \\ -H &#x27;accept: application/json&#x27; \\ -H &#x27;Content-Type: application/json&#x27; \\ -d &#x27;{ &quot;name&quot;: &quot;function_name&quot;, &quot;inferenceUrl&quot;: &quot;v2/models/model_name/versions/model_version/infer&quot;, &quot;inferencePort&quot;: 8001, &quot;helmChart&quot;: &quot;https://helm.ngc.nvidia.com/$ORG_ID/$TEAM_NAME/charts/inference-test-1.0.tgz&quot;, &quot;helmChartServiceName&quot;: &quot;service_name&quot;, &quot;apiBodyFormat&quot;: &quot;CUSTOM&quot; }&#x27; For gRPC-based functions, set &quot;inferenceURL&quot; : &quot;/gRPC&quot; . This signals to Cloud Functions that the function is using gRPC protocol and is not expected to have a /gRPC endpoint exposed for inferencing requests. Proceed with function deployment and invocation normally.", "keywords": []}, {"id": 77, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#create-the-dockerfile", "display_name": "Create the Dockerfile", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "create-the-dockerfile", "priority": -1, "content": "Dockerfile FROM python:3.10.13-bookworm ENV WORKER_COUNT=10 WORKDIR /app COPY requirements.txt ./ RUN python -m pip install --no-cache-dir -U pip &amp;&amp; \\ python -m pip install --no-cache-dir -r requirements.txt COPY http_echo_server.py /app/ CMD uvicorn http_echo_server:app --host=0.0.0.0 --workers=$WORKER_COUNT", "keywords": []}, {"id": 78, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#create-the-function", "display_name": "Create the Function", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "create-the-function", "priority": -1, "content": "Create the function via API by running the following curl with an $API_KEY and your $ORG_NAME . In this example, we defined the inference endpoint as 8000 and are using the default inference and health endpoint paths. curl --location &#x27;https://api.ngc.nvidia.com/v2/nvcf/functions&#x27; \\ --header &#x27;Content-Type: application/json&#x27; \\ --header &#x27;Accept: application/json&#x27; \\ --header &#x27;Authorization: Bearer $API_KEY&#x27; \\ --data &#x27;{ &quot;name&quot;: &quot;my-model-function&quot;, &quot;inferenceUrl&quot;: &quot;/v2/models/my_model_image/infer&quot;, &quot;healthUri&quot;: &quot;/v2/health/ready&quot;, &quot;inferencePort&quot;: 8000, &quot;containerImage&quot;: &quot;nvcr.io/$ORG_NAME/my_model_image:latest&quot; }&#x27;", "keywords": []}, {"id": 79, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#create-the-requirements-txt-file", "display_name": "Create the \u201crequirements.txt\u201d File", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "create-the-requirements-txt-file", "priority": -1, "content": "requirements.txt fastapi==0.110.0 uvicorn==0.29.0", "keywords": []}, {"id": 80, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#create-the-run-py-file", "display_name": "Create the \u201crun.py\u201d File", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "create-the-run-py-file", "priority": -1, "content": "Your run.py file (or similar Python file) needs to define a PyTriton model. This involves importing your model dependencies, creating a PyTritonServer class with an __init__ function, an _infer_fn function and a run function that serves the inference_function, defining the model name, the inputs and the outputs along with optional configuration. Here is an example of a run.py file: run.py import numpy as np from pytriton.model_config import ModelConfig, Tensor from pytriton.triton import Triton, TritonConfig import time .... class PyTritonServer: &quot;&quot;&quot;triton server for timed_sleeper&quot;&quot;&quot; def __init__(self): # basically need to accept image, mask(PIL Images), prompt, negative_prompt(str), seed(int) self.model_name = &quot;timed_sleeper&quot; def _infer_fn(self, requests): responses = [] for req in requests: req_data = req.data sleep_duration = numpy_array_to_variable(req_data.get(&quot;sleep_duration&quot;)) # deal with header dict keys being lowerscale request_parameters_dict = uppercase_keys(req.parameters) time.sleep(sleep_duration) responses.append({&quot;sleep_duration&quot;: np.array([sleep_duration])}) return responses def run(self): &quot;&quot;&quot;run triton server&quot;&quot;&quot; with Triton( config=TritonConfig( http_header_forward_pattern=&quot;NVCF-*&quot;, # this is required http_port=8000, grpc_port=8001, metrics_port=8002, ) ) as triton: triton.bind( model_name=&quot;timed_sleeper&quot;, infer_func=self._infer_fn, inputs=[ Tensor(name=&quot;sleep_duration&quot;, dtype=np.uint32, shape=(1,)), ], outputs=[Tensor(name=&quot;sleep_duration&quot;, dtype=np.uint32, shape=(1,))], config=ModelConfig(batching=False), ) triton.serve() if __name__ == &quot;__main__&quot;: server = PyTritonServer() server.run()", "keywords": []}, {"id": 81, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#creating-a-progress-file", "display_name": "Creating a Progress File", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "creating-a-progress-file", "priority": -1, "content": "Cloud Functions actively observes the output directory for a file named progress . This file is used to communicate progress and partial responses back to the caller. This file should contain well-formed JSON data. Structure the JSON content as follows: { &quot;id&quot;: &quot;{requestId}&quot;, &quot;progress&quot;: 50, &quot;partialResponse&quot;: { &quot;exampleKey&quot;: &quot;Insert any well-formed JSON here, but ensure its size is less than 250K&quot; } } Replace requestId with the actual request id if it\u2019s present. Modify the progress integer as needed, ranging from 0 (just started) to 100 (fully complete). Within partialResponse , insert any JSON content you want to send as a partial response, making sure it\u2019s smaller than 250KB.", "keywords": []}, {"id": 82, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#creating-functions-with-ngc-models-resources", "display_name": "Creating Functions with NGC Models & Resources", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "creating-functions-with-ngc-models-resources", "priority": -1, "content": "When creating a function, models and resources can be mounted to the function instance. The models will be available under /config/models/{modelName} and /config/resources/{resourceName} where modelName and resourceName are specified as part of the API request. Here is an example where a model and resource are added to a function creation API call, for an echo sample function: curl -X &#x27;POST&#x27; \\ &#x27;https://api.ngc.nvidia.com/v2/nvcf/functions&#x27; \\ -H &#x27;Authorization: Bearer $API_KEY&#x27; \\ -H &#x27;accept: application/json&#x27; \\ -H &#x27;Content-Type: application/json&#x27; \\ -d &#x27;{ &quot;name&quot;: &quot;echo_function&quot;, &quot;inferenceUrl&quot;: &quot;/echo&quot;, &quot;containerImage&quot;: &quot;nvcr.io/$ORG_NAME/echo:latest&quot;, &quot;apiBodyFormat&quot;: &quot;CUSTOM&quot;, &quot;models&quot;: [ { &quot;name&quot;: &quot;simple_int8&quot;, &quot;version&quot;: &quot;1&quot;, &quot;uri&quot;: &quot;v2/org/cf/$ORG_NAME/models/simple_int8/versions/1/zip&quot; } ], &quot;resources&quot;: [ { &quot;name&quot;: &quot;simple_resource&quot;, &quot;version&quot;: &quot;1&quot;, &quot;uri&quot;: &quot;v2/org/cf/$ORG_NAME/resources/simple_resource/versions/1/zip&quot; } ] }&#x27; Within the container, once the function instance is deployed, the model would be mounted at /config/models/simple_int8 and resource mounted at /config/resources/simple_int8", "keywords": []}, {"id": 83, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#creating-grpc-based-functions", "display_name": "Creating gRPC-based Functions", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "creating-grpc-based-functions", "priority": -1, "content": "Cloud Functions supports function invocation via gRPC. During function creation, specify that the function is a gRPC function by setting the \u201cInference Protocol\u201d, or inferenceUrl field to /grpc .", "keywords": []}, {"id": 84, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#download-configure-the-ngc-cli", "display_name": "Download & Configure the NGC CLI", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "download-configure-the-ngc-cli", "priority": -1, "content": "Navigate to the NGC CLI Installer Page to download the CLI and follow the installation instructions for your platform. Find your NGC organization name within the NGC Organization Profile Page . This is not the Display Name. For example: qdrlnbkss123 . Run ngc config set and input the Personal API Key generated in the previous step, along with your organization name. If prompted, default to no-team and no-ace . &gt; ngc config set Enter API key [****bi9Z]. Choices: [&lt;VALID_APIKEY&gt;, &#x27;no-apikey&#x27;]: $API_KEY Enter CLI output format type [json]. Choices: [&#x27;ascii&#x27;, &#x27;csv&#x27;, &#x27;json&#x27;]: json Enter org [ax3ysqem02xw]. Choices: [&#x27;$ORG_NAME&#x27;]: $ORG_NAME Enter team [no-team]. Choices: [&#x27;no-team&#x27;]: Enter ace [no-ace]. Choices: [&#x27;no-ace&#x27;]:", "keywords": []}, {"id": 85, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#generate-an-ngc-personal-api-key", "display_name": "Generate an NGC Personal API Key", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "generate-an-ngc-personal-api-key", "priority": -1, "content": "Do this by navigating to the Personal Keys Page . For more details see Generate an NGC Personal API Key . It\u2019s recommended that the API Key that you generate includes both Cloud Functions and Private Registry scopes to enable ideal Cloud Functions workflows.", "keywords": []}, {"id": 86, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#grpc-function-creation-via-api", "display_name": "gRPC Function Creation via API", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "grpc-function-creation-via-api", "priority": -1, "content": "When creating the gRPC function, set the inferenceURl field to /grpc : curl --location &#x27;https://api.ngc.nvidia.com/v2/nvcf/functions&#x27; \\ --header &#x27;Content-Type: application/json&#x27; \\ --header &#x27;Accept: application/json&#x27; \\ --header &#x27;Authorization: Bearer $API_KEY&#x27; \\ --data &#x27;{ &quot;name&quot;: &quot;my-grpc-function&quot;, &quot;inferenceUrl&quot;: &quot;/grpc&quot;, &quot;inferencePort&quot;: 8001, &quot;containerImage&quot;: &quot;nvcr.io/$ORG_NAME/grpc_echo_sample:latest&quot; }&#x27;", "keywords": []}, {"id": 87, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#grpc-function-creation-via-cli", "display_name": "gRPC Function Creation via CLI", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "grpc-function-creation-via-cli", "priority": -1, "content": "When creating the gRPC function, set the --inference-url argument to /grpc : ngc cf function create --inference-port 8001 --container-image nvcr.io/$ORG_NAME/grpc_echo_sample:latest --name my-grpc-function --inference-url /grpc", "keywords": []}, {"id": 88, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#grpc-function-creation-via-ui", "display_name": "gRPC Function Creation via UI", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "grpc-function-creation-via-ui", "priority": -1, "content": "In the Function Creation Page, set the \u201cInference Protocol\u201d to gRPC and port to whatever your gRPC server has implemented.", "keywords": []}, {"id": 89, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#grpc-function-invocation", "display_name": "gRPC Function Invocation", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "grpc-function-invocation", "priority": -1, "content": "See gRPC Invocation for details on how to authenticate and invoke your gRPC function.", "keywords": []}, {"id": 90, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#helm-based-function-creation", "display_name": "Helm-Based Function Creation", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "helm-based-function-creation", "priority": -1, "content": "Cloud functions support helm-based functions for orchestration across multiple containers.", "keywords": []}, {"id": 91, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#helm-chart-overrides", "display_name": "Helm Chart Overrides", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "helm-chart-overrides", "priority": -1, "content": "To override keys in your helm chart values.yml , you can provide the configuration parameter and supply corresponding key-value pairs in JSON format which you would like to be overridden when the function is deployed. Example helm chart override curl -X &#x27;POST&#x27; \\ &#x27;https://api.ngc.nvidia.com/v2/nvcf/deployments/functions/fe6e6589-12bb-423a-9bf6-8b9d028b8bf4/versions/fe6e6589-12bb-423a-9bf6-8b9d028b8bf4&#x27; \\ -H &#x27;Authorization: Bearer $API_KEY&#x27; \\ -H &#x27;accept: application/json&#x27; \\ -H &#x27;Content-Type: application/json&#x27; \\ -d &#x27;{ &quot;deploymentSpecifications&quot;: [{ &quot;gpu&quot;: &quot;L40&quot;, &quot;backend&quot;: &quot;OCI&quot;, &quot;maxInstances&quot;: 2, &quot;minInstances&quot;: 1, &quot;configuration&quot;: { &quot;key_one&quot;: &quot;&lt;value&gt;&quot;, &quot;key_two&quot;: { &quot;key_two_subkey_one&quot;: &quot;&lt;value&gt;&quot;, &quot;key_two_subkey_two&quot;: &quot;&lt;value&gt;&quot; } ... }, { &quot;gpu&quot;: &quot;T10&quot;, &quot;backend&quot;: &quot;GFN&quot;, &quot;maxInstances&quot;: 2, &quot;minInstances&quot;: 1 }] }&#x27;", "keywords": []}, {"id": 92, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#helm-prereq", "display_name": "Prerequisites", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "helm-prereq", "priority": -1, "content": "The helm chart must have a \u201cmini-service\u201d container defined, which will be used as the inference entry point. The name of this service in your helm chart should be supplied by setting helmChartServiceName during the function definition. This allows Cloud Functions to communicate and make inference requests to the \u201cmini-service\u201d endpoint. The servicePort defined within the helm chart should be used as the inferencePort supplied during function creation. Otherwise, Cloud Functions will not be able to reach the \u201cmini-service\u201d. Ensure you have the NGC CLI configured and have pushed your helm chart to NGC Private Registry. Refer to Managing Helm Charts Using the NGC CLI .", "keywords": []}, {"id": 93, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#id5", "display_name": "Create the \u201crequirements.txt\u201d File", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "id5", "priority": -1, "content": "This file should list the Python dependencies required for your model. Add nvidia-pytriton to your requirements.txt file. Here is an example of a requirements.txt file: requirements.txt --extra-index-url https://pypi.ngc.nvidia.com opencv-python-headless pycocotools matplotlib torch==2.1.0 nvidia-pytriton==0.3.0 numpy", "keywords": []}, {"id": 94, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#id6", "display_name": "Create the \u201cDockerfile\u201d", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "id6", "priority": -1, "content": "Create a file named Dockerfile in your model directory. It\u2019s strongly recommended to use NVIDIA-optimized containers like CUDA, Pytorch or TensorRT as your base container . They can be downloaded from the NGC Catalog . Make sure to install your Python requirements in your Dockerfile . Copy in your model source code, and model weights unless you plan to host them in NGC Private Registry. Here is an example of a Dockerfile : Dockerfile FROM nvcr.io/nvidia/cuda:12.1.1-devel-ubuntu22.04 RUN apt-get update &amp;&amp; apt-get install -y \\ git \\ python3 \\ python3-pip \\ python-is-python3 \\ libsm6 \\ libxext6 \\ libxrender-dev \\ curl \\ &amp;&amp; rm -rf /var/lib/apt/lists/* WORKDIR /workspace/ # Install requirements file COPY requirements.txt requirements.txt RUN pip install --no-cache-dir --upgrade pip RUN pip install --no-cache-dir -r requirements.txt ENV DEBIAN_FRONTEND=noninteractive # Copy model source code and weights COPY model_weights /models COPY model_source . COPY run.py . # Set run command to start PyTriton to serve the model CMD python3 run.py", "keywords": []}, {"id": 95, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#implement-the-server", "display_name": "Implement the Server", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "implement-the-server", "priority": -1, "content": "http_echo_server.py import os import time import uvicorn from pydantic import BaseModel from fastapi import FastAPI, status from fastapi.responses import StreamingResponse app = FastAPI() class HealthCheck(BaseModel): status: str = &quot;OK&quot; # Implement the health check endpoint @app.get(&quot;/health&quot;, tags=[&quot;healthcheck&quot;], summary=&quot;Perform a Health Check&quot;, response_description=&quot;Return HTTP Status Code 200 (OK)&quot;, status_code=status.HTTP_200_OK, response_model=HealthCheck) def get_health() -&gt; HealthCheck: return HealthCheck(status=&quot;OK&quot;) class Echo(BaseModel): message: str delay: float = 0.000001 repeats: int = 1 stream: bool = False # Implement the inference endpoint @app.post(&quot;/echo&quot;) async def echo(echo: Echo): if echo.stream: def stream_text(): for _ in range(echo.repeats): time.sleep(echo.delay) yield f&quot;data: {echo.message}\\n\\n&quot; return StreamingResponse(stream_text(), media_type=&quot;text/event-stream&quot;) else: time.sleep(echo.delay) return echo.message*echo.repeats # Serve the endpoints on a port if __name__ == &quot;__main__&quot;: uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=8000, workers=int(os.getenv(&#x27;WORKER_COUNT&#x27;, 500))) Note in the example above, the function\u2019s configuration during creation will be: Inference Protocol: HTTP Inference Endpoint: /echo Health Endpoint: /health Inference Port (also used for health check): 8000", "keywords": []}, {"id": 96, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#limitations", "display_name": "Limitations", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "limitations", "priority": -1, "content": "When using helm charts, the following limitations need to be taken into consideration Automatic mounting of NGC Models and Resources for your container is not supported. For any downloads (such as of assets or models) occurring within your function\u2019s containers, download size is limited by the disk space on the VM - for GFN this is 100GB approximately, and for other clusters this limit will vary. Progress/partial response reporting is not supported, including any additional artifacts generated during inferencing. Consider opting for HTTP streaming or gRPC bidirectional support. Supported k8s artifacts under Helm Chart Namespace are listed below. Others will be rejected: Deployment Service ServiceAccount Role &amp; RoleBindings ConfigMaps Secrets", "keywords": []}, {"id": 97, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#optional-push-a-container-to-the-ngc-private-registry", "display_name": "(Optional) Push a Container to the NGC Private Registry", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "optional-push-a-container-to-the-ngc-private-registry", "priority": -1, "content": "You should now be able to push a container to the NGC Private Registry. Optionally, validate this by pushing an example container from the samples repository : First clone and build the docker image. &gt; git clone https://github.com/NVIDIA/nv-cloud-function-helpers.git &gt; cd nv-cloud-function-helpers/examples/fastapi_echo_sample &gt; docker build . -t fastapi_echo_sample Now tag and push the docker image to the NGC Private Registry. &gt; docker tag fastapi_echo_sample:latest nvcr.io/$ORG_NAME/fastapi_echo_sample:latest &gt; docker push nvcr.io/$ORG_NAME/fastapi_echo_sample:latest Note that any additional slashes in the path when tagging and pushing to nvcr.io will be detected by Private Registry as specifying a team. This is most likely not what you want. Once this finishes, you\u2019ll now be able to see the new container in the NGC Private Registry Containers Page and it will be available for use in function creation.", "keywords": []}, {"id": 98, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#prerequisites", "display_name": "Prerequisites", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "prerequisites", "priority": -1, "content": "The function container must implement a gRPC port, endpoint and health check. The health check is expected to be served by the gRPC inference port, there is no need to define a separate health endpoint path. See gRPC health checking . See an example container with a gRPC server that is Cloud Functions compatible.", "keywords": []}, {"id": 99, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#push-the-docker-image", "display_name": "Push the Docker Image", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "push-the-docker-image", "priority": -1, "content": "Before beginning, ensure that you have authenticated with the NGC Docker Registry . Tag and push the docker image to the NGC Private Registry. &gt; docker tag my_model_image:latest nvcr.io/$ORG_NAME/my_model_image:latest &gt; docker push nvcr.io/$ORG_NAME/my_model_image:latest", "keywords": []}, {"id": 100, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#resources", "display_name": "Resources", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "resources", "priority": -1, "content": "Example containers can be found here . The repository also contains helper functions that are useful when authoring your container, including: Helpers that parse Cloud Functions-specific parameters on invocation Helpers that can be used to instrument your container with Cloud Functions compatible logs Helpers for working with assets After container creation, but before proceeding to deployment, it is strongly recommended to validate your container\u2019s configuration locally, see Deployment Validation . It\u2019s always a best practice to emit logs from your inference container. See Logging and Metrics for how to add logs to your container. Cloud Functions also supports third-party logging and metrics emission from your container.", "keywords": []}, {"id": 101, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#secret-management", "display_name": "Secret Management", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "secret-management", "priority": -1, "content": "For pulling containers defined as part of the helm chart from NGC Private Registry, a new value named ngcImagePullSecretName needs to be defined in the chart.The value is referred to in deployment spec as spec.imagePullSecrets.name of pods in the chart. Containers defined in the helm chart should be in the same NGC Organization and Team that the helm chart itself is being pulled from.", "keywords": []}, {"id": 102, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#storing-partial-and-complete-outputs", "display_name": "Storing Partial and Complete Outputs", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "storing-partial-and-complete-outputs", "priority": -1, "content": "When your Custom BLS generates large outputs, save them temporarily with the \u201c*.partial\u201d extension inside the NVCF-LARGE-OUTPUT-DIR directory. For instance, if you\u2019re writing an image, name it image1.partial . Once the writing of the output file is complete, rename it from \u201c*.partial\u201d to its appropriate extension. Continuing with our example, rename image1.partial to image1.jpg .", "keywords": []}, {"id": 103, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#triton-based-container-configuration", "display_name": "Triton-based Container Configuration", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "triton-based-container-configuration", "priority": -1, "content": "NVIDIA Cloud Functions is designed to work natively with Triton Inference Server based containers, including leveraging metrics and health checks from the server. Pre-built Triton docker images can be found within NGC\u2019s Container catalog . A minimum version of 23.04 (2.33.0) is required.", "keywords": []}, {"id": 104, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#usage-of-ngc-teams", "display_name": "Usage of NGC Teams", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "usage-of-ngc-teams", "priority": -1, "content": "For easier handling of authorization and accessibility, we recommend pushing your containers, helm charts, models and resources to the root of your NGC organization (i.e. \u201cNo Team\u201d), not to a team within the organization. Note that any additional slashes in the path when tagging and pushing to nvcr.io will be detected as an NGC team.", "keywords": []}, {"id": 105, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "page", "name": "cloud-function/function-creation#working-with-ngc-private-registry", "display_name": "Working with NGC Private Registry", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-creation", "anchor": "working-with-ngc-private-registry", "priority": -1, "content": "Function creation requires your model, container, helm chart and/or static resources to be hosted within NGC Private Registry as a prerequisite. Follow the steps below to optimally configure the NGC CLI to work with NGC Private Registry and Cloud Functions. NGC Private Registry has size constraints on layers, images, models and resources. Ensure that your uploaded resources conform to these constraints.", "keywords": []}, {"id": 106, "doc_id": 106, "filename": "cloud-function/function-creation.html", "domain_name": "std", "name": "cloud-function/function-creation", "display_name": "Function Creation", "type": "doc", "display_type": "Page", "docname": "cloud-function/function-creation", "anchor": "", "priority": -1, "content": "This page describes the steps to create a function within Cloud Functions. Please ensure before function creation, you\u2019ve installed and configured the NGC CLI for working with the NGC Private Registry . Functions can be created in one of three ways, listed below, and also visible in the Cloud Functions UI. Custom Container Enables any container-based workload as long as the container exposes an inference endpoint and a health check. Option to leverage any server, ex. PyTriton , FastAPI , Triton . More easily take advantage of Cloud Functions features such as the Asset Management API for input sizes &gt;5MB, HTTP streaming or gRPC, and partial response reporting. See Container-Based Function Creation . Helm Chart Enables orchestration across multiple containers. For complex use cases where a single container isn\u2019t flexible enough. Requires one \u201cmini-service\u201d container defined as the inference entry point for the function. Does not support partial response reporting, gRPC or HTTP streaming-based invocation. See Helm-Based Function Creation .", "keywords": []}, {"id": 107, "doc_id": 117, "filename": "cloud-function/function-deployment.html", "domain_name": "page", "name": "cloud-function/function-deployment#autoscaling-and-instance-counts", "display_name": "Autoscaling and Instance Counts", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-deployment", "anchor": "autoscaling-and-instance-counts", "priority": -1, "content": "Autoscaling of function instances will only occur when the maximum instance count is above the minimum instance count. Scale up or down is determined based on proprietary utilization heuristics and the function\u2019s queue depth. If the minimum instances of the deployment are set to 0, the function status will be ACTIVE , but instances will only be deployed upon first invocation of the function . After an extended idle period with no requests, the function will scale back down to 0. Therefore, setting minimum instance count to 0 is generally a best practice for saving on hardware costs, with the trade-off of function deployment time. It\u2019s especially useful when longer response times are acceptable for infrequently used functions. Invocations made while the instance is starting up will be queued until the instance is ready. Refer to Function States for understanding what each status means.", "keywords": []}, {"id": 108, "doc_id": 117, "filename": "cloud-function/function-deployment.html", "domain_name": "page", "name": "cloud-function/function-deployment#delete-a-deployment", "display_name": "Delete a Deployment", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-deployment", "anchor": "delete-a-deployment", "priority": -1, "content": "To delete a function version deployment, supply the function ID and version ID. Via UI, choose \u201cDisable Function Version\u201d in the Functions List Page for any deployed function and version. Via API: curl -X &#x27;DELETE&#x27; \\ &#x27;https://api.nvcf.nvidia.com/v2/nvcf/functions/$FUNCTION_ID&#x27; Via CLI: ngc cloud-function function deploy remove $FUNCTION_ID:$FUNCTION_VERSION_ID Specify the graceful parameter to true to require active function instances to fulfill any in-flight inference requests and drain all requests in the queue before terminating. When a deployment is deleted, the function\u2019s status will immediately become INACTIVE indicating it can no longer serve invocations.", "keywords": []}, {"id": 109, "doc_id": 117, "filename": "cloud-function/function-deployment.html", "domain_name": "page", "name": "cloud-function/function-deployment#deploy-via-api", "display_name": "Deploy via API", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-deployment", "anchor": "deploy-via-api", "priority": -1, "content": "Ensure you have an API key created, see Generate an NGC Personal API Key . First, list the available GPU clusters, types and configurations. curl --location &#x27;https://api.ngc.nvidia.com/v2/nvcf/clusterGroups&#x27; \\ --header &#x27;Accept: application/json&#x27; \\ --header &#x27;Authorization: Bearer $API_KEY&#x27; \\ See an example response below: { &quot;clusterGroups&quot;: [ { &quot;id&quot;: &quot;...&quot;, &quot;name&quot;: &quot;GCP-ASIASE1-A&quot;, &quot;ncaId&quot;: &quot;...&quot;, &quot;authorizedNcaIds&quot;: [ &quot;*&quot; ], &quot;gpus&quot;: [ { &quot;name&quot;: &quot;H100&quot;, &quot;instanceTypes&quot;: [ { &quot;name&quot;: &quot;a3-highgpu-8g_1x&quot;, &quot;description&quot;: &quot;Single H100 GPU&quot;, &quot;default&quot;: true }, { &quot;name&quot;: &quot;a3-highgpu-8g_4x&quot;, &quot;description&quot;: &quot;Four 80 GB H100 GPU&quot;, &quot;default&quot;: false }, { &quot;name&quot;: &quot;a3-highgpu-8g_2x&quot;, &quot;description&quot;: &quot;Two 80 GB H100 GPU&quot;, &quot;default&quot;: false }, { &quot;name&quot;: &quot;a3-highgpu-8g_8x&quot;, &quot;description&quot;: &quot;Eight 80 GB H100 GPU&quot;, &quot;default&quot;: false } ] } ], &quot;clusters&quot;: [ { &quot;k8sVersion&quot;: &quot;v1.29.2-gke.1060000&quot;, &quot;id&quot;: &quot;...&quot;, &quot;name&quot;: &quot;nvcf-gcp-prod-asiase1-a&quot; } ] } ... ] } In this example (which has some data omitted), the account is authorized to deploy on the GCP-ASIASE1-A cluster, which has the H100 GPU type in four different instance type configurations. Deploy the function via API by creating a deployment specification. curl --location &#x27;https://api.ngc.nvidia.com/v2/nvcf/deployments/functions/$FUNCTION_ID/versions/$FUNCTION_VERSION_ID&#x27; \\ --header &#x27;Content-Type: application/json&#x27; \\ --header &#x27;Accept: application/json&#x27; \\ --header &#x27;Authorization: Bearer $API_KEY&#x27; \\ --data &#x27;{ &quot;deploymentSpecifications&quot;: [ { &quot;backend&quot;: &quot;GCP-ASIASE1-A&quot;, &quot;gpu&quot;: &quot;H100&quot;, &quot;minInstances&quot;: &quot;1&quot; &quot;maxInstances&quot;: &quot;2&quot;, &quot;maxRequestConcurrency&quot;: 1, } ] }&#x27; Refer to the OpenAPI Specification for further API documentation.", "keywords": []}, {"id": 110, "doc_id": 117, "filename": "cloud-function/function-deployment.html", "domain_name": "page", "name": "cloud-function/function-deployment#deploy-via-cli", "display_name": "Deploy via CLI", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-deployment", "anchor": "deploy-via-cli", "priority": -1, "content": "Ensure you have an API key created, see Generate an NGC Personal API Key . Ensure you have the NGC CLI configured . First, list the available GPU clusters, types and configurations. ngc cloud-function available-gpus Deploy the function via CLI by creating a deployment specification. ngc cf function deploy create --deployment-specification $CLUSTER_BACKEND:$GPU_TYPE:$INSTANCE_TYPE:$MIN_INSTANCES:$MAX_INSTANCES $FUNCTION_ID:$FUNCTION_VERSION_ID See NGC CLI Documentation . for further commands.", "keywords": []}, {"id": 111, "doc_id": 117, "filename": "cloud-function/function-deployment.html", "domain_name": "page", "name": "cloud-function/function-deployment#deploy-via-the-ui", "display_name": "Deploy via the UI", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-deployment", "anchor": "deploy-via-the-ui", "priority": -1, "content": "Once you\u2019ve created a function, click on the kebab menu on the right to configure a deployment. First, choose the target cluster, GPU type and instance type. Next, set max concurrency, and instance counts. Your function will be occupying GPUs up to the minimum instance count, even if it\u2019s not necessarily performing work. By default, autoscaling is enabled for all functions. Therefore, it\u2019s most cost-effective to set the minimum number of instances possible and allow Cloud Functions to autoscale as needed. Optionally, set additional deployment specifications. For example, you can do this if the function is compatible across multiple GPU types, or if there are multiple regions you\u2019d like to deploy the same function to. Choose \u201cDeploy Function\u201d. Deployment times will vary depending on the cluster selected and available capacity.", "keywords": []}, {"id": 112, "doc_id": 117, "filename": "cloud-function/function-deployment.html", "domain_name": "page", "name": "cloud-function/function-deployment#deploying-a-function", "display_name": "Deploying a Function", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-deployment", "anchor": "deploying-a-function", "priority": -1, "content": "Before deploying a function, you must create it first. Once created it will be listed as INACTIVE . See Function Creation . Your Cloud Functions account will have access to various GPU clusters, instance types and configurations up to a set amount. This is determined by your NVIDIA Account Manager. Each function version can have a different deployment configuration, allowing for heterogeneous computing infrastructure to be used across a single function endpoint. Once a deployment is created, it can be updated at any time, for example, to change min or max instance counts.", "keywords": []}, {"id": 113, "doc_id": 117, "filename": "cloud-function/function-deployment.html", "domain_name": "page", "name": "cloud-function/function-deployment#deployment-failures", "display_name": "Deployment Failures", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-deployment", "anchor": "deployment-failures", "priority": -1, "content": "Depending on the size of your containers and models, it can take anywhere from 2 minutes to 30 minutes for your function to deploy, although durations up to 2 hours are permitted. This is also dependent on whether your function is deploying from a cold start, or whether it\u2019s scaling up or down (often much faster due to caching in place). Monitor your function\u2019s instance count and scaling via the Function Metrics Page . If you believe your function should have deployed already, or if it has entered an error state, review the logs to understand what happened, or reach out to your NVCF Support Team. Below are some common deployment failures: Failure Type Description Function configuration problems This occurs due to incorrect inference or health endpoints and ports defined, causing the container to be marked unhealthy. Try running Deployment Validation on the container locally to rule out configuration issues. Inadequate capacity for the chosen cluster This will usually be indicated in the deployment failure error message in the UI. Try reducing the number of instances you are requesting or changing the GPU/instance type used by your function. Container in restart loop This will be indicated in the inference container logs (if your container is configured to emit logs) and is fixed by debugging and updating your inferencing container code. Model file not found This error typically occurs when the inference container expects a model file in a specified location, but the file is not present. Ensure the path for your model files is correct and the necessary files, like config.json , are available at that location. The config.json file should be located under the /config/models/$model-name directory.", "keywords": []}, {"id": 114, "doc_id": 117, "filename": "cloud-function/function-deployment.html", "domain_name": "page", "name": "cloud-function/function-deployment#deployment-validation", "display_name": "Deployment Validation", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-deployment", "anchor": "deployment-validation", "priority": -1, "content": "If your function is container-based, before deploying, it\u2019s strongly recommended to run the local Deployment Validator , to catch common configuration issues and enable faster development cycles. Clone the helper repository and install the validator: &gt; git clone https://github.com/NVIDIA/nv-cloud-function-helpers.git &gt; cd nv-cloud-function-helpers/local_deployment_test/ &gt; pip install -r requirements.txt Run the validator on your container. Supported validation arguments: --protocol --health-endpoint --inference-endpoint --container-port &gt; python3 test_container.py --image-name $CONTAINER_IMAGE_NAME --protocol http --health-endpoint v2/health/live --inference-endpoint /v2/models/echo/infer --container-port 8000 Once checks have passed, you\u2019ll be prompted to test out your container\u2019s inference endpoint. Paste your function\u2019s expected inference endpoint JSON request body into the temporary file that is generated. If everything is successful, proceed to deploy your function.", "keywords": []}, {"id": 115, "doc_id": 117, "filename": "cloud-function/function-deployment.html", "domain_name": "page", "name": "cloud-function/function-deployment#function-queueing", "display_name": "Function Queueing", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-deployment", "anchor": "function-queueing", "priority": -1, "content": "The below describes cases which trigger functions to queue. Cloud Functions maintains one queue per function version ID. For synchronous HTTP requests - queuing is triggered when the function reaches its max concurrency limit of requests currently in progress. Example A single function instance is deployed with a max concurrency is set to 2 and min and max instance count of 1. 3 invocation requests hit the Cloud Functions API via /pexec endpoint for the function. Cloud Functions API will forward 2 function invocation requests. The remaining request will be queued, will return 202 and must be handled by HTTP polling . For streaming requests such as gRPC - queuing is triggered when the function reaches its max concurrency limit of current connections. Example gRPC function is deployed with a max concurrency set to 2 and min and max instance count of 1. 3 connection requests hit the Cloud Functions gRPC endpoint for the function. 2 connections will be created to the function. The remaining connection request will wait to connect until one of the 2 current connections is closed.", "keywords": []}, {"id": 116, "doc_id": 117, "filename": "cloud-function/function-deployment.html", "domain_name": "page", "name": "cloud-function/function-deployment#key-concepts", "display_name": "Key Concepts", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-deployment", "anchor": "key-concepts", "priority": -1, "content": "Term Description Cluster Group (Backend) A collection of one or more (though usually one) clusters to deploy on, for example - a CSP such as Azure, OCI, GCP or an NVIDIA-specific cluster like GFN. Instance Type Each GPU type can support one or more instance types, which are different configurations, such as the number of CPU cores, and the number of GPUs per node. Min Instances The minimum number of instances your function should be deployed on. Max Instances The maximum number of instances your function is allowed to autoscale to. Max Concurrency The number of simultaneous invocations your container can handle at any given time. Function Request Queue A first in first out queue that is created during function version deployment, which buffers incoming requests based on function \u201cworker\u201d instance availability. Autoscaling Automatic scaling up or down of instances from minimum instance count to maximum instance count, based on utilization heuristics and queue depth.", "keywords": []}, {"id": 117, "doc_id": 117, "filename": "cloud-function/function-deployment.html", "domain_name": "std", "name": "cloud-function/function-deployment", "display_name": "Function Deployment", "type": "doc", "display_type": "Page", "docname": "cloud-function/function-deployment", "anchor": "", "priority": -1, "content": "This page describes deployment concepts and steps to deploy a function using Cloud Functions. A function deployment refers to one or more function instances running on a cluster. See Function Lifecycle for more key terminology and diagrams.", "keywords": []}, {"id": 118, "doc_id": 126, "filename": "cloud-function/function-lifecycle.html", "domain_name": "page", "name": "cloud-function/function-lifecycle#asset-management", "display_name": "Asset Management", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-lifecycle", "anchor": "asset-management", "priority": -1, "content": "Name Method Endpoint Usage Create Asset POST /v2/nvcf/assets Creates an asset id and a corresponding pre-signed URL to upload a file. List Assets GET /v2/nvcf/assets Returns a list of assets associated with the account. Delete Asset DELETE /v2/nvcf/assets/{assetId} Deletes an asset using the specified asset id. Read more about using assets here .", "keywords": []}, {"id": 119, "doc_id": 126, "filename": "cloud-function/function-lifecycle.html", "domain_name": "page", "name": "cloud-function/function-lifecycle#function-creation-management-deployment", "display_name": "Function Creation, Management & Deployment", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-lifecycle", "anchor": "function-creation-management-deployment", "priority": -1, "content": "The table below provides an overview of the function lifecycle API endpoints and their respective usages. Name Method Endpoint Usage Register Function POST /v2/nvcf/functions Creates a new function. Register Function Version POST /v2/nvcf/functions/{functionId}/versions Creates a new version of a function. Delete Function Version DELETE /v2/nvcf/functions/{functionId}/versions/{functionVersionId} Deletes a function specified by its ID. List Functions GET /v2/nvcf/functions Retrieves a list of functions associated with the account. List Function Versions GET /v2/nvcf/functions/{functionId}/versions Retrieves a list of versions for a specific function. Retrieve Function Details GET /v2/nvcf/functions/{functionId}/versions/{functionVersionId} Retrieves details of a specific function version. Create Function Version Deployment POST /v2/nvcf/deployments/functions/{functionId}/versions/{functionVersionId} Initiates the deployment process for a function version. Delete Function Version Deployment DELETE /v2/nvcf/deployments/functions/{functionId}/versions/{functionVersionId} Initiates the undeployment process for a function version. Retrieve Function Version Deployment GET /v2/nvcf/deployments/functions/{functionId}/versions/{functionVersionId} Retrieves details of a specific function version deployment. Update Function Version Deployment PUT /v2/nvcf/deployments/functions/{functionId}/versions/{functionVersionId} Updates the configuration of a function version deployment.", "keywords": []}, {"id": 120, "doc_id": 126, "filename": "cloud-function/function-lifecycle.html", "domain_name": "page", "name": "cloud-function/function-lifecycle#function-invocation", "display_name": "Function Invocation", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-lifecycle", "anchor": "function-invocation", "priority": -1, "content": "The table below provides an overview of the Function invocation API endpoints and their respective usages. Name Method Endpoint Usage Invoke Function POST v2/nvcf/pexec/functions/{functionId} Invokes the specified function\u2019s inference endpoint and returns the results, if available. Cloud Functions randomly selects one of the active versions of the specified function for inference. Invoke Function Version POST v2/nvcf/pexec/functions/{functionId}/versions/{functionVersionId} Invokes the specified version under the specified function for inference and returns the results. Get Function Invocation Status GET /v2/nvcf/pexec/status/{invocationRequestId} Used to poll for results of an inference request when a 202 response is returned. If the original invocation request returns a 200 response, calls to this API are not necessary. Read more about using the invocation API here .", "keywords": []}, {"id": 121, "doc_id": 126, "filename": "cloud-function/function-lifecycle.html", "domain_name": "page", "name": "cloud-function/function-lifecycle#function-metadata", "display_name": "Function Metadata", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-lifecycle", "anchor": "function-metadata", "priority": -1, "content": "When using the Cloud Functions API to create a function, it\u2019s possible to specify a function description and a list of tags as strings as part of the function creation request body. This metadata is then returned in all responses that include the function definition. This is an API-only feature at this time. Please see the OpenAPI Specification for usage.", "keywords": []}, {"id": 122, "doc_id": 126, "filename": "cloud-function/function-lifecycle.html", "domain_name": "page", "name": "cloud-function/function-lifecycle#function-states", "display_name": "Function States", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-lifecycle", "anchor": "function-states", "priority": -1, "content": "A function can be in any of the following states: ACTIVE - If the function can receive invocations. Only when a function is ACTIVE can it be invoked. ERROR - If all function instances are in an ERROR state. INACTIVE - When a function is created but not yet deployed, it is INACTIVE . When a function is undeployed, the state is changed from ACTIVE to INACTIVE . DEPLOYING - When a function is being deployed and the instances are still coming up to reach the minimum instance count.", "keywords": []}, {"id": 123, "doc_id": 126, "filename": "cloud-function/function-lifecycle.html", "domain_name": "page", "name": "cloud-function/function-lifecycle#key-concepts", "display_name": "Key Concepts", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-lifecycle", "anchor": "key-concepts", "priority": -1, "content": "See below for an overview of some key basic concepts within Cloud Functions. Term Description NVIDIA GPU Cloud (NGC) A portal of enterprise services, software, and management tools supporting end-to-end AI. NGC Private Registry Registry integrated with Cloud Functions for storing custom containers, models, resources, and helm charts. NVIDIA Cloud Account ID (NCA ID) NVIDIA customer billing entity that cloud services are associated with. Function User-defined encapsulated code that implements a server exposing at least one inference endpoint, either based on a container, or helm chart. Function Instance A single deployed copy of a function running on a cluster. Function Deployment One or more function instances running on a cluster. Function Invocation The action of calling (via the Cloud Functions API) a function\u2019s inference endpoint. Asset A file that can be uploaded, downloaded, and used by your function when invoked. Required for function input sizes &gt;5MB. Cluster A collection of GPU-powered Kubernetes nodes/pods. GPU Instance Type Refers to any one of the supported GPU configurations within Cloud Functions, including the GPU model, number of GPUs on a single node, number of CPU cores, etc.", "keywords": []}, {"id": 124, "doc_id": 126, "filename": "cloud-function/function-lifecycle.html", "domain_name": "page", "name": "cloud-function/function-lifecycle#visibility-cluster-groups-gpus", "display_name": "Visibility, Cluster Groups & GPUs", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-lifecycle", "anchor": "visibility-cluster-groups-gpus", "priority": -1, "content": "Name Method Endpoint Usage Get Queue Length for Function id GET /v2/nvcf/queues/functions/{functionId} Returns a list containing a single element with corresponding queue length for the specified Function. Get Queue Length for Version id GET /v2/nvcf/queues/functions/{functionId}/versions/{functionVersionId} Returns a list containing a single element with corresponding queue length for the specified Function version id. Get Available GPUs GET /v2/nvcf/supportedGpus Returns a list of GPU types you have access too. Get Queue Position for Request id GET /v2/nvcf/queues/{requestId}/position Returns estimated position in queue, up to 1000, for a specific request id of a function invocation request.", "keywords": []}, {"id": 125, "doc_id": 126, "filename": "cloud-function/function-lifecycle.html", "domain_name": "page", "name": "cloud-function/function-lifecycle#workflow", "display_name": "Workflow", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-lifecycle", "anchor": "workflow", "priority": -1, "content": "The workflow when using Cloud Functions is usually as follows. Function Creation : Define your function with a container, or helm chart. Function Deployment : Deploy your function on a cluster. Function Invocation : Invoke your function\u2019s inference endpoint. Function Management : Manage your deployed function, for example, add new versions. Function Monitoring : Monitor your function by using the Cloud Functions UI and third-party tooling.", "keywords": []}, {"id": 126, "doc_id": 126, "filename": "cloud-function/function-lifecycle.html", "domain_name": "std", "name": "cloud-function/function-lifecycle", "display_name": "Function Lifecycle", "type": "doc", "display_type": "Page", "docname": "cloud-function/function-lifecycle", "anchor": "", "priority": -1, "content": "Cloud \u201cfunctions\u201d are an abstraction that allows you to run your code without managing deployments and infrastructure. Cloud Functions simplifies hosting AI inference and fine-tuning workloads in the cloud by automatically enabling access to GPU capacity and autoscaling. Cloud functions are generally considered stateless. Therefore, function authors are only responsible for maintaining their AI models and associated code . This is highlighted in the diagram below in green. To use Cloud Functions, you create a function, then define a deployment specification for it, and deploy it on one of the available GPU-backed clusters hosted by NVIDIA. A Cloud Functions account can contain multiple functions, each with multiple function versions. Each function created also creates a single function version. Cloud Functions supports function invocation (calling of the function\u2019s inference endpoint) at the function ID level or the function version ID level. You can create a single function and version and invoke only this function version, or create multiple versions of the same function and spread invocation across all versions.", "keywords": []}, {"id": 127, "doc_id": 131, "filename": "cloud-function/function-management.html", "domain_name": "page", "name": "cloud-function/function-management#function-update-flow-a-b-testing", "display_name": "Function Update Flow (A/B Testing)", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-management", "anchor": "function-update-flow-a-b-testing", "priority": -1, "content": "To initiate an update, create a new version of your function, through API, UI or CLI. Deploy the new version of your function. Ideally, replicate the min and max instance counts to be the same as the existing version of your function. This ensures it will successfully handle the same amount of traffic. Invocations to this function ID will now be routing to both the new and old versions of the function after this deploy based on function instance availability. If you do not want this to occur, for example in order to perform some testing first, deploy a separate development function. Verify that the endpoint deployed successfully by invoking it a few times. Do any additional verification such as load testing or comparing your third party monitoring and analytics across both versions. Once satisfied, to complete your update, undeploy the old version of the function. All invocations will now successfully route to the new version of the function.", "keywords": []}, {"id": 128, "doc_id": 131, "filename": "cloud-function/function-management.html", "domain_name": "page", "name": "cloud-function/function-management#function-versioning", "display_name": "Function Versioning", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-management", "anchor": "function-versioning", "priority": -1, "content": "Each time you create a new function a function ID will be created along with a function version ID. You can create additional versions of this function by specifying other models/containers/helm charts to use and additional configurations etc. Here is a sample API call: curl -X &#x27;POST&#x27; \\ &#x27;https://api.nvcf.nvidia.com/v2/nvcf/functions/$FUNCTION_ID/versions&#x27; \\ -H &#x27;accept: application/json&#x27; \\ -H &#x27;Authorization: Bearer $API_KEY&#x27; -d &#x27;{ &quot;name&quot;: &quot;echo_function&quot;, &quot;inferenceUrl&quot;: &quot;/echo&quot;, &quot;containerImage&quot;: &quot;nvcr.io/$ORG_NAME/echo:latest&quot;, &quot;apiBodyFormat&quot;: &quot;CUSTOM&quot; }&#x27; Multiple function versions allow for different deployment configurations for each version while still being accessible through a single function endpoint. Multiple function versions can also be used to support A/B testing. Function versioning should only be used if the APIs between the various versions are compatible. Different APIs should be created as new functions.", "keywords": []}, {"id": 129, "doc_id": 131, "filename": "cloud-function/function-management.html", "domain_name": "page", "name": "cloud-function/function-management#function-versioning-best-practices", "display_name": "Function Versioning Best Practices", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-management", "anchor": "function-versioning-best-practices", "priority": -1, "content": "Leverage function versioning when making transparent production updates to a function. See also Container Versioning .", "keywords": []}, {"id": 130, "doc_id": 131, "filename": "cloud-function/function-management.html", "domain_name": "page", "name": "cloud-function/function-management#prerequisites", "display_name": "Prerequisites", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-management", "anchor": "prerequisites", "priority": -1, "content": "First, ensure that your client application is invoking your function by function ID, not function version ID. This allows the spread of requests to be even across all deployed versions of your function. Invocation API endpoint should be https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/{functionID}", "keywords": []}, {"id": 131, "doc_id": 131, "filename": "cloud-function/function-management.html", "domain_name": "std", "name": "cloud-function/function-management", "display_name": "Function Management", "type": "doc", "display_type": "Page", "docname": "cloud-function/function-management", "anchor": "", "priority": -1, "content": "", "keywords": []}, {"id": 132, "doc_id": 139, "filename": "cloud-function/function-monitoring.html", "domain_name": "page", "name": "cloud-function/function-monitoring#emit-and-view-inference-container-logs", "display_name": "Emit and View Inference Container Logs", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-monitoring", "anchor": "emit-and-view-inference-container-logs", "priority": -1, "content": "View inference container logs in the Cloud Functions UI via the \u201cLogs\u201d tab in the function details page. To get here, click any function version from the \u201cFunctions\u201d list and click \u201cView Details\u201d on the side panel to the right. Logs are currently available with up to 48 hours history, with the ability to view as expanded rows for scanning, or as a \u201cwindow\u201d view for ease of copying and pasting. Note as a prerequisite, your inference container will have to be instrumented to emit logs. This is highly recommended.", "keywords": []}, {"id": 133, "doc_id": 139, "filename": "cloud-function/function-monitoring.html", "domain_name": "page", "name": "cloud-function/function-monitoring#how-to-add-logs-to-your-inference-container", "display_name": "How to Add Logs to Your Inference Container", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-monitoring", "anchor": "how-to-add-logs-to-your-inference-container", "priority": -1, "content": "Here is an example of adding NVCF-compatible logs. The helper function for logging below, along with other helper functions, can be imported from the Helper Functions repository. import logging def get_logger() -&gt; logging.Logger: &quot;&quot;&quot; gets a Logger that logs in a format compatible with NVCF :return: logging.Logger &quot;&quot;&quot; sys.stdout.reconfigure(encoding=&quot;utf-8&quot;) logging.basicConfig( level=logging.INFO, format=&quot;%(asctime)s [%(levelname)s] [INFERENCE] %(message)s&quot;, handlers=[logging.StreamHandler(sys.stdout)], ) logger = logging.getLogger(__name__) return logger class MyServer: def __init__(self): self.logger = get_logger() def _infer_fn(self, request): self.logger.info(&quot;Got a request!&quot;)", "keywords": []}, {"id": 134, "doc_id": 139, "filename": "cloud-function/function-monitoring.html", "domain_name": "page", "name": "cloud-function/function-monitoring#instrument-with-opentelemetry", "display_name": "Instrument with OpenTelemetry", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-monitoring", "anchor": "instrument-with-opentelemetry", "priority": -1, "content": "Users can (auto)instrument their container functions with OpenTelemetry SDK and have the signals (logs, traces and metrics) to observability backend such as Grafana Cloud. See examples of container functions in GitHub .", "keywords": []}, {"id": 135, "doc_id": 139, "filename": "cloud-function/function-monitoring.html", "domain_name": "page", "name": "cloud-function/function-monitoring#logging-and-metrics", "display_name": "Logging and Metrics", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-monitoring", "anchor": "logging-and-metrics", "priority": -1, "content": "This section gives an overview of available metrics and logs within the Cloud Functions UI. Note that for full observability of production workloads, it\u2019s recommended to emit logs, metrics, analytics etc. to third-party monitoring tools from within your container.", "keywords": []}, {"id": 136, "doc_id": 139, "filename": "cloud-function/function-monitoring.html", "domain_name": "page", "name": "cloud-function/function-monitoring#logging-and-metrics-internal", "display_name": "Logging and Metrics (Internal)", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-monitoring", "anchor": "logging-and-metrics-internal", "priority": -1, "content": "Internal NVIDIAN users have access to additional logging and metrics. Refer to Function Monitoring &amp; Reliability . Besides additional logging and metrics, internal NVIDIAN users can leverage a Helm Chart Observability &lt;https://gitlab-master.nvidia.com/nvcf/monitoring/helm-chart-observability#helm-chart-observability&gt; to collect and export metrics and traces from their Helm Chart functions to Kratos and Lightstep. Documentation for this Helm Chart is available here .", "keywords": []}, {"id": 137, "doc_id": 139, "filename": "cloud-function/function-monitoring.html", "domain_name": "page", "name": "cloud-function/function-monitoring#troubleshooting", "display_name": "Troubleshooting", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-monitoring", "anchor": "troubleshooting", "priority": -1, "content": "For troubleshooting deployment failures see Deployment Failures . For troubleshooting invocation failures see Statuses and Errors . See below for adding logging to your inference container, and viewing metrics.", "keywords": []}, {"id": 138, "doc_id": 139, "filename": "cloud-function/function-monitoring.html", "domain_name": "page", "name": "cloud-function/function-monitoring#view-function-metrics", "display_name": "View Function Metrics", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-monitoring", "anchor": "view-function-metrics", "priority": -1, "content": "NVCF exposes the following metrics by default. Instance counts (current, min and max) Invocation activity and queue depth Total invocation count, success rate and failure count Average inference time Metrics are viewable upon clicking any function from the \u201cFunctions\u201d list page. The function overview page will display aggregated values across all function versions. When clicking on a function version\u2019s details page, you will then see metrics for this specific function version. There may be up to a 5-minute delay on metric ingestion. Any time-series queries within the page are aggregated at 5-minute intervals with a step set to show 500 data points. All stat queries are based on the total selected period and reduced to either show the latest total value or a mean value.", "keywords": []}, {"id": 139, "doc_id": 139, "filename": "cloud-function/function-monitoring.html", "domain_name": "std", "name": "cloud-function/function-monitoring", "display_name": "Function Monitoring", "type": "doc", "display_type": "Page", "docname": "cloud-function/function-monitoring", "anchor": "", "priority": -1, "content": "", "keywords": []}, {"id": 140, "doc_id": 141, "filename": "cloud-function/function-permissions.html", "domain_name": "page", "name": "cloud-function/function-permissions#listing-functions-via-api", "display_name": "Listing Functions via API", "type": "section", "display_type": "Page section", "docname": "cloud-function/function-permissions", "anchor": "listing-functions-via-api", "priority": -1, "content": "All functions available to your Cloud Functions account are listable via API with the following curl. curl --location &#x27;https://api.ngc.nvidia.com/v2/nvcf/functions?visibility=authorized&amp;visibility=private&amp;visibility=public&#x27; \\ --header &#x27;Accept: application/json&#x27; \\ --header &#x27;Authorization: Bearer $API_KEY&#x27; Available parameters for filtering include: visibility=private - Account-owned functions visibility=authorized - Functions shared to your account visibility=public - Functions created by NVIDIA available to all accounts", "keywords": []}, {"id": 141, "doc_id": 141, "filename": "cloud-function/function-permissions.html", "domain_name": "std", "name": "cloud-function/function-permissions", "display_name": "Function Permissions", "type": "doc", "display_type": "Page", "docname": "cloud-function/function-permissions", "anchor": "", "priority": -1, "content": "This page describes function invocation permissions and public and shared function access within your Cloud Functions account. Function permissions within Cloud Functions fall into three categories: Account-owned functions These are functions that are scoped to just your Cloud Functions account, within your NGC organization, listed as \u201cMy Functions\u201d. Any user within your NGC organization can invoke account-owned functions by generating a Personal API Key. See Generate an NGC Personal API Key . Metrics and logs in the Cloud Functions UI are currently only available for account-owned functions. Shared functions These are functions that are shared to your Cloud Functions account, from a different Cloud Functions account, listed as \u201cShared Functions\u201d. Functions shared to your account are invoke-able by any user within your NGC organization, via generating a Personal API Key. Public functions These are NVIDIA NIM functions that are maintained by NVIDIA and shared to all NGC organizations for public access, listed as \u201cCreated by NVIDIA\u201d. The functions listed here are also available to invoke via NVIDIA\u2019s AI API Catalog . Refer to the samples within the catalog for instructions on how to invoke the function. Reach out to your NVIDIA account manager to request shared function capabilities for your account\u2019s functions and options for self-hosting NIMs within your account. All function categories are visible in the UI under the Functions List page tabs and are also visible in the API when listing functions.", "keywords": []}, {"id": 142, "doc_id": 144, "filename": "cloud-function/overview.html", "domain_name": "page", "name": "cloud-function/overview#function-types", "display_name": "Function Types", "type": "section", "display_type": "Page section", "docname": "cloud-function/overview", "anchor": "function-types", "priority": -1, "content": "Cloud Functions supports two different workload and deployment types for defining functions - \u201cContainer\u201d and \u201cHelm Chart\u201d function types. The workloads are ephemeral and preemptable, so ensure you are not running long tasks, such as those spanning several hours, without expecting to save your work to the local disk. Cloud Function models, containers, helm charts, and any additional resources are hosted by and pulled from NGC Private Registry . See Function Creation for an in-depth overview of each function type.", "keywords": []}, {"id": 143, "doc_id": 144, "filename": "cloud-function/overview.html", "domain_name": "page", "name": "cloud-function/overview#getting-started", "display_name": "Getting Started", "type": "section", "display_type": "Page section", "docname": "cloud-function/overview", "anchor": "getting-started", "priority": -1, "content": "The best way to get started with Cloud Functions is to follow along with the Quickstart .", "keywords": []}, {"id": 144, "doc_id": 144, "filename": "cloud-function/overview.html", "domain_name": "std", "name": "cloud-function/overview", "display_name": "Overview", "type": "doc", "display_type": "Page", "docname": "cloud-function/overview", "anchor": "", "priority": -1, "content": "NVIDIA Cloud Functions (NVCF) is a serverless API to deploy &amp; manage AI workloads on GPUs, which provides security, scale and reliability to your workloads. The API to access the workloads is un-opinionated and supports HTTP polling, HTTP streaming &amp; gRPC. Cloud Functions is available via the NGC Portal . To gain access to Cloud Functions, talk to your NVIDIA Account Manager. Cloud Functions is primarily suited for shorter running, preemptable workloads such as inferencing and fine-tuning. See Function Lifecycle for key concepts and terminology.", "keywords": []}, {"id": 145, "doc_id": 150, "filename": "cloud-function/quickstart.html", "domain_name": "page", "name": "cloud-function/quickstart#clone-build-and-push-the-docker-image-to-ngc-private-registry", "display_name": "Clone, Build and Push the Docker Image to NGC Private Registry", "type": "section", "display_type": "Page section", "docname": "cloud-function/quickstart", "anchor": "clone-build-and-push-the-docker-image-to-ngc-private-registry", "priority": -1, "content": "First clone and build the docker image. &gt; git clone https://github.com/NVIDIA/nv-cloud-function-helpers.git &gt; cd nv-cloud-function-helpers/examples/fastapi_echo_sample &gt; docker build . -t fastapi_echo_sample Now tag and push the docker image to the NGC Private Registry. &gt; docker tag fastapi_echo_sample:latest nvcr.io/$ORG_NAME/fastapi_echo_sample:latest &gt; docker push nvcr.io/$ORG_NAME/fastapi_echo_sample:latest Once this finishes, you\u2019ll now be able to see the new container in the NGC Private Registry Containers Page and it will be available for use in function creation.", "keywords": []}, {"id": 146, "doc_id": 150, "filename": "cloud-function/quickstart.html", "domain_name": "page", "name": "cloud-function/quickstart#create-deploy-the-function-using-the-cloud-functions-api", "display_name": "Create & Deploy the Function Using the Cloud Functions API", "type": "section", "display_type": "Page section", "docname": "cloud-function/quickstart", "anchor": "create-deploy-the-function-using-the-cloud-functions-api", "priority": -1, "content": "Ensure you have an API key created, see Generate an NGC Personal API Key . Find your NGC organization name within the NGC Organization Profile Page . This is not the Display Name. For example: qdrlnbkss123 . Create the function via API by running the following curl with an $API_KEY and your $ORG_NAME . curl --location &#x27;https://api.ngc.nvidia.com/v2/nvcf/functions&#x27; \\ --header &#x27;Content-Type: application/json&#x27; \\ --header &#x27;Accept: application/json&#x27; \\ --header &#x27;Authorization: Bearer $API_KEY&#x27; \\ --data &#x27;{ &quot;name&quot;: &quot;my-echo-function&quot;, &quot;inferenceUrl&quot;: &quot;/echo&quot;, &quot;healthUri&quot;: &quot;/health&quot;, &quot;inferencePort&quot;: 8000, &quot;containerImage&quot;: &quot;nvcr.io/$ORG_NAME/fastapi_echo_sample:latest&quot; }&#x27; It will return a function ID listed as id and a function version ID listed as versionId . List functions you\u2019ve created via API using the following curl: curl --location &#x27;https://api.ngc.nvidia.com/v2/nvcf/functions?visibility=private&#x27; \\ --header &#x27;Accept: application/json&#x27; \\ --header &#x27;Authorization: Bearer $API_KEY&#x27; \\ Deploy the function via API using the following curl: curl --location &#x27;https://api.ngc.nvidia.com/v2/nvcf/deployments/functions/$FUNCTION_ID/versions/$FUNCTION_VERSION_ID&#x27; \\ --header &#x27;Content-Type: application/json&#x27; \\ --header &#x27;Accept: application/json&#x27; \\ --header &#x27;Authorization: Bearer $API_KEY&#x27; \\ --data &#x27;{ &quot;deploymentSpecifications&quot;: [ { &quot;backend&quot;: &quot;GFN&quot;, &quot;gpu&quot;: &quot;L40G&quot;, &quot;maxInstances&quot;: &quot;1&quot;, &quot;minInstances&quot;: &quot;1&quot; } ] }&#x27; Refer to the OpenAPI Specification for further API documentation.", "keywords": []}, {"id": 147, "doc_id": 150, "filename": "cloud-function/quickstart.html", "domain_name": "page", "name": "cloud-function/quickstart#create-deploy-the-function-using-the-cloud-functions-ui", "display_name": "Create & Deploy the Function Using the Cloud Functions UI", "type": "section", "display_type": "Page section", "docname": "cloud-function/quickstart", "anchor": "create-deploy-the-function-using-the-cloud-functions-ui", "priority": -1, "content": "Navigate to the Cloud Functions UI and choose \u201cCreate Function\u201d, then \u201cCustom Container\u201d. Enter the details for the fastapi_echo_sample . In this container, the inference endpoint is /echo , the health endpoint is /health and it exposes these on the default 8000 port. No other configuration changes are needed. Once created, the function will show up in the Functions List Page with the status INACTIVE because it is not yet deployed. By default, a function ID and function version ID is generated every time you create a new function or new function version. Deploy the function by clicking the kebab menu on the right of the function row.", "keywords": []}, {"id": 148, "doc_id": 150, "filename": "cloud-function/quickstart.html", "domain_name": "page", "name": "cloud-function/quickstart#create-deploy-the-function-using-the-ngc-cli", "display_name": "Create & Deploy the Function Using the NGC CLI", "type": "section", "display_type": "Page section", "docname": "cloud-function/quickstart", "anchor": "create-deploy-the-function-using-the-ngc-cli", "priority": -1, "content": "Ensure you have an API key created, see Generate an NGC Personal API Key . Ensure you have the NGC CLI configured . Run the following command to create the function, it will return a function ID and function version ID. ngc cf function create --health-uri /health --inference-port 8000 --container-image nvcr.io/$ORG_NAME/fastapi_echo_sample:latest --name my-echo-function --inference-url /echo List functions you\u2019ve created using the following command: ngc cf function list --access-filter=private Deploy the function using the following command: ngc cf function deploy create --deployment-specification GFN:L40:gl40_1.br20_2xlarge:1:1 $FUNCTION_ID:$FUNCTION_VERSION_ID See NGC CLI Documentation . for further commands.", "keywords": []}, {"id": 149, "doc_id": 150, "filename": "cloud-function/quickstart.html", "domain_name": "page", "name": "cloud-function/quickstart#invoke-the-function", "display_name": "Invoke the Function", "type": "section", "display_type": "Page section", "docname": "cloud-function/quickstart", "anchor": "invoke-the-function", "priority": -1, "content": "Once your function is deployed, invoke its inference endpoint with the following curl: curl --location &#x27;https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/$FUNCTION_ID&#x27; \\ --header &#x27;Content-Type: application/json&#x27; \\ --header &#x27;Authorization: Bearer $API_KEY&#x27; \\ --data &#x27;{ &quot;message&quot;: &quot;hello world&quot; }&#x27;", "keywords": []}, {"id": 150, "doc_id": 150, "filename": "cloud-function/quickstart.html", "domain_name": "std", "name": "cloud-function/quickstart", "display_name": "Quickstart", "type": "doc", "display_type": "Page", "docname": "cloud-function/quickstart", "anchor": "", "priority": -1, "content": "This guide gives an end-to-end workflow of how to create and deploy a container-based function on Cloud Functions, using the fastapi_echo_sample container . This is a minimal server that\u2019s been configured to work with Cloud Functions, echoing whatever is sent to its inference endpoint. Before beginning, ensure that you have created an NGC Personal API Key and authenticated with the NGC Docker Registry .", "keywords": []}, {"id": 151, "doc_id": 151, "filename": "index.html", "domain_name": "std", "name": "index", "display_name": "NVIDIA Cloud Functions", "type": "doc", "display_type": "Page", "docname": "index", "anchor": "", "priority": -1, "content": "Overview Function Types Getting Started Quickstart Clone, Build and Push the Docker Image to NGC Private Registry Create &amp; Deploy the Function Using the Cloud Functions UI Create &amp; Deploy the Function Using the Cloud Functions API Create &amp; Deploy the Function Using the NGC CLI Invoke the Function Elastic NIM About NIM Prerequisites Setup NGC Authentication Download the NIM Export the NGC API Key NGC CLI Tool Docker Login to NGC List Available NIMs Upload NIM to Private Registry Create the Function Validating the Function Deployment Troubleshooting API OpenAPI Specification Authorization Generate an NGC Personal API Key API Key Scopes and Domains JWT Based Authorization Using the NVCF Invocation API HTTP (Polling) Polling After Initial Invocation Large Responses (302 Status Code) HTTP Streaming Advantages of HTTP Streaming gRPC Statuses and Errors Inference Container Status Codes and Responses NVCF API Status Codes Common Function Invocation Errors Function Lifecycle Key Concepts Function States Workflow Function Lifecycle Endpoints Function Creation, Management &amp; Deployment Function Metadata Function Invocation Asset Management Visibility, Cluster Groups &amp; GPUs Function Creation Working with NGC Private Registry Generate an NGC Personal API Key Download &amp; Configure the NGC CLI Authenticate with NGC Docker Registry (Optional) Push a Container to the NGC Private Registry Best Practices with NGC Docker Registry and Cloud Functions Container Versioning Usage of NGC Teams Container-Based Function Creation Resources Container Endpoints Composing a FastAPI Container Create the \u201crequirements.txt\u201d File Implement the Server Create the Dockerfile Build the Container &amp; Create the Function Composing a PyTriton Container Create the \u201crequirements.txt\u201d File Create the \u201crun.py\u201d File Create the \u201cDockerfile\u201d Build the Docker Image Push the Docker Image Create the Function Additional Examples Triton-based Container Configuration Configuration Creating Functions with NGC Models &amp; Resources Creating gRPC-based Functions Prerequisites gRPC Function Creation via UI gRPC Function Creation via CLI gRPC Function Creation via API gRPC Function Invocation Available Container Variables Adding Partial Response (Progress) Storing Partial and Complete Outputs Creating a Progress File Best Practices Helm-Based Function Creation Prerequisites Secret Management Create a Helm-based Function Limitations Helm Chart Overrides Function Deployment Deployment Validation Deploying a Function Key Concepts Function Queueing Autoscaling and Instance Counts Deploy via the UI Deploy via API Deploy via CLI Delete a Deployment Deployment Failures Function Management Function Versioning Function Versioning Best Practices Prerequisites Function Update Flow (A/B Testing) Function Monitoring Troubleshooting Logging and Metrics Emit and View Inference Container Logs How to Add Logs to Your Inference Container View Function Metrics Instrument with OpenTelemetry Logging and Metrics (Internal) Function Permissions Listing Functions via API Asset Management Creating an Asset ID and Pre-signed Upload URL Uploading Assets to Cloud Storage Listing Assets Deleting Assets Specifying Assets when invoking a Function Using Assets with Custom Containers Retrieving Asset Directory and Asset ID in Triton Python Backend Cluster Setup &amp; Management Prerequisites Supported Kubernetes Versions Considerations Register the Cluster Configuration Advanced Settings Caching Support Install the Cluster Agent View &amp; Validate Cluster Setup Verify Cluster Agent Installation via UI Verify Cluster Agent Installation via Terminal Cluster Agent Monitoring and Reliability Monitoring Data Metrics Logs Tracing Cluster Key Rotation Advanced: NVCA Operator Configuration Options Node Affinity NVCA Operator Parameters NGC Configuration Node Selector Configuration OpenTelemetry Configuration Advanced: Manual Instance Configuration NGC Account Access Login as the Organization Owner FAQ", "keywords": []}]};